{"meta":{"exported_on":1599241941656,"version":"3.31.5"},"data":{"migrations_lock":[{"lock_key":"km01","locked":0,"acquired_at":"2020-09-04 17:40:19","released_at":"2020-09-04 17:40:20"}],"migrations":[{"id":1,"name":"1-create-tables.js","version":"init","currentVersion":"3.31"},{"id":2,"name":"2-create-fixtures.js","version":"init","currentVersion":"3.31"},{"id":3,"name":"1-post-excerpt.js","version":"1.3","currentVersion":"3.31"},{"id":4,"name":"1-codeinjection-post.js","version":"1.4","currentVersion":"3.31"},{"id":5,"name":"1-og-twitter-post.js","version":"1.5","currentVersion":"3.31"},{"id":6,"name":"1-add-backup-client.js","version":"1.7","currentVersion":"3.31"},{"id":7,"name":"1-add-permissions-redirect.js","version":"1.9","currentVersion":"3.31"},{"id":8,"name":"1-custom-template-post.js","version":"1.13","currentVersion":"3.31"},{"id":9,"name":"2-theme-permissions.js","version":"1.13","currentVersion":"3.31"},{"id":10,"name":"1-add-webhooks-table.js","version":"1.18","currentVersion":"3.31"},{"id":11,"name":"1-webhook-permissions.js","version":"1.19","currentVersion":"3.31"},{"id":12,"name":"1-remove-settings-keys.js","version":"1.20","currentVersion":"3.31"},{"id":13,"name":"1-add-contributor-role.js","version":"1.21","currentVersion":"3.31"},{"id":14,"name":"1-multiple-authors-DDL.js","version":"1.22","currentVersion":"3.31"},{"id":15,"name":"1-multiple-authors-DML.js","version":"1.22","currentVersion":"3.31"},{"id":16,"name":"1-update-koenig-beta-html.js","version":"1.25","currentVersion":"3.31"},{"id":17,"name":"2-demo-post.js","version":"1.25","currentVersion":"3.31"},{"id":18,"name":"1-rename-amp-column.js","version":"2.0","currentVersion":"3.31"},{"id":19,"name":"2-update-posts.js","version":"2.0","currentVersion":"3.31"},{"id":20,"name":"3-remove-koenig-labs.js","version":"2.0","currentVersion":"3.31"},{"id":21,"name":"4-permalink-setting.js","version":"2.0","currentVersion":"3.31"},{"id":22,"name":"5-remove-demo-post.js","version":"2.0","currentVersion":"3.31"},{"id":23,"name":"6-replace-fixture-posts.js","version":"2.0","currentVersion":"3.31"},{"id":24,"name":"1-add-sessions-table.js","version":"2.2","currentVersion":"3.31"},{"id":25,"name":"2-add-integrations-and-api-key-tables.js","version":"2.2","currentVersion":"3.31"},{"id":26,"name":"3-insert-admin-integration-role.js","version":"2.2","currentVersion":"3.31"},{"id":27,"name":"4-insert-integration-and-api-key-permissions.js","version":"2.2","currentVersion":"3.31"},{"id":28,"name":"5-add-mobiledoc-revisions-table.js","version":"2.2","currentVersion":"3.31"},{"id":29,"name":"1-add-webhook-columns.js","version":"2.3","currentVersion":"3.31"},{"id":30,"name":"2-add-webhook-edit-permission.js","version":"2.3","currentVersion":"3.31"},{"id":31,"name":"1-add-webhook-permission-roles.js","version":"2.6","currentVersion":"3.31"},{"id":32,"name":"1-add-members-table.js","version":"2.8","currentVersion":"3.31"},{"id":33,"name":"1-remove-empty-strings.js","version":"2.13","currentVersion":"3.31"},{"id":34,"name":"1-add-actions-table.js","version":"2.14","currentVersion":"3.31"},{"id":35,"name":"2-add-actions-permissions.js","version":"2.14","currentVersion":"3.31"},{"id":36,"name":"1-add-type-column-to-integrations.js","version":"2.15","currentVersion":"3.31"},{"id":37,"name":"2-insert-zapier-integration.js","version":"2.15","currentVersion":"3.31"},{"id":38,"name":"1-add-members-perrmissions.js","version":"2.16","currentVersion":"3.31"},{"id":39,"name":"1-normalize-settings.js","version":"2.17","currentVersion":"3.31"},{"id":40,"name":"2-posts-add-canonical-url.js","version":"2.17","currentVersion":"3.31"},{"id":41,"name":"1-restore-settings-from-backup.js","version":"2.18","currentVersion":"3.31"},{"id":42,"name":"1-update-editor-permissions.js","version":"2.21","currentVersion":"3.31"},{"id":43,"name":"1-add-member-permissions-to-roles.js","version":"2.22","currentVersion":"3.31"},{"id":44,"name":"1-insert-ghost-db-backup-role.js","version":"2.27","currentVersion":"3.31"},{"id":45,"name":"2-insert-db-backup-integration.js","version":"2.27","currentVersion":"3.31"},{"id":46,"name":"3-add-subdirectory-to-relative-canonical-urls.js","version":"2.27","currentVersion":"3.31"},{"id":47,"name":"1-add-db-backup-content-permission.js","version":"2.28","currentVersion":"3.31"},{"id":48,"name":"2-add-db-backup-content-permission-to-roles.js","version":"2.28","currentVersion":"3.31"},{"id":49,"name":"3-insert-ghost-scheduler-role.js","version":"2.28","currentVersion":"3.31"},{"id":50,"name":"4-insert-scheduler-integration.js","version":"2.28","currentVersion":"3.31"},{"id":51,"name":"5-add-scheduler-permission-to-roles.js","version":"2.28","currentVersion":"3.31"},{"id":52,"name":"6-add-type-column.js","version":"2.28","currentVersion":"3.31"},{"id":53,"name":"7-populate-type-column.js","version":"2.28","currentVersion":"3.31"},{"id":54,"name":"8-remove-page-column.js","version":"2.28","currentVersion":"3.31"},{"id":55,"name":"1-add-post-page-column.js","version":"2.29","currentVersion":"3.31"},{"id":56,"name":"2-populate-post-page-column.js","version":"2.29","currentVersion":"3.31"},{"id":57,"name":"3-remove-page-type-column.js","version":"2.29","currentVersion":"3.31"},{"id":58,"name":"1-remove-name-and-password-from-members-table.js","version":"2.31","currentVersion":"3.31"},{"id":59,"name":"01-add-members-stripe-customers-table.js","version":"2.32","currentVersion":"3.31"},{"id":60,"name":"02-add-name-to-members-table.js","version":"2.32","currentVersion":"3.31"},{"id":61,"name":"01-correct-members-stripe-customers-table.js","version":"2.33","currentVersion":"3.31"},{"id":62,"name":"01-add-stripe-customers-subscriptions-table.js","version":"2.34","currentVersion":"3.31"},{"id":63,"name":"02-add-email-to-members-stripe-customers-table.js","version":"2.34","currentVersion":"3.31"},{"id":64,"name":"03-add-name-to-members-stripe-customers-table.js","version":"2.34","currentVersion":"3.31"},{"id":65,"name":"01-add-note-to-members-table.js","version":"2.35","currentVersion":"3.31"},{"id":66,"name":"01-add-self-signup-and-from address-to-members-settings.js","version":"2.37","currentVersion":"3.31"},{"id":67,"name":"01-remove-user-ghost-auth-columns.js","version":"3.0","currentVersion":"3.31"},{"id":68,"name":"02-drop-token-auth-tables.js","version":"3.0","currentVersion":"3.31"},{"id":69,"name":"03-drop-client-auth-tables.js","version":"3.0","currentVersion":"3.31"},{"id":70,"name":"04-add-posts-meta-table.js","version":"3.0","currentVersion":"3.31"},{"id":71,"name":"05-populate-posts-meta-table.js","version":"3.0","currentVersion":"3.31"},{"id":72,"name":"06-remove-posts-meta-columns.js","version":"3.0","currentVersion":"3.31"},{"id":73,"name":"07-add-posts-type-column.js","version":"3.0","currentVersion":"3.31"},{"id":74,"name":"08-populate-posts-type-column.js","version":"3.0","currentVersion":"3.31"},{"id":75,"name":"09-remove-posts-page-column.js","version":"3.0","currentVersion":"3.31"},{"id":76,"name":"10-remove-empty-strings.js","version":"3.0","currentVersion":"3.31"},{"id":77,"name":"11-update-posts-html.js","version":"3.0","currentVersion":"3.31"},{"id":78,"name":"12-populate-members-table-from-subscribers.js","version":"3.0","currentVersion":"3.31"},{"id":79,"name":"13-drop-subscribers-table.js","version":"3.0","currentVersion":"3.31"},{"id":80,"name":"14-remove-subscribers-flag.js","version":"3.0","currentVersion":"3.31"},{"id":81,"name":"01-add-send-email-when-published-to-posts.js","version":"3.1","currentVersion":"3.31"},{"id":82,"name":"02-add-email-subject-to-posts-meta.js","version":"3.1","currentVersion":"3.31"},{"id":83,"name":"03-add-email-preview-permissions.js","version":"3.1","currentVersion":"3.31"},{"id":84,"name":"04-add-subscribed-flag-to-members.js","version":"3.1","currentVersion":"3.31"},{"id":85,"name":"05-add-emails-table.js","version":"3.1","currentVersion":"3.31"},{"id":86,"name":"06-add-email-permissions.js","version":"3.1","currentVersion":"3.31"},{"id":87,"name":"07-add-uuid-field-to-members.js","version":"3.1","currentVersion":"3.31"},{"id":88,"name":"08-add-uuid-values-to-members.js","version":"3.1","currentVersion":"3.31"},{"id":89,"name":"09-add-further-email-permissions.js","version":"3.1","currentVersion":"3.31"},{"id":90,"name":"10-add-email-error-data-column.js","version":"3.1","currentVersion":"3.31"},{"id":91,"name":"01-add-cancel-at-period-end-to-subscriptions.js","version":"3.2","currentVersion":"3.31"},{"id":92,"name":"1-add-labels-table.js","version":"3.6","currentVersion":"3.31"},{"id":93,"name":"2-add-members-labels-table.js","version":"3.6","currentVersion":"3.31"},{"id":94,"name":"3-add-labels-permissions.js","version":"3.6","currentVersion":"3.31"},{"id":95,"name":"01-fix-incorrect-member-labels-foreign-keys.js","version":"3.7","currentVersion":"3.31"},{"id":96,"name":"01-add-geolocation-to-members.js","version":"3.8","currentVersion":"3.31"},{"id":97,"name":"01-add-member-sigin-url-permissions.js","version":"3.9","currentVersion":"3.31"},{"id":98,"name":"01-remove-broken-complimentary-plan-from-members-settings.js","version":"3.11","currentVersion":"3.31"},{"id":99,"name":"01-add-identity-permission.js","version":"3.12","currentVersion":"3.31"},{"id":100,"name":"02-remove-legacy-is-paid-flag-from-settings.js","version":"3.12","currentVersion":"3.31"},{"id":101,"name":"01-add-email-preview-permissions-to-roles.js","version":"3.18","currentVersion":"3.31"},{"id":102,"name":"02-add-members_stripe_connect-auth-permissions.js","version":"3.18","currentVersion":"3.31"},{"id":103,"name":"01-update-member-from-email-address.js","version":"3.19","currentVersion":"3.31"},{"id":104,"name":"01-removed-legacy-values-from-settings-table.js","version":"3.22","currentVersion":"3.31"},{"id":105,"name":"02-settings-key-renames.js","version":"3.22","currentVersion":"3.31"},{"id":106,"name":"03-add-group-and-flags-to-settings.js","version":"3.22","currentVersion":"3.31"},{"id":107,"name":"04-populate-settings-groups-and-flags.js","version":"3.22","currentVersion":"3.31"},{"id":108,"name":"05-migrate-members-subscription-settings.js","version":"3.22","currentVersion":"3.31"},{"id":109,"name":"06-migrate-stripe-connect-settings.js","version":"3.22","currentVersion":"3.31"},{"id":110,"name":"07-update-type-for-settings.js","version":"3.22","currentVersion":"3.31"},{"id":111,"name":"01-migrate-bulk-email-settings.js","version":"3.23","currentVersion":"3.31"},{"id":112,"name":"02-remove-bulk-email-settings.js","version":"3.23","currentVersion":"3.31"},{"id":113,"name":"03-update-portal-button-setting.js","version":"3.23","currentVersion":"3.31"},{"id":114,"name":"04-add-meta-columns-to-tags-table.js","version":"3.23","currentVersion":"3.31"},{"id":115,"name":"01-populate-group-for-new-portal-settings.js","version":"3.24","currentVersion":"3.31"},{"id":116,"name":"01-add-members-stripe-webhook-settings.js","version":"3.25","currentVersion":"3.31"},{"id":117,"name":"01-add-amp-gtag-id-setting.js","version":"3.26","currentVersion":"3.31"},{"id":118,"name":"01-remove-duplicate-subscriptions.js","version":"3.29","currentVersion":"3.31"},{"id":119,"name":"02-remove-duplicate-customers.js","version":"3.29","currentVersion":"3.31"},{"id":120,"name":"03-remove-orphaned-customers.js","version":"3.29","currentVersion":"3.31"},{"id":121,"name":"04-remove-orphaned-subscriptions.js","version":"3.29","currentVersion":"3.31"},{"id":122,"name":"05-add-member-constraints.js","version":"3.29","currentVersion":"3.31"},{"id":123,"name":"01-add-member-signin-url-permission-roles.js","version":"3.30","currentVersion":"3.31"}],"posts":[{"id":"5f527c0484e1ba4f6553db80","uuid":"3bd7fdbe-40c3-4a6f-b2ff-53f98e252b02","title":"Creating a custom theme","slug":"themes","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"https://static.ghost.org/v3.0.0/images/theme-marketplace.png\",\"caption\":\"Anyone can write a completely custom Ghost theme with some solid knowledge of HTML and CSS\",\"alt\":\"Ghost theme marketplace screenshot\"}]],\"markups\":[[\"a\",[\"href\",\"https://ghost.org/marketplace/\"]],[\"code\"],[\"a\",[\"href\",\"https://github.com/TryGhost/Casper\"]],[\"a\",[\"href\",\"https://ghost.org/docs/api/handlebars-themes/\"]],[\"a\",[\"href\",\"https://github.com/TryGhost/Starter/\"]],[\"strong\"],[\"a\",[\"href\",\"https://forum.ghost.org/c/themes\"]]],\"sections\":[[1,\"h2\",[[0,[],0,\"Ghost themes\"]]],[1,\"p\",[[0,[],0,\"Ghost comes with a default theme called Casper, which is designed to be a clean, readable publication layout and can be easily adapted for most purposes.\"]]],[1,\"p\",[[0,[],0,\"If you need something a little more customised, it's entirely possible to build on top of existing open source themes, or to build your own from scratch. Rather than giving you a few basic settings which act as a poor proxy for code, we just let you write code.\"]]],[1,\"h2\",[[0,[],0,\"Marketplace\"]]],[1,\"p\",[[0,[],0,\"There are a huge range of both free and premium pre-built themes which you can download from the \"],[0,[0],1,\"Ghost Theme Marketplace\"],[0,[],0,\":\"]]],[10,0],[1,\"h2\",[[0,[],0,\"Theme development\"]]],[1,\"p\",[[0,[],0,\"Ghost themes are written with a templating language called handlebars, which has a set of dynamic helpers to insert your data into template files. For example: \"],[0,[1],1,\"{{author.name}}\"],[0,[],0,\" outputs the name of the current author.\"]]],[1,\"p\",[[0,[],0,\"The best way to learn how to write your own Ghost theme is to have a look at \"],[0,[2],1,\"the source code for Casper\"],[0,[],0,\", which is heavily commented and should give you a sense of how everything fits together.\"],[1,[],0,0]]],[3,\"ul\",[[[0,[1],1,\"default.hbs\"],[0,[],0,\" is the main template file, all contexts will load inside this file unless specifically told to use a different template.\"]],[[0,[1],1,\"post.hbs\"],[0,[],0,\" is the file used in the context of viewing a post.\"]],[[0,[1],1,\"index.hbs\"],[0,[],0,\" is the file used in the context of viewing the home page.\"]],[[0,[],0,\"and so on\"]]]],[1,\"p\",[[0,[],0,\"We've got \"],[0,[3],1,\"full and extensive theme documentation\"],[0,[],0,\" which outlines every template file, context and helper that you can use. You can also get started with our useful \"],[0,[4],1,\"starter theme\"],[0,[],0,\", which includes the most common foundations and components required to build your own theme.\"]]],[1,\"blockquote\",[[0,[],0,\"If you want to chat with other people making Ghost themes to get any advice or help, there's also a \"],[0,[5],1,\"themes\"],[0,[],0,\" section on our \"],[0,[6],1,\"public Ghost forum\"],[0,[],0,\".\"]]]]}","html":"<h2 id=\"ghost-themes\">Ghost themes</h2><p>Ghost comes with a default theme called Casper, which is designed to be a clean, readable publication layout and can be easily adapted for most purposes.</p><p>If you need something a little more customised, it's entirely possible to build on top of existing open source themes, or to build your own from scratch. Rather than giving you a few basic settings which act as a poor proxy for code, we just let you write code.</p><h2 id=\"marketplace\">Marketplace</h2><p>There are a huge range of both free and premium pre-built themes which you can download from the <a href=\"https://ghost.org/marketplace/\">Ghost Theme Marketplace</a>:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://static.ghost.org/v3.0.0/images/theme-marketplace.png\" class=\"kg-image\" alt=\"Ghost theme marketplace screenshot\"><figcaption>Anyone can write a completely custom Ghost theme with some solid knowledge of HTML and CSS</figcaption></figure><h2 id=\"theme-development\">Theme development</h2><p>Ghost themes are written with a templating language called handlebars, which has a set of dynamic helpers to insert your data into template files. For example: <code>{{author.name}}</code> outputs the name of the current author.</p><p>The best way to learn how to write your own Ghost theme is to have a look at <a href=\"https://github.com/TryGhost/Casper\">the source code for Casper</a>, which is heavily commented and should give you a sense of how everything fits together.<br></p><ul><li><code>default.hbs</code> is the main template file, all contexts will load inside this file unless specifically told to use a different template.</li><li><code>post.hbs</code> is the file used in the context of viewing a post.</li><li><code>index.hbs</code> is the file used in the context of viewing the home page.</li><li>and so on</li></ul><p>We've got <a href=\"https://ghost.org/docs/api/handlebars-themes/\">full and extensive theme documentation</a> which outlines every template file, context and helper that you can use. You can also get started with our useful <a href=\"https://github.com/TryGhost/Starter/\">starter theme</a>, which includes the most common foundations and components required to build your own theme.</p><blockquote>If you want to chat with other people making Ghost themes to get any advice or help, there's also a <strong>themes</strong> section on our <a href=\"https://forum.ghost.org/c/themes\">public Ghost forum</a>.</blockquote>","comment_id":"5f527c0484e1ba4f6553db80","plaintext":"Ghost themes\nGhost comes with a default theme called Casper, which is designed to be a clean,\nreadable publication layout and can be easily adapted for most purposes.\n\nIf you need something a little more customised, it's entirely possible to build\non top of existing open source themes, or to build your own from scratch. Rather\nthan giving you a few basic settings which act as a poor proxy for code, we just\nlet you write code.\n\nMarketplace\nThere are a huge range of both free and premium pre-built themes which you can\ndownload from the Ghost Theme Marketplace [https://ghost.org/marketplace/]:\n\nAnyone can write a completely custom Ghost theme with some solid knowledge of\nHTML and CSSTheme development\nGhost themes are written with a templating language called handlebars, which has\na set of dynamic helpers to insert your data into template files. For example: \n{{author.name}} outputs the name of the current author.\n\nThe best way to learn how to write your own Ghost theme is to have a look at \nthe\nsource code for Casper [https://github.com/TryGhost/Casper], which is heavily\ncommented and should give you a sense of how everything fits together.\n\n\n * default.hbs is the main template file, all contexts will load inside this\n   file unless specifically told to use a different template.\n * post.hbs is the file used in the context of viewing a post.\n * index.hbs is the file used in the context of viewing the home page.\n * and so on\n\nWe've got full and extensive theme documentation\n[https://ghost.org/docs/api/handlebars-themes/] which outlines every template\nfile, context and helper that you can use. You can also get started with our\nuseful starter theme [https://github.com/TryGhost/Starter/], which includes the\nmost common foundations and components required to build your own theme.\n\n> If you want to chat with other people making Ghost themes to get any advice or\nhelp, there's also a themes section on our public Ghost forum\n[https://forum.ghost.org/c/themes].","feature_image":"https://static.ghost.org/v3.0.0/images/creating-a-custom-theme.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"5951f5fca366002ebd5dbef7","created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1","published_at":"2020-09-04 17:40:20","published_by":"1","custom_excerpt":"Ghost comes with a beautiful default theme designed for publishers which can easily be adapted for most purposes, or you can build a custom theme to suit your needs.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527c0484e1ba4f6553db82","uuid":"6093d031-f255-463d-9813-1da2af63c9c5","title":"Apps & integrations","slug":"apps-integrations","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"https://static.ghost.org/v3.0.0/images/integrations-icons.png\",\"cardWidth\":\"full\"}],[\"markdown\",{\"markdown\":\"<script src=\\\"https://zapier.com/apps/embed/widget.js?services=Ghost&container=true&limit=8\\\"></script>\\n\"}],[\"image\",{\"src\":\"https://static.ghost.org/v3.0.0/images/integrations-and-webhooks-in-ghost.png\",\"alt\":\"Screenshot of custom integrations with webhooks in Ghost Admin\",\"cardWidth\":\"\"}]],\"markups\":[[\"a\",[\"href\",\"https://ghost.org/integrations/\"]],[\"a\",[\"href\",\"https://zapier.com\"]],[\"strong\"],[\"a\",[\"href\",\"https://ghost.org/docs/api/\"]],[\"a\",[\"href\",\"/themes/\"]]],\"sections\":[[1,\"h2\",[[0,[],0,\"Work with your existing tools\"]]],[1,\"p\",[[0,[],0,\"It's possible to connect your Ghost site to hundreds of the most popular apps and tools using integrations that take no more than a few minutes to setup.\"]]],[1,\"p\",[[0,[],0,\"Whether you need to automate workflows, connect your email list, build a community or embed products from your ecommerce store, our \"],[0,[0],1,\"integrations library\"],[0,[],0,\" has got it all covered with hundreds of tutorials.\"]]],[10,0],[1,\"h2\",[[0,[],0,\"Zapier\"]]],[1,\"p\",[[0,[],0,\"On top of this, you can connect your Ghost site to more than 1,000 external services using the official integration with \"],[0,[1],1,\"Zapier\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Zapier sets up automations with Triggers and Actions, which allows you to create and customise a wide range of connected applications.\"]]],[1,\"blockquote\",[[0,[2],1,\"Example\"],[0,[],0,\": When someone new subscribes to a newsletter on a Ghost site (Trigger) then the contact information is automatically pushed into MailChimp (Action).\"]]],[1,\"p\",[[0,[2],1,\"Here are the most popular Ghost<>Zapier automation templates:\"],[0,[],0,\" \"]]],[10,1],[1,\"h2\",[[0,[],0,\"Custom integrations\"]]],[1,\"p\",[[0,[],0,\"At the heart of Ghost sits a robust JSON API – designed to create, manage and retrieve content with ease. \"]]],[1,\"p\",[[0,[],0,\"It's possible to create custom Ghost integrations with dedicated API keys and webhooks from the Integrations page within Ghost Admin. \"]]],[10,2],[1,\"p\",[[0,[],0,\"Beyond that, the API allows you to build entirely custom publishing apps. You can send content from your favourite desktop editor, build a custom interface for handling editorial workflow or use Ghost as a full headless CMS with a custom front-end.\"]]],[1,\"p\",[[0,[],0,\"The Ghost API is \"],[0,[3],1,\"thoroughly documented\"],[0,[],0,\" and straightforward to work with for developers of almost any level. \"]]],[1,\"h2\",[[0,[],0,\"Final step: Themes\"]]],[1,\"p\",[[0,[],0,\"Alright, on to the last post in our welcome-series! If you're curious about creating your own Ghost theme from scratch, \"],[0,[4],1,\"find out how that works\"],[0,[],0,\".\"]]]]}","html":"<h2 id=\"work-with-your-existing-tools\">Work with your existing tools</h2><p>It's possible to connect your Ghost site to hundreds of the most popular apps and tools using integrations that take no more than a few minutes to setup.</p><p>Whether you need to automate workflows, connect your email list, build a community or embed products from your ecommerce store, our <a href=\"https://ghost.org/integrations/\">integrations library</a> has got it all covered with hundreds of tutorials.</p><figure class=\"kg-card kg-image-card kg-width-full\"><img src=\"https://static.ghost.org/v3.0.0/images/integrations-icons.png\" class=\"kg-image\" alt></figure><h2 id=\"zapier\">Zapier</h2><p>On top of this, you can connect your Ghost site to more than 1,000 external services using the official integration with <a href=\"https://zapier.com\">Zapier</a>.</p><p>Zapier sets up automations with Triggers and Actions, which allows you to create and customise a wide range of connected applications.</p><blockquote><strong>Example</strong>: When someone new subscribes to a newsletter on a Ghost site (Trigger) then the contact information is automatically pushed into MailChimp (Action).</blockquote><p><strong>Here are the most popular Ghost&lt;&gt;Zapier automation templates:</strong> </p><!--kg-card-begin: markdown--><script src=\"https://zapier.com/apps/embed/widget.js?services=Ghost&container=true&limit=8\"></script>\n<!--kg-card-end: markdown--><h2 id=\"custom-integrations\">Custom integrations</h2><p>At the heart of Ghost sits a robust JSON API – designed to create, manage and retrieve content with ease. </p><p>It's possible to create custom Ghost integrations with dedicated API keys and webhooks from the Integrations page within Ghost Admin. </p><figure class=\"kg-card kg-image-card\"><img src=\"https://static.ghost.org/v3.0.0/images/integrations-and-webhooks-in-ghost.png\" class=\"kg-image\" alt=\"Screenshot of custom integrations with webhooks in Ghost Admin\"></figure><p>Beyond that, the API allows you to build entirely custom publishing apps. You can send content from your favourite desktop editor, build a custom interface for handling editorial workflow or use Ghost as a full headless CMS with a custom front-end.</p><p>The Ghost API is <a href=\"https://ghost.org/docs/api/\">thoroughly documented</a> and straightforward to work with for developers of almost any level. </p><h2 id=\"final-step-themes\">Final step: Themes</h2><p>Alright, on to the last post in our welcome-series! If you're curious about creating your own Ghost theme from scratch, <a href=\"/themes/\">find out how that works</a>.</p>","comment_id":"5f527c0484e1ba4f6553db82","plaintext":"Work with your existing tools\nIt's possible to connect your Ghost site to hundreds of the most popular apps\nand tools using integrations that take no more than a few minutes to setup.\n\nWhether you need to automate workflows, connect your email list, build a\ncommunity or embed products from your ecommerce store, our integrations library\n[https://ghost.org/integrations/] has got it all covered with hundreds of\ntutorials.\n\nZapier\nOn top of this, you can connect your Ghost site to more than 1,000 external\nservices using the official integration with Zapier [https://zapier.com].\n\nZapier sets up automations with Triggers and Actions, which allows you to create\nand customise a wide range of connected applications.\n\n> Example: When someone new subscribes to a newsletter on a Ghost site (Trigger)\nthen the contact information is automatically pushed into MailChimp (Action).\nHere are the most popular Ghost<>Zapier automation templates: \n\nCustom integrations\nAt the heart of Ghost sits a robust JSON API – designed to create, manage and\nretrieve content with ease. \n\nIt's possible to create custom Ghost integrations with dedicated API keys and\nwebhooks from the Integrations page within Ghost Admin. \n\nBeyond that, the API allows you to build entirely custom publishing apps. You\ncan send content from your favourite desktop editor, build a custom interface\nfor handling editorial workflow or use Ghost as a full headless CMS with a\ncustom front-end.\n\nThe Ghost API is thoroughly documented [https://ghost.org/docs/api/] and\nstraightforward to work with for developers of almost any level. \n\nFinal step: Themes\nAlright, on to the last post in our welcome-series! If you're curious about\ncreating your own Ghost theme from scratch, find out how that works [/themes/].","feature_image":"https://static.ghost.org/v3.0.0/images/app-integrations.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"5951f5fca366002ebd5dbef7","created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1","published_at":"2020-09-04 17:40:21","published_by":"1","custom_excerpt":"Work with all your favourite apps and tools or create your own custom integrations using the Ghost API.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527c0484e1ba4f6553db84","uuid":"69d33c90-993b-4993-80c1-02d756a8821b","title":"Organising your content","slug":"organising-content","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[],\"markups\":[[\"code\"],[\"em\"],[\"strong\"],[\"a\",[\"href\",\"https://ghost.org/docs/api/handlebars-themes/\"]],[\"a\",[\"href\",\"http://yaml.org/spec/1.2/spec.html\",\"rel\",\"noreferrer nofollow noopener\"]],[\"a\",[\"href\",\"https://ghost.org/docs/api/handlebars-themes/routing/\"]],[\"a\",[\"href\",\"/apps-integrations/\"]]],\"sections\":[[1,\"h2\",[[0,[],0,\"Sensible tagging\"]]],[1,\"p\",[[0,[],0,\"You can think of tags like Gmail labels. By tagging posts with one or more keyword, you can organise articles into buckets of related content.\"]]],[1,\"p\",[[0,[],0,\"When you create content for your publication you can assign tags to help differentiate between categories of content. \"]]],[1,\"p\",[[0,[],0,\"For example you may tag some content with News and other content with Podcast, which would create two distinct categories of content listed on \"],[0,[0],1,\"/tag/news/\"],[0,[],0,\" and \"],[0,[0],1,\"/tag/podcast/\"],[0,[],0,\", respectively.\"]]],[1,\"p\",[[0,[],0,\"If you tag a post with both \"],[0,[0],1,\"News\"],[0,[],0,\" \"],[0,[1],1,\"and\"],[0,[],0,\" \"],[0,[0],1,\"Podcast\"],[0,[],0,\" - then it appears in both sections. Tag archives are like dedicated home-pages for each category of content that you have. They have their own pages, their own RSS feeds, and can support their own cover images and meta data.\"]]],[1,\"h3\",[[0,[],0,\"The primary tag\"]]],[1,\"p\",[[0,[],0,\"Inside the Ghost editor, you can drag and drop tags into a specific order. The first tag in the list is always given the most importance, and some themes will only display the primary tag (the first tag in the list) by default. \"]]],[1,\"blockquote\",[[0,[1,2],1,\"News\"],[0,[],1,\", Technology, Startup\"]]],[1,\"p\",[[0,[],0,\"So you can add the most important tag which you want to show up in your theme, but also add related tags which are less important.\"]]],[1,\"h3\",[[0,[],0,\"Private tags\"]]],[1,\"p\",[[0,[],0,\"Sometimes you may want to assign a post a specific tag, but you don't necessarily want that tag appearing in the theme or creating an archive page. In Ghost, hashtags are private and can be used for special styling.\"]]],[1,\"p\",[[0,[],0,\"For example, if you sometimes publish posts with video content - you might want your theme to adapt and get rid of the sidebar for these posts, to give more space for an embedded video to fill the screen. In this case, you could use private tags to tell your theme what to do.\"]]],[1,\"blockquote\",[[0,[1,2],1,\"News\"],[0,[],1,\", #video\"]]],[1,\"p\",[[0,[],0,\"Here, the theme would assign the post publicly displayed tags of News - but it would also keep a private record of the post being tagged with #video. In your theme, you could then look for private tags conditionally and give them special formatting. \"]]],[1,\"blockquote\",[[0,[1],0,\"You can find documentation for theme development techniques like this and many more over on Ghost's extensive \"],[0,[3],1,\"theme docs\"],[0,[],1,\".\"]]],[1,\"h2\",[[0,[],0,\"Dynamic routing\"]]],[1,\"p\",[[0,[],0,\"Dynamic routing gives you the ultimate freedom to build a custom publication to suit your needs. Routes are rules that map URL patterns to your content and templates. \"]]],[1,\"p\",[[0,[],0,\"You may not want content tagged with \"],[0,[0],1,\"News\"],[0,[],0,\" to exist on: \"],[0,[0],1,\"example.com/tag/news\"],[0,[],0,\". Instead, you want it to exist on \"],[0,[0],1,\"example.com/news\"],[0,[],0,\" .\"]]],[1,\"p\",[[0,[],0,\"In this case you can use dynamic routes to create customised collections of content on your site. It's also possible to use multiple templates in your theme to render each content type differently.\"]]],[1,\"p\",[[0,[],0,\"There are lots of use cases for dynamic routing with Ghost, here are a few common examples: \"]]],[3,\"ul\",[[[0,[],0,\"Setting a custom home page with its own template\"]],[[0,[],0,\"Having separate content hubs for blog and podcast, that render differently, and have custom RSS feeds to support two types of content\"]],[[0,[],0,\"Creating a founders column as a unique view, by filtering content created by specific authors\"]],[[0,[],0,\"Including dates in permalinks for your posts\"]],[[0,[],0,\"Setting posts to have a URL relative to their primary tag like \"],[0,[0],1,\"example.com/europe/story-title/\"],[1,[],0,0]]]],[1,\"blockquote\",[[0,[1],0,\"Dynamic routing can be configured in Ghost using \"],[0,[4],1,\"YAML\"],[0,[],0,\" files. Read our dynamic routing \"],[0,[5],1,\"documentation\"],[0,[],1,\" for further details.\"]]],[1,\"h2\",[[0,[],0,\"Next: Apps & Integrations\"]]],[1,\"p\",[[0,[],0,\"Work with all your favourite apps and tools using our \"],[0,[6],1,\"integrations\"],[0,[],0,\", or create your own custom integrations with webhooks.\"]]],[1,\"p\",[]]]}","html":"<h2 id=\"sensible-tagging\">Sensible tagging</h2><p>You can think of tags like Gmail labels. By tagging posts with one or more keyword, you can organise articles into buckets of related content.</p><p>When you create content for your publication you can assign tags to help differentiate between categories of content. </p><p>For example you may tag some content with News and other content with Podcast, which would create two distinct categories of content listed on <code>/tag/news/</code> and <code>/tag/podcast/</code>, respectively.</p><p>If you tag a post with both <code>News</code> <em>and</em> <code>Podcast</code> - then it appears in both sections. Tag archives are like dedicated home-pages for each category of content that you have. They have their own pages, their own RSS feeds, and can support their own cover images and meta data.</p><h3 id=\"the-primary-tag\">The primary tag</h3><p>Inside the Ghost editor, you can drag and drop tags into a specific order. The first tag in the list is always given the most importance, and some themes will only display the primary tag (the first tag in the list) by default. </p><blockquote><em><strong>News</strong>, Technology, Startup</em></blockquote><p>So you can add the most important tag which you want to show up in your theme, but also add related tags which are less important.</p><h3 id=\"private-tags\">Private tags</h3><p>Sometimes you may want to assign a post a specific tag, but you don't necessarily want that tag appearing in the theme or creating an archive page. In Ghost, hashtags are private and can be used for special styling.</p><p>For example, if you sometimes publish posts with video content - you might want your theme to adapt and get rid of the sidebar for these posts, to give more space for an embedded video to fill the screen. In this case, you could use private tags to tell your theme what to do.</p><blockquote><em><strong>News</strong>, #video</em></blockquote><p>Here, the theme would assign the post publicly displayed tags of News - but it would also keep a private record of the post being tagged with #video. In your theme, you could then look for private tags conditionally and give them special formatting. </p><blockquote><em>You can find documentation for theme development techniques like this and many more over on Ghost's extensive <a href=\"https://ghost.org/docs/api/handlebars-themes/\">theme docs</a>.</em></blockquote><h2 id=\"dynamic-routing\">Dynamic routing</h2><p>Dynamic routing gives you the ultimate freedom to build a custom publication to suit your needs. Routes are rules that map URL patterns to your content and templates. </p><p>You may not want content tagged with <code>News</code> to exist on: <code>example.com/tag/news</code>. Instead, you want it to exist on <code>example.com/news</code> .</p><p>In this case you can use dynamic routes to create customised collections of content on your site. It's also possible to use multiple templates in your theme to render each content type differently.</p><p>There are lots of use cases for dynamic routing with Ghost, here are a few common examples: </p><ul><li>Setting a custom home page with its own template</li><li>Having separate content hubs for blog and podcast, that render differently, and have custom RSS feeds to support two types of content</li><li>Creating a founders column as a unique view, by filtering content created by specific authors</li><li>Including dates in permalinks for your posts</li><li>Setting posts to have a URL relative to their primary tag like <code>example.com/europe/story-title/</code><br></li></ul><blockquote><em>Dynamic routing can be configured in Ghost using <a href=\"http://yaml.org/spec/1.2/spec.html\" rel=\"noreferrer nofollow noopener\">YAML</a> files. Read our dynamic routing <a href=\"https://ghost.org/docs/api/handlebars-themes/routing/\">documentation</a> for further details.</em></blockquote><h2 id=\"next-apps-integrations\">Next: Apps &amp; Integrations</h2><p>Work with all your favourite apps and tools using our <a href=\"/apps-integrations/\">integrations</a>, or create your own custom integrations with webhooks.</p>","comment_id":"5f527c0484e1ba4f6553db84","plaintext":"Sensible tagging\nYou can think of tags like Gmail labels. By tagging posts with one or more\nkeyword, you can organise articles into buckets of related content.\n\nWhen you create content for your publication you can assign tags to help\ndifferentiate between categories of content. \n\nFor example you may tag some content with News and other content with Podcast,\nwhich would create two distinct categories of content listed on /tag/news/ and \n/tag/podcast/, respectively.\n\nIf you tag a post with both News and Podcast - then it appears in both sections.\nTag archives are like dedicated home-pages for each category of content that you\nhave. They have their own pages, their own RSS feeds, and can support their own\ncover images and meta data.\n\nThe primary tag\nInside the Ghost editor, you can drag and drop tags into a specific order. The\nfirst tag in the list is always given the most importance, and some themes will\nonly display the primary tag (the first tag in the list) by default. \n\n> News, Technology, Startup\nSo you can add the most important tag which you want to show up in your theme,\nbut also add related tags which are less important.\n\nPrivate tags\nSometimes you may want to assign a post a specific tag, but you don't\nnecessarily want that tag appearing in the theme or creating an archive page. In\nGhost, hashtags are private and can be used for special styling.\n\nFor example, if you sometimes publish posts with video content - you might want\nyour theme to adapt and get rid of the sidebar for these posts, to give more\nspace for an embedded video to fill the screen. In this case, you could use\nprivate tags to tell your theme what to do.\n\n> News, #video\nHere, the theme would assign the post publicly displayed tags of News - but it\nwould also keep a private record of the post being tagged with #video. In your\ntheme, you could then look for private tags conditionally and give them special\nformatting. \n\n> You can find documentation for theme development techniques like this and many\nmore over on Ghost's extensive theme docs\n[https://ghost.org/docs/api/handlebars-themes/].\nDynamic routing\nDynamic routing gives you the ultimate freedom to build a custom publication to\nsuit your needs. Routes are rules that map URL patterns to your content and\ntemplates. \n\nYou may not want content tagged with News to exist on: example.com/tag/news.\nInstead, you want it to exist on example.com/news .\n\nIn this case you can use dynamic routes to create customised collections of\ncontent on your site. It's also possible to use multiple templates in your theme\nto render each content type differently.\n\nThere are lots of use cases for dynamic routing with Ghost, here are a few\ncommon examples: \n\n * Setting a custom home page with its own template\n * Having separate content hubs for blog and podcast, that render differently,\n   and have custom RSS feeds to support two types of content\n * Creating a founders column as a unique view, by filtering content created by\n   specific authors\n * Including dates in permalinks for your posts\n * Setting posts to have a URL relative to their primary tag like \n   example.com/europe/story-title/\n   \n\n> Dynamic routing can be configured in Ghost using YAML\n[http://yaml.org/spec/1.2/spec.html] files. Read our dynamic routing \ndocumentation [https://ghost.org/docs/api/handlebars-themes/routing/] for\nfurther details.\nNext: Apps & Integrations\nWork with all your favourite apps and tools using our integrations\n[/apps-integrations/], or create your own custom integrations with webhooks.","feature_image":"https://static.ghost.org/v3.0.0/images/organising-your-content.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"5951f5fca366002ebd5dbef7","created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1","published_at":"2020-09-04 17:40:22","published_by":"1","custom_excerpt":"Ghost has a flexible organisational taxonomy called tags and the ability to create custom site structures using dynamic routes.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527c0484e1ba4f6553db86","uuid":"0bfbf42f-8dc2-4b67-bb11-ad53d01591ae","title":"Managing admin settings","slug":"admin-settings","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"https://static.ghost.org/v1.0.0/images/private.png\"}]],\"markups\":[[\"em\"],[\"strong\"],[\"a\",[\"href\",\"https://ghost.org/pricing/\"]],[\"a\",[\"href\",\"/organising-content/\"]]],\"sections\":[[1,\"h2\",[[0,[],0,\"Make your site private\"]]],[1,\"p\",[[0,[],0,\"If you've got a publication that you don't want the world to see yet because it's not ready to launch, you can hide your Ghost site behind a basic shared pass-phrase.\"]]],[1,\"p\",[[0,[],0,\"You can toggle this preference on at the bottom of Ghost's General Settings:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Ghost will give you a short, randomly generated pass-phrase which you can share with anyone who needs access to the site while you're working on it. While this setting is enabled, all search engine optimisation features will be switched off to help keep your site under the radar.\"]]],[1,\"p\",[[0,[],0,\"Do remember though, this is \"],[0,[0],1,\"not\"],[0,[],0,\" secure authentication. You shouldn't rely on this feature for protecting important private data. It's just a simple, shared pass-phrase for some very basic privacy.\"]]],[1,\"h2\",[[0,[],0,\"Invite your team \"]]],[1,\"p\",[[0,[],0,\"Ghost has a number of different user roles for your team:\"]]],[1,\"p\",[[0,[1],1,\"Contributors\"],[1,[],0,0],[0,[],0,\"This is the base user level in Ghost. Contributors can create and edit their own draft posts, but they are unable to edit drafts of others or publish posts. Contributors are \"],[0,[1],1,\"untrusted\"],[0,[],0,\" users with the most basic access to your publication.\"]]],[1,\"p\",[[0,[1],1,\"Authors\"],[1,[],0,1],[0,[],0,\"Authors are the 2nd user level in Ghost. Authors can write, edit  and publish their own posts. Authors are \"],[0,[1],1,\"trusted\"],[0,[],0,\" users. If you don't trust users to be allowed to publish their own posts, they should be set as Contributors.\"]]],[1,\"p\",[[0,[1],1,\"Editors\"],[1,[],0,2],[0,[],0,\"Editors are the 3rd user level in Ghost. Editors can do everything that an Author can do, but they can also edit and publish the posts of others - as well as their own. Editors can also invite new Contributors & Authors to the site.\"]]],[1,\"p\",[[0,[1],1,\"Administrators\"],[1,[],0,3],[0,[],0,\"The top user level in Ghost is Administrator. Again, administrators can do everything that Authors and Editors can do, but they can also edit all site settings and data, not just content. Additionally, administrators have full access to invite, manage or remove any other user of the site.\"],[1,[],0,4],[1,[],0,5],[0,[1],1,\"The Owner\"],[1,[],0,6],[0,[],0,\"There is only ever one owner of a Ghost site. The owner is a special user which has all the same permissions as an Administrator, but with two exceptions: The Owner can never be deleted. And in some circumstances the owner will have access to additional special settings if applicable. For example: billing details, if using \"],[0,[2,1],2,\"Ghost(Pro)\"],[0,[],0,\".\"]]],[1,\"blockquote\",[[0,[0],1,\"It's a good idea to ask all of your users to fill out their user profiles, including bio and social links. These will populate rich structured data for posts and generally create more opportunities for themes to fully populate their design.\"]]],[1,\"h2\",[[0,[],0,\"Next: Organising content\"]]],[1,\"p\",[[0,[],0,\"Find out how to \"],[0,[3],1,\"organise your content\"],[0,[],0,\" with sensible tags and authors, or for more advanced configurations, how to create custom content structures using dynamic routing.\"]]]]}","html":"<h2 id=\"make-your-site-private\">Make your site private</h2><p>If you've got a publication that you don't want the world to see yet because it's not ready to launch, you can hide your Ghost site behind a basic shared pass-phrase.</p><p>You can toggle this preference on at the bottom of Ghost's General Settings:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://static.ghost.org/v1.0.0/images/private.png\" class=\"kg-image\" alt></figure><p>Ghost will give you a short, randomly generated pass-phrase which you can share with anyone who needs access to the site while you're working on it. While this setting is enabled, all search engine optimisation features will be switched off to help keep your site under the radar.</p><p>Do remember though, this is <em>not</em> secure authentication. You shouldn't rely on this feature for protecting important private data. It's just a simple, shared pass-phrase for some very basic privacy.</p><h2 id=\"invite-your-team\">Invite your team </h2><p>Ghost has a number of different user roles for your team:</p><p><strong>Contributors</strong><br>This is the base user level in Ghost. Contributors can create and edit their own draft posts, but they are unable to edit drafts of others or publish posts. Contributors are <strong>untrusted</strong> users with the most basic access to your publication.</p><p><strong>Authors</strong><br>Authors are the 2nd user level in Ghost. Authors can write, edit  and publish their own posts. Authors are <strong>trusted</strong> users. If you don't trust users to be allowed to publish their own posts, they should be set as Contributors.</p><p><strong>Editors</strong><br>Editors are the 3rd user level in Ghost. Editors can do everything that an Author can do, but they can also edit and publish the posts of others - as well as their own. Editors can also invite new Contributors &amp; Authors to the site.</p><p><strong>Administrators</strong><br>The top user level in Ghost is Administrator. Again, administrators can do everything that Authors and Editors can do, but they can also edit all site settings and data, not just content. Additionally, administrators have full access to invite, manage or remove any other user of the site.<br><br><strong>The Owner</strong><br>There is only ever one owner of a Ghost site. The owner is a special user which has all the same permissions as an Administrator, but with two exceptions: The Owner can never be deleted. And in some circumstances the owner will have access to additional special settings if applicable. For example: billing details, if using <a href=\"https://ghost.org/pricing/\"><strong>Ghost(Pro)</strong></a>.</p><blockquote><em>It's a good idea to ask all of your users to fill out their user profiles, including bio and social links. These will populate rich structured data for posts and generally create more opportunities for themes to fully populate their design.</em></blockquote><h2 id=\"next-organising-content\">Next: Organising content</h2><p>Find out how to <a href=\"/organising-content/\">organise your content</a> with sensible tags and authors, or for more advanced configurations, how to create custom content structures using dynamic routing.</p>","comment_id":"5f527c0484e1ba4f6553db86","plaintext":"Make your site private\nIf you've got a publication that you don't want the world to see yet because\nit's not ready to launch, you can hide your Ghost site behind a basic shared\npass-phrase.\n\nYou can toggle this preference on at the bottom of Ghost's General Settings:\n\nGhost will give you a short, randomly generated pass-phrase which you can share\nwith anyone who needs access to the site while you're working on it. While this\nsetting is enabled, all search engine optimisation features will be switched off\nto help keep your site under the radar.\n\nDo remember though, this is not secure authentication. You shouldn't rely on\nthis feature for protecting important private data. It's just a simple, shared\npass-phrase for some very basic privacy.\n\nInvite your team \nGhost has a number of different user roles for your team:\n\nContributors\nThis is the base user level in Ghost. Contributors can create and edit their own\ndraft posts, but they are unable to edit drafts of others or publish posts.\nContributors are untrusted users with the most basic access to your publication.\n\nAuthors\nAuthors are the 2nd user level in Ghost. Authors can write, edit  and publish\ntheir own posts. Authors are trusted users. If you don't trust users to be\nallowed to publish their own posts, they should be set as Contributors.\n\nEditors\nEditors are the 3rd user level in Ghost. Editors can do everything that an\nAuthor can do, but they can also edit and publish the posts of others - as well\nas their own. Editors can also invite new Contributors & Authors to the site.\n\nAdministrators\nThe top user level in Ghost is Administrator. Again, administrators can do\neverything that Authors and Editors can do, but they can also edit all site\nsettings and data, not just content. Additionally, administrators have full\naccess to invite, manage or remove any other user of the site.\n\nThe Owner\nThere is only ever one owner of a Ghost site. The owner is a special user which\nhas all the same permissions as an Administrator, but with two exceptions: The\nOwner can never be deleted. And in some circumstances the owner will have access\nto additional special settings if applicable. For example: billing details, if\nusing Ghost(Pro) [https://ghost.org/pricing/].\n\n> It's a good idea to ask all of your users to fill out their user profiles,\nincluding bio and social links. These will populate rich structured data for\nposts and generally create more opportunities for themes to fully populate their\ndesign.\nNext: Organising content\nFind out how to organise your content [/organising-content/] with sensible tags\nand authors, or for more advanced configurations, how to create custom content\nstructures using dynamic routing.","feature_image":"https://static.ghost.org/v3.0.0/images/admin-settings.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"5951f5fca366002ebd5dbef7","created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1","published_at":"2020-09-04 17:40:23","published_by":"1","custom_excerpt":"There are a couple of things to do next while you're getting set up: making your site private and inviting your team.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527c0484e1ba4f6553db88","uuid":"410f9bf1-b99b-4d0a-9165-9a8915cd2380","title":"Publishing options","slug":"publishing-options","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"{\\n    \\\"@context\\\": \\\"https://schema.org\\\",\\n    \\\"@type\\\": \\\"Article\\\",\\n    \\\"publisher\\\": {\\n        \\\"@type\\\": \\\"Organization\\\",\\n        \\\"name\\\": \\\"Publishing options\\\",\\n        \\\"logo\\\": \\\"https://static.ghost.org/ghost-logo.svg\\\"\\n    },\\n    \\\"author\\\": {\\n        \\\"@type\\\": \\\"Person\\\",\\n        \\\"name\\\": \\\"Ghost\\\",\\n        \\\"url\\\": \\\"http://demo.ghost.io/author/ghost/\\\",\\n        \\\"sameAs\\\": []\\n    },\\n    \\\"headline\\\": \\\"Publishing options\\\",\\n    \\\"url\\\": \\\"http://demo.ghost.io/publishing-options\\\",\\n    \\\"datePublished\\\": \\\"2018-08-08T11:44:00.000Z\\\",\\n    \\\"dateModified\\\": \\\"2018-08-09T12:06:21.000Z\\\",\\n    \\\"keywords\\\": \\\"Getting Started\\\",\\n    \\\"description\\\": \\\"The Ghost editor has everything you need to fully optimise your content. This is where you can add tags and authors, feature a post, or turn a post into a page.\\\"\\n}\\n    \"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://schema.org/\"]],[\"a\",[\"href\",\"https://search.google.com/structured-data/testing-tool\",\"rel\",\"noreferrer nofollow noopener\"]],[\"a\",[\"href\",\"/admin-settings/\"]]],\"sections\":[[1,\"h2\",[[0,[],0,\"Distribute your content\"]]],[1,\"p\",[[0,[],0,\"Access the post settings menu by clicking the settings icon in the top right hand corner of the editor and discover everything you need to get your content ready for publishing. This is where you can edit things like tags, post URL, publish date and custom meta data.\"]]],[1,\"h2\",[[0,[],0,\"Feature images, URL & excerpts\"]]],[1,\"p\",[[0,[],0,\"Insert your post feature image from the very top of the post settings menu. Consider resizing or optimising your image first to ensure it's an appropriate size. Below this, you can set your post URL, publish date and add a custom excerpt.\"]]],[1,\"h2\",[[0,[],0,\"Tags & authors\"]]],[1,\"p\",[[0,[],0,\"You can easily add multiple tags and authors to any post to filter and organise the relationships between your content in Ghost.\"]]],[1,\"h2\",[[0,[],0,\"Structured data & SEO\"]]],[1,\"p\",[[0,[],0,\"There's no need to hard code your meta data. In fact, Ghost will generate default meta data automatically using the content in your post.\"]]],[1,\"p\",[[0,[],0,\"Alternatively, you can override this by adding a custom meta title and description, as well as unique information for social media sharing cards on Facebook and Twitter.\"]]],[1,\"p\",[[0,[],0,\"It's also possible to set custom canonicals, which is useful for guest posts or curated lists of external links.\"]]],[1,\"p\",[[0,[],0,\"Ghost will automatically implement \"],[0,[0],1,\"structured data\"],[0,[],0,\" for your publication using JSON-LD to further optimise your content.\"]]],[10,0],[1,\"p\",[[0,[],0,\"You can test that the structured data \"],[0,[1],1,\"schema\"],[0,[],0,\" on your site is working as it should using \"],[0,[2],1,\"Google’s structured data tool\"],[0,[],0,\". \"]]],[1,\"h2\",[[0,[],0,\"Code injection\"]]],[1,\"p\",[[0,[],0,\"This tool allows you to inject code on a per post or page basis, or across your entire site. This means you can modify CSS, add unique tracking codes, or add other scripts to the head or foot of your publication without making edits to your theme files. \"]]],[1,\"p\",[[0,[0],1,\"To add code site-wide\"],[0,[],0,\", use the code injection tool in the main admin menu. This is useful for adding a Google Analytics tracking code, or to start tracking with any other analytics tool.\"]]],[1,\"p\",[[0,[0],1,\"To add code to a post or page\"],[0,[],0,\", use the code injection tool within the post settings menu. This is useful if you want to add art direction, scripts or styles that are only applicable to one post or page.\"]]],[1,\"h2\",[[0,[],0,\"Next: Admin settings\"]]],[1,\"p\",[[0,[],0,\"Now you understand how to create and optimise content, let's explore some \"],[0,[3],1,\"admin settings\"],[0,[],0,\" so you can invite your team and start collaborating.\"]]]]}","html":"<h2 id=\"distribute-your-content\">Distribute your content</h2><p>Access the post settings menu by clicking the settings icon in the top right hand corner of the editor and discover everything you need to get your content ready for publishing. This is where you can edit things like tags, post URL, publish date and custom meta data.</p><h2 id=\"feature-images-url-excerpts\">Feature images, URL &amp; excerpts</h2><p>Insert your post feature image from the very top of the post settings menu. Consider resizing or optimising your image first to ensure it's an appropriate size. Below this, you can set your post URL, publish date and add a custom excerpt.</p><h2 id=\"tags-authors\">Tags &amp; authors</h2><p>You can easily add multiple tags and authors to any post to filter and organise the relationships between your content in Ghost.</p><h2 id=\"structured-data-seo\">Structured data &amp; SEO</h2><p>There's no need to hard code your meta data. In fact, Ghost will generate default meta data automatically using the content in your post.</p><p>Alternatively, you can override this by adding a custom meta title and description, as well as unique information for social media sharing cards on Facebook and Twitter.</p><p>It's also possible to set custom canonicals, which is useful for guest posts or curated lists of external links.</p><p>Ghost will automatically implement <strong>structured data</strong> for your publication using JSON-LD to further optimise your content.</p><pre><code>{\n    \"@context\": \"https://schema.org\",\n    \"@type\": \"Article\",\n    \"publisher\": {\n        \"@type\": \"Organization\",\n        \"name\": \"Publishing options\",\n        \"logo\": \"https://static.ghost.org/ghost-logo.svg\"\n    },\n    \"author\": {\n        \"@type\": \"Person\",\n        \"name\": \"Ghost\",\n        \"url\": \"http://demo.ghost.io/author/ghost/\",\n        \"sameAs\": []\n    },\n    \"headline\": \"Publishing options\",\n    \"url\": \"http://demo.ghost.io/publishing-options\",\n    \"datePublished\": \"2018-08-08T11:44:00.000Z\",\n    \"dateModified\": \"2018-08-09T12:06:21.000Z\",\n    \"keywords\": \"Getting Started\",\n    \"description\": \"The Ghost editor has everything you need to fully optimise your content. This is where you can add tags and authors, feature a post, or turn a post into a page.\"\n}\n    </code></pre><p>You can test that the structured data <a href=\"https://schema.org/\">schema</a> on your site is working as it should using <a href=\"https://search.google.com/structured-data/testing-tool\" rel=\"noreferrer nofollow noopener\">Google’s structured data tool</a>. </p><h2 id=\"code-injection\">Code injection</h2><p>This tool allows you to inject code on a per post or page basis, or across your entire site. This means you can modify CSS, add unique tracking codes, or add other scripts to the head or foot of your publication without making edits to your theme files. </p><p><strong>To add code site-wide</strong>, use the code injection tool in the main admin menu. This is useful for adding a Google Analytics tracking code, or to start tracking with any other analytics tool.</p><p><strong>To add code to a post or page</strong>, use the code injection tool within the post settings menu. This is useful if you want to add art direction, scripts or styles that are only applicable to one post or page.</p><h2 id=\"next-admin-settings\">Next: Admin settings</h2><p>Now you understand how to create and optimise content, let's explore some <a href=\"/admin-settings/\">admin settings</a> so you can invite your team and start collaborating.</p>","comment_id":"5f527c0484e1ba4f6553db88","plaintext":"Distribute your content\nAccess the post settings menu by clicking the settings icon in the top right\nhand corner of the editor and discover everything you need to get your content\nready for publishing. This is where you can edit things like tags, post URL,\npublish date and custom meta data.\n\nFeature images, URL & excerpts\nInsert your post feature image from the very top of the post settings menu.\nConsider resizing or optimising your image first to ensure it's an appropriate\nsize. Below this, you can set your post URL, publish date and add a custom\nexcerpt.\n\nTags & authors\nYou can easily add multiple tags and authors to any post to filter and organise\nthe relationships between your content in Ghost.\n\nStructured data & SEO\nThere's no need to hard code your meta data. In fact, Ghost will generate\ndefault meta data automatically using the content in your post.\n\nAlternatively, you can override this by adding a custom meta title and\ndescription, as well as unique information for social media sharing cards on\nFacebook and Twitter.\n\nIt's also possible to set custom canonicals, which is useful for guest posts or\ncurated lists of external links.\n\nGhost will automatically implement structured data for your publication using\nJSON-LD to further optimise your content.\n\n{\n    \"@context\": \"https://schema.org\",\n    \"@type\": \"Article\",\n    \"publisher\": {\n        \"@type\": \"Organization\",\n        \"name\": \"Publishing options\",\n        \"logo\": \"https://static.ghost.org/ghost-logo.svg\"\n    },\n    \"author\": {\n        \"@type\": \"Person\",\n        \"name\": \"Ghost\",\n        \"url\": \"http://demo.ghost.io/author/ghost/\",\n        \"sameAs\": []\n    },\n    \"headline\": \"Publishing options\",\n    \"url\": \"http://demo.ghost.io/publishing-options\",\n    \"datePublished\": \"2018-08-08T11:44:00.000Z\",\n    \"dateModified\": \"2018-08-09T12:06:21.000Z\",\n    \"keywords\": \"Getting Started\",\n    \"description\": \"The Ghost editor has everything you need to fully optimise your content. This is where you can add tags and authors, feature a post, or turn a post into a page.\"\n}\n    \n\nYou can test that the structured data schema [https://schema.org/] on your site\nis working as it should using Google’s structured data tool\n[https://search.google.com/structured-data/testing-tool]. \n\nCode injection\nThis tool allows you to inject code on a per post or page basis, or across your\nentire site. This means you can modify CSS, add unique tracking codes, or add\nother scripts to the head or foot of your publication without making edits to\nyour theme files. \n\nTo add code site-wide, use the code injection tool in the main admin menu. This\nis useful for adding a Google Analytics tracking code, or to start tracking with\nany other analytics tool.\n\nTo add code to a post or page, use the code injection tool within the post\nsettings menu. This is useful if you want to add art direction, scripts or\nstyles that are only applicable to one post or page.\n\nNext: Admin settings\nNow you understand how to create and optimise content, let's explore some admin\nsettings [/admin-settings/] so you can invite your team and start collaborating.","feature_image":"https://static.ghost.org/v3.0.0/images/publishing-options.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"5951f5fca366002ebd5dbef7","created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1","published_at":"2020-09-04 17:40:24","published_by":"1","custom_excerpt":"The Ghost editor post settings menu has everything you need to fully optimise and distribute your content effectively.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527c0484e1ba4f6553db8a","uuid":"7c1872a8-7e61-452b-89cf-9056fec9ec34","title":"Writing posts with Ghost ✍️","slug":"the-editor","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"https://static.ghost.org/v2.0.0/images/formatting-editor-demo.gif\"}],[\"code\",{\"code\":\"<header class=\\\"site-header outer\\\">\\n    <div class=\\\"inner\\\">\\n        {{> \\\"site-nav\\\"}}\\n    </div>\\n</header>\"}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://ghost.org/\",\"metadata\":{\"url\":\"https://ghost.org\",\"title\":\"Ghost: The #1 open source headless Node.js CMS\",\"description\":\"The world’s most popular modern open source publishing platform. A headless Node.js CMS used by Apple, Sky News, Tinder and thousands more. MIT licensed, with 30k+ stars on Github.\",\"author\":null,\"publisher\":\"Ghost\",\"thumbnail\":\"https://ghost.org/images/meta/Ghost.png\",\"icon\":\"https://ghost.org/icons/icon-512x512.png?v=188b8b6d743c6338ba2eab2e35bab4f5\"}}],[\"image\",{\"src\":\"https://static.ghost.org/v3.0.0/images/image-sizes-ghost-editor.png\"}],[\"gallery\",{\"images\":[{\"fileName\":\"gallery-sample-1.jpg\",\"row\":0,\"width\":6000,\"height\":4000,\"src\":\"https://static.ghost.org/v3.0.0/images/gallery-sample-1.jpg\"},{\"fileName\":\"gallery-sample-2.jpg\",\"row\":0,\"width\":5746,\"height\":3831,\"src\":\"https://static.ghost.org/v3.0.0/images/gallery-sample-2.jpg\"},{\"fileName\":\"gallery-sample-3.jpg\",\"row\":0,\"width\":5872,\"height\":3915,\"src\":\"https://static.ghost.org/v3.0.0/images/gallery-sample-3.jpg\"}]}]],\"markups\":[[\"strong\"],[\"code\"],[\"a\",[\"href\",\"/publishing-options/\"]]],\"sections\":[[1,\"h2\",[[0,[],0,\"Just start writing\"]]],[1,\"p\",[[0,[],0,\"Ghost has a powerful visual editor with familiar formatting options, as well as the ability to add dynamic content.\"]]],[1,\"p\",[[0,[],0,\"Select your text to add formatting such as headers or to create links. Or use Markdown shortcuts to do the work for you - if that's your thing. \"]]],[10,0],[1,\"h2\",[[0,[],0,\"Rich editing at your fingertips\"]]],[1,\"p\",[[0,[],0,\"The editor can also handle rich media objects, called \"],[0,[0],1,\"cards\"],[0,[],0,\", which can be organised and re-ordered using drag and drop. \"]]],[1,\"p\",[[0,[],0,\"You can insert a card either by clicking the  \"],[0,[1],1,\"+\"],[0,[],0,\"  button, or typing  \"],[0,[1],1,\"/\"],[0,[],0,\"  on a new line to search for a particular card. This allows you to efficiently insert\"],[0,[0],1,\" images\"],[0,[],0,\", \"],[0,[0],1,\"markdown\"],[0,[],0,\", \"],[0,[0],1,\"html, embeds \"],[0,[],0,\"and more.\"]]],[1,\"p\",[[0,[0],1,\"For example\"],[0,[],0,\":\"]]],[3,\"ul\",[[[0,[],0,\"Insert a video from YouTube directly by pasting the URL\"]],[[0,[],0,\"Create unique content like buttons or forms using the HTML card\"]],[[0,[],0,\"Need to share some code? Embed code blocks directly \"]]]],[10,1],[1,\"p\",[[0,[],0,\"It's also possible to share links from across the web in a visual way using bookmark cards that automatically render information from a websites meta data. Paste any URL to try it out: \"]]],[10,2],[1,\"h2\",[[0,[],0,\"Working with images in posts\"]]],[1,\"p\",[[0,[],0,\"You can add images to your posts in many ways:\"]]],[3,\"ul\",[[[0,[],0,\"Upload from your computer\"]],[[0,[],0,\"Click and drag an image into the browser\"]],[[0,[],0,\"Paste directly into the editor from your clipboard\"]],[[0,[],0,\"Insert using a URL\"]]]],[1,\"h3\",[[0,[],0,\"Image sizes\"]]],[1,\"p\",[[0,[],0,\"Once inserted you can blend images beautifully into your content at different sizes and add captions and alt tags wherever needed.\"]]],[10,3],[1,\"h3\",[[0,[],0,\"Image galleries\"]]],[1,\"p\",[[0,[],0,\"Tell visual stories using the gallery card to add up to 9 images that will display as a responsive image gallery: \"]]],[10,4],[1,\"h3\",[[0,[],0,\"Image optimisation\"]]],[1,\"p\",[[0,[],0,\"Ghost will automatically resize and optimise your images with lossless compression. Your posts will be fully optimised for the web without any extra effort on your part.\"]]],[1,\"h2\",[[0,[],0,\"Next: Publishing Options\"]]],[1,\"p\",[[0,[],0,\"Once your post is looking good, you'll want to use the \"],[0,[2],1,\"publishing options\"],[0,[],0,\" to ensure it gets distributed in the right places, with custom meta data, feature images and more.\"]]],[1,\"p\",[]]]}","html":"<h2 id=\"just-start-writing\">Just start writing</h2><p>Ghost has a powerful visual editor with familiar formatting options, as well as the ability to add dynamic content.</p><p>Select your text to add formatting such as headers or to create links. Or use Markdown shortcuts to do the work for you - if that's your thing. </p><figure class=\"kg-card kg-image-card\"><img src=\"https://static.ghost.org/v2.0.0/images/formatting-editor-demo.gif\" class=\"kg-image\" alt></figure><h2 id=\"rich-editing-at-your-fingertips\">Rich editing at your fingertips</h2><p>The editor can also handle rich media objects, called <strong>cards</strong>, which can be organised and re-ordered using drag and drop. </p><p>You can insert a card either by clicking the  <code>+</code>  button, or typing  <code>/</code>  on a new line to search for a particular card. This allows you to efficiently insert<strong> images</strong>, <strong>markdown</strong>, <strong>html, embeds </strong>and more.</p><p><strong>For example</strong>:</p><ul><li>Insert a video from YouTube directly by pasting the URL</li><li>Create unique content like buttons or forms using the HTML card</li><li>Need to share some code? Embed code blocks directly </li></ul><pre><code>&lt;header class=\"site-header outer\"&gt;\n    &lt;div class=\"inner\"&gt;\n        {{&gt; \"site-nav\"}}\n    &lt;/div&gt;\n&lt;/header&gt;</code></pre><p>It's also possible to share links from across the web in a visual way using bookmark cards that automatically render information from a websites meta data. Paste any URL to try it out: </p><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://ghost.org/\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">Ghost: The #1 open source headless Node.js CMS</div><div class=\"kg-bookmark-description\">The world’s most popular modern open source publishing platform. A headless Node.js CMS used by Apple, Sky News, Tinder and thousands more. MIT licensed, with 30k+ stars on Github.</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://ghost.org/icons/icon-512x512.png?v&#x3D;188b8b6d743c6338ba2eab2e35bab4f5\"><span class=\"kg-bookmark-publisher\">Ghost</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://ghost.org/images/meta/Ghost.png\"></div></a></figure><h2 id=\"working-with-images-in-posts\">Working with images in posts</h2><p>You can add images to your posts in many ways:</p><ul><li>Upload from your computer</li><li>Click and drag an image into the browser</li><li>Paste directly into the editor from your clipboard</li><li>Insert using a URL</li></ul><h3 id=\"image-sizes\">Image sizes</h3><p>Once inserted you can blend images beautifully into your content at different sizes and add captions and alt tags wherever needed.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://static.ghost.org/v3.0.0/images/image-sizes-ghost-editor.png\" class=\"kg-image\" alt></figure><h3 id=\"image-galleries\">Image galleries</h3><p>Tell visual stories using the gallery card to add up to 9 images that will display as a responsive image gallery: </p><figure class=\"kg-card kg-gallery-card kg-width-wide\"><div class=\"kg-gallery-container\"><div class=\"kg-gallery-row\"><div class=\"kg-gallery-image\"><img src=\"https://static.ghost.org/v3.0.0/images/gallery-sample-1.jpg\" width=\"6000\" height=\"4000\" alt></div><div class=\"kg-gallery-image\"><img src=\"https://static.ghost.org/v3.0.0/images/gallery-sample-2.jpg\" width=\"5746\" height=\"3831\" alt></div><div class=\"kg-gallery-image\"><img src=\"https://static.ghost.org/v3.0.0/images/gallery-sample-3.jpg\" width=\"5872\" height=\"3915\" alt></div></div></div></figure><h3 id=\"image-optimisation\">Image optimisation</h3><p>Ghost will automatically resize and optimise your images with lossless compression. Your posts will be fully optimised for the web without any extra effort on your part.</p><h2 id=\"next-publishing-options\">Next: Publishing Options</h2><p>Once your post is looking good, you'll want to use the <a href=\"/publishing-options/\">publishing options</a> to ensure it gets distributed in the right places, with custom meta data, feature images and more.</p>","comment_id":"5f527c0484e1ba4f6553db8a","plaintext":"Just start writing\nGhost has a powerful visual editor with familiar formatting options, as well as\nthe ability to add dynamic content.\n\nSelect your text to add formatting such as headers or to create links. Or use\nMarkdown shortcuts to do the work for you - if that's your thing. \n\nRich editing at your fingertips\nThe editor can also handle rich media objects, called cards, which can be\norganised and re-ordered using drag and drop. \n\nYou can insert a card either by clicking the+ button, or typing/ on a new line\nto search for a particular card. This allows you to efficiently insert images, \nmarkdown, html, embeds and more.\n\nFor example:\n\n * Insert a video from YouTube directly by pasting the URL\n * Create unique content like buttons or forms using the HTML card\n * Need to share some code? Embed code blocks directly \n\n<header class=\"site-header outer\">\n    <div class=\"inner\">\n        {{> \"site-nav\"}}\n    </div>\n</header>\n\nIt's also possible to share links from across the web in a visual way using\nbookmark cards that automatically render information from a websites meta data.\nPaste any URL to try it out: \n\nGhost: The #1 open source headless Node.js CMSThe world’s most popular modern\nopen source publishing platform. A headless Node.js CMS used by Apple, Sky\nNews,\nTinder and thousands more. MIT licensed, with 30k+ stars on Github.Ghost\n[https://ghost.org/]Working with images in posts\nYou can add images to your posts in many ways:\n\n * Upload from your computer\n * Click and drag an image into the browser\n * Paste directly into the editor from your clipboard\n * Insert using a URL\n\nImage sizes\nOnce inserted you can blend images beautifully into your content at different\nsizes and add captions and alt tags wherever needed.\n\nImage galleries\nTell visual stories using the gallery card to add up to 9 images that will\ndisplay as a responsive image gallery: \n\nImage optimisation\nGhost will automatically resize and optimise your images with lossless\ncompression. Your posts will be fully optimised for the web without any extra\neffort on your part.\n\nNext: Publishing Options\nOnce your post is looking good, you'll want to use the publishing options\n[/publishing-options/] to ensure it gets distributed in the right places, with\ncustom meta data, feature images and more.","feature_image":"https://static.ghost.org/v3.0.0/images/writing-posts-with-ghost.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"5951f5fca366002ebd5dbef7","created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1","published_at":"2020-09-04 17:40:25","published_by":"1","custom_excerpt":"Discover familiar formatting options in a functional toolbar and the ability to add dynamic content seamlessly.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527c0484e1ba4f6553db8c","uuid":"75d90355-c78b-4d7f-8b2c-652a91b0344e","title":"Welcome to Ghost","slug":"welcome","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://ghost.org/integrations/\"]],[\"a\",[\"href\",\"https://ghost.org/pricing\"]],[\"a\",[\"href\",\"https://github.com/TryGhost\"]],[\"a\",[\"href\",\"/the-editor/\"]]],\"sections\":[[1,\"h2\",[[0,[0],1,\"A few things you should know\"]]],[3,\"ol\",[[[0,[],0,\"Ghost is designed for ambitious, professional publishers who want to actively build a business around their content. That's who it works best for. \"]],[[0,[],0,\"The entire platform can be modified and customised to suit your needs. It's very powerful, but does require some knowledge of code. Ghost is not necessarily a good platform for beginners or people who just want a simple personal blog. \"]],[[0,[],0,\"It's possible to work with all your favourite tools and apps with hundreds of \"],[0,[1],1,\"integrations\"],[0,[],0,\" to speed up your workflows, connect email lists, build communities and much more.\"]]]],[1,\"h2\",[[0,[],0,\"Behind the scenes\"]]],[1,\"p\",[[0,[],0,\"Ghost is made by an independent non-profit organisation called the Ghost Foundation. We are 100% self funded by revenue from our \"],[0,[2],1,\"Ghost(Pro)\"],[0,[],0,\" service, and every penny we make is re-invested into funding further development of free, open source technology for modern publishing.\"]]],[1,\"p\",[[0,[],0,\"The version of Ghost you are looking at right now would not have been made possible without generous contributions from the open source \"],[0,[3],1,\"community\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Next up, the editor\"]]],[1,\"p\",[[0,[],0,\"The main thing you'll want to read about next is probably: \"],[0,[4],1,\"the Ghost editor\"],[0,[],0,\". This is where the good stuff happens.\"]]],[1,\"blockquote\",[[0,[],0,\"By the way, once you're done reading, you can simply delete the default Ghost user from your team to remove all of these introductory posts! \"]]]]}","html":"<h2 id=\"a-few-things-you-should-know\"><strong>A few things you should know</strong></h2><ol><li>Ghost is designed for ambitious, professional publishers who want to actively build a business around their content. That's who it works best for. </li><li>The entire platform can be modified and customised to suit your needs. It's very powerful, but does require some knowledge of code. Ghost is not necessarily a good platform for beginners or people who just want a simple personal blog. </li><li>It's possible to work with all your favourite tools and apps with hundreds of <a href=\"https://ghost.org/integrations/\">integrations</a> to speed up your workflows, connect email lists, build communities and much more.</li></ol><h2 id=\"behind-the-scenes\">Behind the scenes</h2><p>Ghost is made by an independent non-profit organisation called the Ghost Foundation. We are 100% self funded by revenue from our <a href=\"https://ghost.org/pricing\">Ghost(Pro)</a> service, and every penny we make is re-invested into funding further development of free, open source technology for modern publishing.</p><p>The version of Ghost you are looking at right now would not have been made possible without generous contributions from the open source <a href=\"https://github.com/TryGhost\">community</a>.</p><h2 id=\"next-up-the-editor\">Next up, the editor</h2><p>The main thing you'll want to read about next is probably: <a href=\"/the-editor/\">the Ghost editor</a>. This is where the good stuff happens.</p><blockquote>By the way, once you're done reading, you can simply delete the default Ghost user from your team to remove all of these introductory posts! </blockquote>","comment_id":"5f527c0484e1ba4f6553db8c","plaintext":"A few things you should know\n 1. Ghost is designed for ambitious, professional publishers who want to\n    actively build a business around their content. That's who it works best\n    for. \n 2. The entire platform can be modified and customised to suit your needs. It's\n    very powerful, but does require some knowledge of code. Ghost is not\n    necessarily a good platform for beginners or people who just want a simple\n    personal blog. \n 3. It's possible to work with all your favourite tools and apps with hundreds\n    of integrations [https://ghost.org/integrations/] to speed up your\n    workflows, connect email lists, build communities and much more.\n\nBehind the scenes\nGhost is made by an independent non-profit organisation called the Ghost\nFoundation. We are 100% self funded by revenue from our Ghost(Pro)\n[https://ghost.org/pricing] service, and every penny we make is re-invested into\nfunding further development of free, open source technology for modern\npublishing.\n\nThe version of Ghost you are looking at right now would not have been made\npossible without generous contributions from the open source community\n[https://github.com/TryGhost].\n\nNext up, the editor\nThe main thing you'll want to read about next is probably: the Ghost editor\n[/the-editor/]. This is where the good stuff happens.\n\n> By the way, once you're done reading, you can simply delete the default Ghost\nuser from your team to remove all of these introductory posts!","feature_image":"https://static.ghost.org/v3.0.0/images/welcome-to-ghost.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"5951f5fca366002ebd5dbef7","created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1","published_at":"2020-09-04 17:40:26","published_by":"1","custom_excerpt":"Welcome, it's great to have you here.\nWe know that first impressions are important, so we've populated your new site with some initial getting started posts that will help you get familiar with everything in no time.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcdb","uuid":"c31d49d1-d428-4096-be0f-faacf66bb90a","title":"About","slug":"about","mobiledoc":"{\"version\":\"0.3.2\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\">How can I know what I think until I read what I write?\\n\\nI started this website in order to learn more about the web, how do you setup, administer and maintain a website? As I _read and write_ I hope to use this blogging platform as a place to collect my thoughts on a topic, documenting useful bits of code to save repeating the furious internet search it took to find the answer and I hope enable others to learn too.\\n\\n\\nIf you you've found any of the content interesting why not...\\n\\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\\n  <link href=\\\"https://fonts.googleapis.com/css?family=Cookie\\\" rel=\\\"stylesheet\\\"><a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\\n\\n#### Blog Roll\\nhttps://www.r-bloggers.com\"}]],\"markups\":[],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><blockquote>\n<p>How can I know what I think until I read what I write?</p>\n</blockquote>\n<p>I started this website in order to learn more about the web, how do you setup, administer and maintain a website? As I <em>read and write</em> I hope to use this blogging platform as a place to collect my thoughts on a topic, documenting useful bits of code to save repeating the furious internet search it took to find the answer and I hope enable others to learn too.</p>\n<p>If you you've found any of the content interesting why not...</p>\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\n  <link href=\"https://fonts.googleapis.com/css?family=Cookie\" rel=\"stylesheet\"><a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a>\n<h4 id=\"blogroll\">Blog Roll</h4>\n<p><a href=\"https://www.r-bloggers.com\">https://www.r-bloggers.com</a></p>\n<!--kg-card-end: markdown-->","comment_id":"3","plaintext":"> How can I know what I think until I read what I write?\n\n\nI started this website in order to learn more about the web, how do you setup,\nadminister and maintain a website? As I read and write I hope to use this\nblogging platform as a place to collect my thoughts on a topic, documenting\nuseful bits of code to save repeating the furious internet search it took to\nfind the answer and I hope enable others to learn too.\n\nIf you you've found any of the content interesting why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]Blog Roll\nhttps://www.r-bloggers.com","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2015-04-18 03:42:00","created_by":"1","updated_at":"2020-05-31 16:37:30","updated_by":null,"published_at":"2015-04-18 03:44:17","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcdc","uuid":"390ef8b1-6922-4a56-9735-19a4d8c9a734","title":"Behavourial Economics, Randomised Controlled Trials and Measuring Policy Effectiveness","slug":"behavourial-economics-randomised-controlled-trials-and-measuring-policy-effectiveness","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"What is 'beahvourial economics'?\\n\\nRandomised Controlled trials\\n\\nAutomated solution....\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>What is 'beahvourial economics'?</p>\n<p>Randomised Controlled trials</p>\n<p>Automated solution....</p>\n<!--kg-card-end: markdown-->","comment_id":"4","plaintext":"What is 'beahvourial economics'?\n\nRandomised Controlled trials\n\nAutomated solution....","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2015-04-23 00:26:36","created_by":"1","updated_at":"2015-04-23 04:42:32","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcdd","uuid":"0ceac2d3-331d-484c-b22d-d3cabb085b74","title":"How to Make a Wordcloud Using R","slug":"how-to-make-a-word-cloud-using-r","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"_Written about a year ago and reposted here..._\\n\\nRecently I have been using R for some basic data visualisations, outputs like word clouds and heat maps. I don't have a programming background so upon first look the R command line based environment can seem a little daunting. However, the ease at which I have been able to create some pretty amazing outputs with very little code has surprised me. In this blog I will attempt to share the steps in a simple process as well as the small amount of code that is needed.\\n\\n####1. RStudio + Packages\\n\\nFirst of all, you will need to install [RStudio](http://cran.rstudio.com/). The program gives the user a nice interface to operate within. The code can be typed in the window to the top left of the program, useful particularly if you want to save your code as a script. The code can be sent to the command line from there, or you can simply start typing the code into the Console.\\n\\n    # Install required packages\\n    install.packages(c(\\\"tm\\\", \\\"wordcloud\\\",\\\"SnowballC\\\"))\\n \\n    # Load libraries\\n    library(tm)\\n    library(wordcloud)\\n    library(SnowballC)\\n\\n####2. Load the Text \\n\\nThis is the point where you load the text with which you would like to create your word cloud with. For this example I am using [JFK's 'We choose to go to the Moon'](http://en.wikipedia.org/wiki/We_choose_to_go_to_the_Moon) speech.\\n```\\n# Create a corpus variable\\nmooncloud <- Corpus(DirSource(\\\"/put_your_path_to_your_text_here/\\\"))\\n \\n# Make sure it has loaded properly - have a look!\\ninspect(mooncloud)\\n```\\n\\n####3. Format and Clean the Text \\nThese commands will remove various things like punctuations and english words you aren't particularly interested in for the cloud like conjunctions. Additionally it will format the case of the text, I am going to go with lowercase, however you can run various combinations of these arguments including arguments not listed here.\\n\\n```r\\n# Strip unnecessary whitespace\\nmooncloud <- tm_map(mooncloud, stripWhitespace)\\n# Convert to lowercase\\nmooncloud <- tm_map(mooncloud, tolower)\\n# Remove conjunctions etc.\\nmooncloud <- tm_map(mooncloud, removeWords, stopwords(\\\"english\\\")) \\n# Remove suffixes to the common 'stem'\\nmooncloud <- tm_map(mooncloud, stemDocument)\\n# Remove commas etc.\\nmooncloud <- tm_map(mooncloud, removePunctuation)\\n \\n#(optional) arguments of 'tm' are converting the document to something other than text, to avoid, run this line\\nmooncloud <- tm_map(mooncloud, PlainTextDocument)\\n```\\n\\n####4. Word Cloud Time!\\nTime to produce a word cloud, run the following command and watch RStudio populate the 'Plots' window to the right of the console.\\n\\n```\\n# Time to generate a wordcloud!\\nwordcloud(mooncloud\\n        , scale=c(5,0.5)     # Set min and max scale\\n        , max.words=100      # Set top n words\\n        , random.order=FALSE # Words in decreasing freq\\n        , rot.per=0.35       # % of vertical words\\n        , use.r.layout=FALSE # Use C++ collision detection\\n        , colors=brewer.pal(8, \\\"Dark2\\\"))\\n```\\n\\n####5. The Result\\n![moon-speech-wordcloud](/content/images/2015/04/mooncloud.png)\\nVoilà! Where there is a 'will' there is a way. It's hard to imagine a current leader of a western country announcing such a bold and expensive policy now - despite the world increasing its wealth substantially in the last  half century of rapid development and therefore being even more capable than it was in the 60s. \\n\\nBack to technical aspects, for further information and optional arguments you can use to customise the word cloud please see the following:\\n\\n* [tm](http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) - the text mining package\\n* [wordcloud](http://cran.r-project.org/web/packages/wordcloud/wordcloud.pdf) - the cloud generator, also does commonality clouds e.g. compare two political speeches for common themes\\n* [SnowballC](http://cran.r-project.org/web/packages/SnowballC/SnowballC.pdf) - multi-language stemming algorithm package\\n\\nI learnt to do this from a fair amount of Googling. The most helpful blog I came across was [Georeferenced](https://georeferenced.wordpress.com/2013/01/15/rwordcloud/) - so credit where credit is due. I learnt this method due to [Wordle]( http://www.wordle.net/) being blocked from my workplace, so of course use the website if you want to keep your hands clean!\\n\\nIf you've found this content helpful why not...\\n\\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\\n  <link href=\\\"https://fonts.googleapis.com/css?family=Cookie\\\" rel=\\\"stylesheet\\\"><a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><em>Written about a year ago and reposted here...</em></p>\n<p>Recently I have been using R for some basic data visualisations, outputs like word clouds and heat maps. I don't have a programming background so upon first look the R command line based environment can seem a little daunting. However, the ease at which I have been able to create some pretty amazing outputs with very little code has surprised me. In this blog I will attempt to share the steps in a simple process as well as the small amount of code that is needed.</p>\n<h4 id=\"1rstudiopackages\">1. RStudio + Packages</h4>\n<p>First of all, you will need to install <a href=\"http://cran.rstudio.com/\">RStudio</a>. The program gives the user a nice interface to operate within. The code can be typed in the window to the top left of the program, useful particularly if you want to save your code as a script. The code can be sent to the command line from there, or you can simply start typing the code into the Console.</p>\n<pre><code># Install required packages\ninstall.packages(c(&quot;tm&quot;, &quot;wordcloud&quot;,&quot;SnowballC&quot;))\n\n# Load libraries\nlibrary(tm)\nlibrary(wordcloud)\nlibrary(SnowballC)\n</code></pre>\n<h4 id=\"2loadthetext\">2. Load the Text</h4>\n<p>This is the point where you load the text with which you would like to create your word cloud with. For this example I am using <a href=\"http://en.wikipedia.org/wiki/We_choose_to_go_to_the_Moon\">JFK's 'We choose to go to the Moon'</a> speech.</p>\n<pre><code># Create a corpus variable\nmooncloud &lt;- Corpus(DirSource(&quot;/put_your_path_to_your_text_here/&quot;))\n \n# Make sure it has loaded properly - have a look!\ninspect(mooncloud)\n</code></pre>\n<h4 id=\"3formatandcleanthetext\">3. Format and Clean the Text</h4>\n<p>These commands will remove various things like punctuations and english words you aren't particularly interested in for the cloud like conjunctions. Additionally it will format the case of the text, I am going to go with lowercase, however you can run various combinations of these arguments including arguments not listed here.</p>\n<pre><code class=\"language-r\"># Strip unnecessary whitespace\nmooncloud &lt;- tm_map(mooncloud, stripWhitespace)\n# Convert to lowercase\nmooncloud &lt;- tm_map(mooncloud, tolower)\n# Remove conjunctions etc.\nmooncloud &lt;- tm_map(mooncloud, removeWords, stopwords(&quot;english&quot;)) \n# Remove suffixes to the common 'stem'\nmooncloud &lt;- tm_map(mooncloud, stemDocument)\n# Remove commas etc.\nmooncloud &lt;- tm_map(mooncloud, removePunctuation)\n \n#(optional) arguments of 'tm' are converting the document to something other than text, to avoid, run this line\nmooncloud &lt;- tm_map(mooncloud, PlainTextDocument)\n</code></pre>\n<h4 id=\"4wordcloudtime\">4. Word Cloud Time!</h4>\n<p>Time to produce a word cloud, run the following command and watch RStudio populate the 'Plots' window to the right of the console.</p>\n<pre><code># Time to generate a wordcloud!\nwordcloud(mooncloud\n        , scale=c(5,0.5)     # Set min and max scale\n        , max.words=100      # Set top n words\n        , random.order=FALSE # Words in decreasing freq\n        , rot.per=0.35       # % of vertical words\n        , use.r.layout=FALSE # Use C++ collision detection\n        , colors=brewer.pal(8, &quot;Dark2&quot;))\n</code></pre>\n<h4 id=\"5theresult\">5. The Result</h4>\n<p><img src=\"/content/images/2015/04/mooncloud.png\" alt=\"moon-speech-wordcloud\"><br>\nVoilà! Where there is a 'will' there is a way. It's hard to imagine a current leader of a western country announcing such a bold and expensive policy now - despite the world increasing its wealth substantially in the last  half century of rapid development and therefore being even more capable than it was in the 60s.</p>\n<p>Back to technical aspects, for further information and optional arguments you can use to customise the word cloud please see the following:</p>\n<ul>\n<li><a href=\"http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf\">tm</a> - the text mining package</li>\n<li><a href=\"http://cran.r-project.org/web/packages/wordcloud/wordcloud.pdf\">wordcloud</a> - the cloud generator, also does commonality clouds e.g. compare two political speeches for common themes</li>\n<li><a href=\"http://cran.r-project.org/web/packages/SnowballC/SnowballC.pdf\">SnowballC</a> - multi-language stemming algorithm package</li>\n</ul>\n<p>I learnt to do this from a fair amount of Googling. The most helpful blog I came across was <a href=\"https://georeferenced.wordpress.com/2013/01/15/rwordcloud/\">Georeferenced</a> - so credit where credit is due. I learnt this method due to <a href=\"http://www.wordle.net/\">Wordle</a> being blocked from my workplace, so of course use the website if you want to keep your hands clean!</p>\n<p>If you've found this content helpful why not...</p>\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\n  <link href=\"https://fonts.googleapis.com/css?family=Cookie\" rel=\"stylesheet\"><a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a><!--kg-card-end: markdown-->","comment_id":"5","plaintext":"Written about a year ago and reposted here...\n\nRecently I have been using R for some basic data visualisations, outputs like\nword clouds and heat maps. I don't have a programming background so upon first\nlook the R command line based environment can seem a little daunting. However,\nthe ease at which I have been able to create some pretty amazing outputs with\nvery little code has surprised me. In this blog I will attempt to share the\nsteps in a simple process as well as the small amount of code that is needed.\n\n1. RStudio + Packages\nFirst of all, you will need to install RStudio [http://cran.rstudio.com/]. The\nprogram gives the user a nice interface to operate within. The code can be typed\nin the window to the top left of the program, useful particularly if you want to\nsave your code as a script. The code can be sent to the command line from there,\nor you can simply start typing the code into the Console.\n\n# Install required packages\ninstall.packages(c(\"tm\", \"wordcloud\",\"SnowballC\"))\n\n# Load libraries\nlibrary(tm)\nlibrary(wordcloud)\nlibrary(SnowballC)\n\n\n2. Load the Text\nThis is the point where you load the text with which you would like to create\nyour word cloud with. For this example I am using JFK's 'We choose to go to the\nMoon' [http://en.wikipedia.org/wiki/We_choose_to_go_to_the_Moon] speech.\n\n# Create a corpus variable\nmooncloud <- Corpus(DirSource(\"/put_your_path_to_your_text_here/\"))\n \n# Make sure it has loaded properly - have a look!\ninspect(mooncloud)\n\n\n3. Format and Clean the Text\nThese commands will remove various things like punctuations and english words\nyou aren't particularly interested in for the cloud like conjunctions.\nAdditionally it will format the case of the text, I am going to go with\nlowercase, however you can run various combinations of these arguments including\narguments not listed here.\n\n# Strip unnecessary whitespace\nmooncloud <- tm_map(mooncloud, stripWhitespace)\n# Convert to lowercase\nmooncloud <- tm_map(mooncloud, tolower)\n# Remove conjunctions etc.\nmooncloud <- tm_map(mooncloud, removeWords, stopwords(\"english\")) \n# Remove suffixes to the common 'stem'\nmooncloud <- tm_map(mooncloud, stemDocument)\n# Remove commas etc.\nmooncloud <- tm_map(mooncloud, removePunctuation)\n \n#(optional) arguments of 'tm' are converting the document to something other than text, to avoid, run this line\nmooncloud <- tm_map(mooncloud, PlainTextDocument)\n\n\n4. Word Cloud Time!\nTime to produce a word cloud, run the following command and watch RStudio\npopulate the 'Plots' window to the right of the console.\n\n# Time to generate a wordcloud!\nwordcloud(mooncloud\n        , scale=c(5,0.5)     # Set min and max scale\n        , max.words=100      # Set top n words\n        , random.order=FALSE # Words in decreasing freq\n        , rot.per=0.35       # % of vertical words\n        , use.r.layout=FALSE # Use C++ collision detection\n        , colors=brewer.pal(8, \"Dark2\"))\n\n\n5. The Result\n\nVoilà! Where there is a 'will' there is a way. It's hard to imagine a current\nleader of a western country announcing such a bold and expensive policy now -\ndespite the world increasing its wealth substantially in the last half century\nof rapid development and therefore being even more capable than it was in the\n60s.\n\nBack to technical aspects, for further information and optional arguments you\ncan use to customise the word cloud please see the following:\n\n * tm [http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf] - the text\n   mining package\n * wordcloud [http://cran.r-project.org/web/packages/wordcloud/wordcloud.pdf] -\n   the cloud generator, also does commonality clouds e.g. compare two political\n   speeches for common themes\n * SnowballC [http://cran.r-project.org/web/packages/SnowballC/SnowballC.pdf] -\n   multi-language stemming algorithm package\n\nI learnt to do this from a fair amount of Googling. The most helpful blog I came\nacross was Georeferenced\n[https://georeferenced.wordpress.com/2013/01/15/rwordcloud/] - so credit where\ncredit is due. I learnt this method due to Wordle [http://www.wordle.net/] being\nblocked from my workplace, so of course use the website if you want to keep your\nhands clean!\n\nIf you've found this content helpful why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]","feature_image":null,"featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2015-04-23 04:43:08","created_by":"1","updated_at":"2019-08-04 13:09:09","updated_by":null,"published_at":"2015-04-24 01:57:57","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcde","uuid":"bb6d2e72-9f72-432b-9db1-99dd8d352372","title":"Automated PDF Reports using R","slug":"automated-pdf-reports","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Aim: Create individual pdf reports for each user and email these to each users email address. \\n\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Aim: Create individual pdf reports for each user and email these to each users email address.</p>\n<!--kg-card-end: markdown-->","comment_id":"6","plaintext":"Aim: Create individual pdf reports for each user and email these to each users\nemail address.","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2015-07-13 01:17:39","created_by":"1","updated_at":"2015-07-13 01:25:31","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcdf","uuid":"f1a92863-c7c5-40c4-bc6e-159a6d80ba6b","title":"A Quick Look into Stamp Duties & Land Taxes","slug":"a-look-into-land-taxes","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"###Stamp Duties\\nStamp duties are a transaction tax. Prior to the digital era, the transfer of legal ownership presented a convenient time for governments to levy a tax. Upon the transfer of ownership a stamp duty was paid and the relevant transfer of ownership document was either stamped... \\n![stamp-duty-stamped](http://www.ozrevenues.com/Revenue-Railway-Local-Perfin-Catalogue/nsw-revs/202.jpg)\\n\\nOr a stamp was affixed...\\n![stamp-duty-stamp](/content/images/2017/08/stamp-duty-stamp-affixed.jpeg)\\n\\nNowadays, most Australians recognise stamp duties as the tax paid upon the purchase of a property. For example, in NSW the rate of stamp duty ranges from 1.25% to 7%.\\n\\n![nsw-stamp-duty-rates](/content/images/2015/07/Screen-Shot-2015-07-17-at-10-59-47-am.png)Source: [osr.nsw.gov.au](http://www.osr.nsw.gov.au/taxes/transfer-land/about)\\n\\n###Land Value Tax\\nA popular alternative, or at least with economists, is that of a Land Value Tax (LVT). Despite the idea having been argued for since [Adam Smith](https://en.wikipedia.org/wiki/Adam_Smith), the idea is most strongly associated with [Henry George](https://en.wikipedia.org/wiki/Henry_George) who famously argued in his best selling book [*Progress and Poverty*](https://en.wikipedia.org/wiki/Progress_and_Poverty) for an LVT to replace all other inefficient taxes. \\n\\nThe crux of the argument is that since land is fixed (as opposed to say the supply of labour or level of consumption) disincentives or *distortionary* effects of land taxes are not applicable. On the other hand, a stamp duty causes a distortion to the property market by disincentivising transactions. Logically, stamp duties decrease the propensity of someone to move, thus reducing labour mobility. Those individuals who have to relocate often pay more in stamp duties, which unless there was some reason to discourage people from moving, is inequitable. The [Treasury's graphs](http://taxreview.treasury.gov.au/content/finalreport.aspx?doc=html/publications/papers/final_report_part_1/chapter_6.htm) below show the shape of the effective tax rate of a stamp duty over years of occupancy. The panel B graph shows the impacts of several moves resulting in a much higher effective tax rate. A flat rate shows the effect of a land tax (flat rate). \\n\\n![effective-tax-rate-stamp-duty](/content/images/2015/07/Screen-Shot-2015-07-19-at-11-56-38-am.png)\\n\\nBy replacing stamp duties with an annually payable LVT land hoarding behaviour would be heavily discouraged. Governments would stand to benefit as stamp duty revenue fluctuates with the property market, whereas a land value tax would provide a steady annual revenue changing only when land values are revised.\\n\\nThe [Henry Review](http://taxreview.treasury.gov.au/content/finalreport.aspx?doc=html/publications/papers/final_report_part_1/chapter_6.htm) makes several important points in considering a land tax:\\n\\n* A land tax could encompass all property types such that the use of land is more likely to go that which is most productive, whether that be commercial, residential or industrial.\\n\\n* Tax-free thresholds could apply to exempt low value land. Particularly of benefit to agricultural land holders.\\n\\n* Different rates of land tax could be constructed in a progressive manner. Much like the increasing rates of income tax or stamp duty seen above.\\n\\n* Replacing stamp duty with land tax would remove the supply impediments caused by the tax system. However, the tax system itself is not the major source of supply constraints.\\n\\n* Currently land tax policy exempts owner-occupied housing, removing 60% of land by value from the potential revenue base.\\n\\n* To avoid double taxation of those who have already paid stamp duty, the tax could be introduced by only applying to new transfers.\\n\\n[The Grattan Institute estimated in 2012](https://grattan.edu.au/wp-content/uploads/2014/04/Game_Changers_Web.pdf) that replacing stamp duty with a land tax would increase GDP by $25 billion. Other [research based on Melbourne house prices](http://www.ahuri.edu.au/downloads/publications/EvRevReports/AHURI_Final_Report_No182_The_spatial_and_distributional_impacts_of_the_Henry_Review_recommendations_on_stamp_duty_and_land_tax.pdf) suggested that the introduction of a land tax would decrease average house prices by 5% and would have a greater reduction on inner CDB house prices with up to 12% reductions. \\n\\nOne potential issue with the introduction of the policy is the impact it would have on the asset rich but income poor i.e. pensioners. However, as [Michael Janda points out:](http://www.abc.net.au/news/2013-02-01/janda-stamping-out-inefficient-duties/4496356) \\n>... it would be very simple for state governments to allow such people to defer their land tax payments, which would be indexed at a suitable rate and only fall due when the property next changed hands. Thus the land tax bill could be automatically deducted from the sale proceeds of the property when it was eventually sold.\\n\\nAnother administrative consideration is the difficulty in valuing land. However as [the Economist points out](http://www.economist.com/blogs/economist-explains/2014/11/economist-explains-0), \\\"*the efficiency of the tax does not depend on accurate valuations.\\\"*\\n\\n**The biggest issue is political...**\\n\\nPeople don't like paying taxes. The issue with a land tax is that it is hard to avoid and an annually payable (or annually painful). It would be very easy to politically tar such a economically sound policy, as the *great big new tax grab*. This hasn't stopped up to [twenty countries having implemented or are considering](http://www.elibrary.imf.org/view/IMF001/20531-9781484369050/20531-9781484369050/20531-9781484369050.xml) implementing the policy at some level.\\n\\nIn Australia, the ACT introduced a land tax three years ago to replace stamp duties. This year, the South Australian government has removed stamp duties on business transfers and [will remove stamp duties on commercial property transfers in 2016](http://www.afr.com/news/sa-budget-2015-stamp-duty-on-commercial-property-to-go-20150618-ghqt4d). However, they have backed down on their [suggestion earlier this year](http://www.abc.net.au/news/2015-02-11/sa-government-releases-discussion-paper-on-tax-system/6085352) of replacing stamp duties with a land tax on residential properties for political reasons.\\n\\nThere are of course bodies such as the [Urban Development Institute of Australia (UDIA)](http://www.udia.com.au/) and the [Property Council](http://www.propertyoz.com.au/) who would stand to gain from a tax change that would increase property market activity. Active engagement of such bodies along with public education would be necessary for a government wanting to minimise political fallout.\\n\\nFor now it remains the economist's consensus and the politician's discord.\\n\\nIf you've found this content helpful why not...\\n\\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\\n  <link href=\\\"https://fonts.googleapis.com/css?family=Cookie\\\" rel=\\\"stylesheet\\\"><a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><h3 id=\"stampduties\">Stamp Duties</h3>\n<p>Stamp duties are a transaction tax. Prior to the digital era, the transfer of legal ownership presented a convenient time for governments to levy a tax. Upon the transfer of ownership a stamp duty was paid and the relevant transfer of ownership document was either stamped...<br>\n<img src=\"http://www.ozrevenues.com/Revenue-Railway-Local-Perfin-Catalogue/nsw-revs/202.jpg\" alt=\"stamp-duty-stamped\"></p>\n<p>Or a stamp was affixed...<br>\n<img src=\"/content/images/2017/08/stamp-duty-stamp-affixed.jpeg\" alt=\"stamp-duty-stamp\"></p>\n<p>Nowadays, most Australians recognise stamp duties as the tax paid upon the purchase of a property. For example, in NSW the rate of stamp duty ranges from 1.25% to 7%.</p>\n<p><img src=\"/content/images/2015/07/Screen-Shot-2015-07-17-at-10-59-47-am.png\" alt=\"nsw-stamp-duty-rates\">Source: <a href=\"http://www.osr.nsw.gov.au/taxes/transfer-land/about\">osr.nsw.gov.au</a></p>\n<h3 id=\"landvaluetax\">Land Value Tax</h3>\n<p>A popular alternative, or at least with economists, is that of a Land Value Tax (LVT). Despite the idea having been argued for since <a href=\"https://en.wikipedia.org/wiki/Adam_Smith\">Adam Smith</a>, the idea is most strongly associated with <a href=\"https://en.wikipedia.org/wiki/Henry_George\">Henry George</a> who famously argued in his best selling book <a href=\"https://en.wikipedia.org/wiki/Progress_and_Poverty\"><em>Progress and Poverty</em></a> for an LVT to replace all other inefficient taxes.</p>\n<p>The crux of the argument is that since land is fixed (as opposed to say the supply of labour or level of consumption) disincentives or <em>distortionary</em> effects of land taxes are not applicable. On the other hand, a stamp duty causes a distortion to the property market by disincentivising transactions. Logically, stamp duties decrease the propensity of someone to move, thus reducing labour mobility. Those individuals who have to relocate often pay more in stamp duties, which unless there was some reason to discourage people from moving, is inequitable. The <a href=\"http://taxreview.treasury.gov.au/content/finalreport.aspx?doc=html/publications/papers/final_report_part_1/chapter_6.htm\">Treasury's graphs</a> below show the shape of the effective tax rate of a stamp duty over years of occupancy. The panel B graph shows the impacts of several moves resulting in a much higher effective tax rate. A flat rate shows the effect of a land tax (flat rate).</p>\n<p><img src=\"/content/images/2015/07/Screen-Shot-2015-07-19-at-11-56-38-am.png\" alt=\"effective-tax-rate-stamp-duty\"></p>\n<p>By replacing stamp duties with an annually payable LVT land hoarding behaviour would be heavily discouraged. Governments would stand to benefit as stamp duty revenue fluctuates with the property market, whereas a land value tax would provide a steady annual revenue changing only when land values are revised.</p>\n<p>The <a href=\"http://taxreview.treasury.gov.au/content/finalreport.aspx?doc=html/publications/papers/final_report_part_1/chapter_6.htm\">Henry Review</a> makes several important points in considering a land tax:</p>\n<ul>\n<li>\n<p>A land tax could encompass all property types such that the use of land is more likely to go that which is most productive, whether that be commercial, residential or industrial.</p>\n</li>\n<li>\n<p>Tax-free thresholds could apply to exempt low value land. Particularly of benefit to agricultural land holders.</p>\n</li>\n<li>\n<p>Different rates of land tax could be constructed in a progressive manner. Much like the increasing rates of income tax or stamp duty seen above.</p>\n</li>\n<li>\n<p>Replacing stamp duty with land tax would remove the supply impediments caused by the tax system. However, the tax system itself is not the major source of supply constraints.</p>\n</li>\n<li>\n<p>Currently land tax policy exempts owner-occupied housing, removing 60% of land by value from the potential revenue base.</p>\n</li>\n<li>\n<p>To avoid double taxation of those who have already paid stamp duty, the tax could be introduced by only applying to new transfers.</p>\n</li>\n</ul>\n<p><a href=\"https://grattan.edu.au/wp-content/uploads/2014/04/Game_Changers_Web.pdf\">The Grattan Institute estimated in 2012</a> that replacing stamp duty with a land tax would increase GDP by $25 billion. Other <a href=\"http://www.ahuri.edu.au/downloads/publications/EvRevReports/AHURI_Final_Report_No182_The_spatial_and_distributional_impacts_of_the_Henry_Review_recommendations_on_stamp_duty_and_land_tax.pdf\">research based on Melbourne house prices</a> suggested that the introduction of a land tax would decrease average house prices by 5% and would have a greater reduction on inner CDB house prices with up to 12% reductions.</p>\n<p>One potential issue with the introduction of the policy is the impact it would have on the asset rich but income poor i.e. pensioners. However, as <a href=\"http://www.abc.net.au/news/2013-02-01/janda-stamping-out-inefficient-duties/4496356\">Michael Janda points out:</a></p>\n<blockquote>\n<p>... it would be very simple for state governments to allow such people to defer their land tax payments, which would be indexed at a suitable rate and only fall due when the property next changed hands. Thus the land tax bill could be automatically deducted from the sale proceeds of the property when it was eventually sold.</p>\n</blockquote>\n<p>Another administrative consideration is the difficulty in valuing land. However as <a href=\"http://www.economist.com/blogs/economist-explains/2014/11/economist-explains-0\">the Economist points out</a>, &quot;<em>the efficiency of the tax does not depend on accurate valuations.&quot;</em></p>\n<p><strong>The biggest issue is political...</strong></p>\n<p>People don't like paying taxes. The issue with a land tax is that it is hard to avoid and an annually payable (or annually painful). It would be very easy to politically tar such a economically sound policy, as the <em>great big new tax grab</em>. This hasn't stopped up to <a href=\"http://www.elibrary.imf.org/view/IMF001/20531-9781484369050/20531-9781484369050/20531-9781484369050.xml\">twenty countries having implemented or are considering</a> implementing the policy at some level.</p>\n<p>In Australia, the ACT introduced a land tax three years ago to replace stamp duties. This year, the South Australian government has removed stamp duties on business transfers and <a href=\"http://www.afr.com/news/sa-budget-2015-stamp-duty-on-commercial-property-to-go-20150618-ghqt4d\">will remove stamp duties on commercial property transfers in 2016</a>. However, they have backed down on their <a href=\"http://www.abc.net.au/news/2015-02-11/sa-government-releases-discussion-paper-on-tax-system/6085352\">suggestion earlier this year</a> of replacing stamp duties with a land tax on residential properties for political reasons.</p>\n<p>There are of course bodies such as the <a href=\"http://www.udia.com.au/\">Urban Development Institute of Australia (UDIA)</a> and the <a href=\"http://www.propertyoz.com.au/\">Property Council</a> who would stand to gain from a tax change that would increase property market activity. Active engagement of such bodies along with public education would be necessary for a government wanting to minimise political fallout.</p>\n<p>For now it remains the economist's consensus and the politician's discord.</p>\n<p>If you've found this content helpful why not...</p>\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\n  <link href=\"https://fonts.googleapis.com/css?family=Cookie\" rel=\"stylesheet\"><a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a><!--kg-card-end: markdown-->","comment_id":"7","plaintext":"Stamp Duties\nStamp duties are a transaction tax. Prior to the digital era, the transfer of\nlegal ownership presented a convenient time for governments to levy a tax. Upon\nthe transfer of ownership a stamp duty was paid and the relevant transfer of\nownership document was either stamped...\n\n\nOr a stamp was affixed...\n\n\nNowadays, most Australians recognise stamp duties as the tax paid upon the\npurchase of a property. For example, in NSW the rate of stamp duty ranges from\n1.25% to 7%.\n\nSource: osr.nsw.gov.au [http://www.osr.nsw.gov.au/taxes/transfer-land/about]\n\nLand Value Tax\nA popular alternative, or at least with economists, is that of a Land Value Tax\n(LVT). Despite the idea having been argued for since Adam Smith\n[https://en.wikipedia.org/wiki/Adam_Smith], the idea is most strongly associated\nwith Henry George [https://en.wikipedia.org/wiki/Henry_George] who famously\nargued in his best selling book Progress and Poverty\n[https://en.wikipedia.org/wiki/Progress_and_Poverty] for an LVT to replace all\nother inefficient taxes.\n\nThe crux of the argument is that since land is fixed (as opposed to say the\nsupply of labour or level of consumption) disincentives or distortionary effects\nof land taxes are not applicable. On the other hand, a stamp duty causes a\ndistortion to the property market by disincentivising transactions. Logically,\nstamp duties decrease the propensity of someone to move, thus reducing labour\nmobility. Those individuals who have to relocate often pay more in stamp duties,\nwhich unless there was some reason to discourage people from moving, is\ninequitable. The Treasury's graphs\n[http://taxreview.treasury.gov.au/content/finalreport.aspx?doc=html/publications/papers/final_report_part_1/chapter_6.htm] \nbelow show the shape of the effective tax rate of a stamp duty over years of\noccupancy. The panel B graph shows the impacts of several moves resulting in a\nmuch higher effective tax rate. A flat rate shows the effect of a land tax (flat\nrate).\n\n\n\nBy replacing stamp duties with an annually payable LVT land hoarding behaviour\nwould be heavily discouraged. Governments would stand to benefit as stamp duty\nrevenue fluctuates with the property market, whereas a land value tax would\nprovide a steady annual revenue changing only when land values are revised.\n\nThe Henry Review\n[http://taxreview.treasury.gov.au/content/finalreport.aspx?doc=html/publications/papers/final_report_part_1/chapter_6.htm] \nmakes several important points in considering a land tax:\n\n * A land tax could encompass all property types such that the use of land is\n   more likely to go that which is most productive, whether that be commercial,\n   residential or industrial.\n   \n   \n * Tax-free thresholds could apply to exempt low value land. Particularly of\n   benefit to agricultural land holders.\n   \n   \n * Different rates of land tax could be constructed in a progressive manner.\n   Much like the increasing rates of income tax or stamp duty seen above.\n   \n   \n * Replacing stamp duty with land tax would remove the supply impediments caused\n   by the tax system. However, the tax system itself is not the major source of\n   supply constraints.\n   \n   \n * Currently land tax policy exempts owner-occupied housing, removing 60% of\n   land by value from the potential revenue base.\n   \n   \n * To avoid double taxation of those who have already paid stamp duty, the tax\n   could be introduced by only applying to new transfers.\n   \n   \n\nThe Grattan Institute estimated in 2012\n[https://grattan.edu.au/wp-content/uploads/2014/04/Game_Changers_Web.pdf] that\nreplacing stamp duty with a land tax would increase GDP by $25 billion. Other \nresearch based on Melbourne house prices\n[http://www.ahuri.edu.au/downloads/publications/EvRevReports/AHURI_Final_Report_No182_The_spatial_and_distributional_impacts_of_the_Henry_Review_recommendations_on_stamp_duty_and_land_tax.pdf] \nsuggested that the introduction of a land tax would decrease average house\nprices by 5% and would have a greater reduction on inner CDB house prices with\nup to 12% reductions.\n\nOne potential issue with the introduction of the policy is the impact it would\nhave on the asset rich but income poor i.e. pensioners. However, as Michael\nJanda points out:\n[http://www.abc.net.au/news/2013-02-01/janda-stamping-out-inefficient-duties/4496356]\n\n> ... it would be very simple for state governments to allow such people to defer\ntheir land tax payments, which would be indexed at a suitable rate and only fall\ndue when the property next changed hands. Thus the land tax bill could be\nautomatically deducted from the sale proceeds of the property when it was\neventually sold.\n\n\nAnother administrative consideration is the difficulty in valuing land. However\nas the Economist points out\n[http://www.economist.com/blogs/economist-explains/2014/11/economist-explains-0]\n, \"the efficiency of the tax does not depend on accurate valuations.\"\n\nThe biggest issue is political...\n\nPeople don't like paying taxes. The issue with a land tax is that it is hard to\navoid and an annually payable (or annually painful). It would be very easy to\npolitically tar such a economically sound policy, as the great big new tax grab.\nThis hasn't stopped up to twenty countries having implemented or are considering\n[http://www.elibrary.imf.org/view/IMF001/20531-9781484369050/20531-9781484369050/20531-9781484369050.xml] \nimplementing the policy at some level.\n\nIn Australia, the ACT introduced a land tax three years ago to replace stamp\nduties. This year, the South Australian government has removed stamp duties on\nbusiness transfers and will remove stamp duties on commercial property\ntransfers\nin 2016\n[http://www.afr.com/news/sa-budget-2015-stamp-duty-on-commercial-property-to-go-20150618-ghqt4d]\n. However, they have backed down on their suggestion earlier this year\n[http://www.abc.net.au/news/2015-02-11/sa-government-releases-discussion-paper-on-tax-system/6085352] \nof replacing stamp duties with a land tax on residential properties for\npolitical reasons.\n\nThere are of course bodies such as the Urban Development Institute of Australia\n(UDIA) [http://www.udia.com.au/] and the Property Council\n[http://www.propertyoz.com.au/] who would stand to gain from a tax change that\nwould increase property market activity. Active engagement of such bodies along\nwith public education would be necessary for a government wanting to minimise\npolitical fallout.\n\nFor now it remains the economist's consensus and the politician's discord.\n\nIf you've found this content helpful why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]","feature_image":null,"featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2015-07-16 04:41:16","created_by":"1","updated_at":"2019-09-22 19:11:57","updated_by":null,"published_at":"2015-07-22 11:40:06","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dce0","uuid":"5348088c-fc68-4848-a8f8-2320a0835df6","title":"A foray into Text Mining -  Part I","slug":"a-foray-into-text-mining-part-i","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"have a go on mac using the apis wrapped in this package\\nhttps://cran.r-project.org/web/packages/indicoio/index.html\\n\\n\\n```\\n#helpful links\\n# http://web.stanford.edu/class/cs124/lec/sentiment.pdf\\n#\\nlibrary(tm)         \\nlibrary(wordcloud)\\nlibrary(SnowballC)\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nlibrary(stringr)\\nlibrary(RWeka)\\n\\nsetwd(\\\"/home/ucg8j/workforce/ScriptsMisc/Wordcloud/\\\")\\n\\n# load text ####\\ntext <- readLines(\\\"/home/ucg8j/workforce/ScriptsMisc/Wordcloud/Corpora/2015CensusComments.txt\\\")\\ncensusComments <- Corpus(VectorSource(text)) #vector source required in order to findAssoc() \\n\\n# CLEANING COMMANDS ####\\n# stripWhitespace\\ncensusComments <- tm_map(censusComments, stripWhitespace)  \\n# command to lower case\\ncensusComments <- tm_map(censusComments, tolower)\\n# remove common english words\\ncensusComments <- tm_map(censusComments, removeWords, stopwords(\\\"english\\\"))\\n# rule below useful for removing suffixes for the common 'stem'.\\ncensusComments <- tm_map(censusComments, stemDocument)\\n# remove punctuation\\ncensusComments <- tm_map(censusComments, removePunctuation)\\n# error fix...\\ncensusComments <- tm_map(censusComments, PlainTextDocument)\\n\\n# word cloud ####\\nset.seed(123) #to replicate the same cloud\\n#png(\\\"2015CensusCloud3.png\\\", width = 12, height = 8, res = 300)\\nwordcloud(censusComments\\n          , scale=c(5,0.1)\\n          , max.words=180\\n          , random.order=FALSE\\n          , rot.per=0.35 #rot[ate].per[centage] the amt of vertical words\\n          , use.r.layout=FALSE\\n          , colors=brewer.pal(8, \\\"Dark2\\\"))\\n#dev.off() \\n\\n# document-term matrix / frequency of terms ####\\ndtm <- TermDocumentMatrix(censusComments)\\n# tt <- findFreqTerms(dtm, lowfreq = 450)         #min of 450x mentioned\\n# termFrequency <- rowSums(as.matrix(dtm[tt,]))\\n\\n# BAR PLOT w. some data manipulation ####\\nm <- as.matrix(dtm)\\nv <- sort(rowSums(m), decreasing = T)\\nd <- data.frame(word = names(v), freq=v)\\nd <- transform(d,word = reorder(word, freq))\\nqplot(d[1:15,]$word, d[1:15,]$freq, geom = \\\"bar\\\", stat = \\\"identity\\\", ylab = \\\"Mentions\\\", xlab = \\\"Words\\\") +\\n   coord_flip()\\nggsave(\\\"termFreqBarPlotOrdered.png\\\")\\n\\n# word associations ( not working)\\n# assoc <- findAssocs(dtm, terms = \\\"management\\\", corlimit = 0.2)\\n\\n# Bi-Gram Frequency ####\\nBigramTokenizer <- function(x) NGramTokenizer(x,\\n                                              Weka_control(min=2,max=2))\\ndtm2 <- DocumentTermMatrix(censusComments, control = list(tokenize = BigramTokenizer))\\nfreq <- sort(colSums(as.matrix(dtm2)), decreasing = T)\\nwof <- data.frame(word=names(freq), freq=freq)\\nwof <- transform(wof,word = reorder(word, freq))\\nqplot(wof[1:15,]$word, wof[1:15,]$freq, geom = \\\"bar\\\", stat = \\\"identity\\\", ylab = \\\"Mentions\\\", xlab = \\\"Words\\\") +\\n    coord_flip()\\nggsave(\\\"BigramFreq.png\\\")\\n\\n# Tri-Gram Frequency ####\\nTrigramTokenizer <- function(x) NGramTokenizer(x,\\n                                              Weka_control(min=3,max=3))\\ndtm3 <- DocumentTermMatrix(censusComments, control = list(tokenize = TrigramTokenizer))\\nfreq <- sort(colSums(as.matrix(dtm3)), decreasing = T)\\nwof <- data.frame(word=names(freq), freq=freq)\\nwof <- transform(wof,word = reorder(word, freq))\\nqplot(wof[1:15,]$word, wof[1:15,]$freq, geom = \\\"bar\\\", stat = \\\"identity\\\", ylab = \\\"Mentions\\\", xlab = \\\"Words\\\") +\\n    coord_flip()\\nggsave(\\\"TrigramFreq.png\\\")\\n\\n# Quad-Gram Frequency ####\\nQuadgramTokenizer <- function(x) NGramTokenizer(x,\\n                                               Weka_control(min=4,max=4))\\ndtm4 <- DocumentTermMatrix(censusComments, control = list(tokenize = QuadgramTokenizer))\\nfreq <- sort(colSums(as.matrix(dtm4)), decreasing = T)\\nwof <- data.frame(word=names(freq), freq=freq)\\nwof <- transform(wof,word = reorder(word, freq))\\nqplot(wof[1:15,]$word, wof[1:15,]$freq, geom = \\\"bar\\\", stat = \\\"identity\\\", ylab = \\\"Mentions\\\", xlab = \\\"Words\\\") +\\n    coord_flip()\\nggsave(\\\"QuadgramFreq.png\\\")\\n\\n#wordcloud...\\n\\n# correlation plot (NOT SO GOOD)\\n# plot(dtm, \\n#      terms = findFreqTerms(dtm, lowfreq = 40)[1:25],\\n#      corThreshold = 0.2)\\n\\n#Word Clustering Dendogram\\n# tdmat <- as.matrix(\\n#     removeSparseTerms(dtm, sparse = 0.8)\\n# )\\n# distMatrix <- dist(scale(dtm))\\n# fit <- hclust(distMatrix, method=\\\"ward.D2\\\")\\n# plot(fit)\\n\\n# Sentiment Analysis (Jeffrey Breen attempt) ####\\n#notes - improve the pos/neg txt inputs\\n# - match in Groups/BSLs\\n\\nlibrary(plyr)\\nlibrary(stringr)\\n\\n\\n# function score.sentiment\\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\\n{\\n    # Parameters\\n    # sentences: vector of text to score\\n    # pos.words: vector of words of postive sentiment\\n    # neg.words: vector of words of negative sentiment\\n    # .progress: passed to laply() to control of progress bar\\n    \\n    # create simple array of scores with laply\\n    scores = laply(sentences,\\n                   function(sentence, pos.words, neg.words)\\n                   {\\n                       # remove punctuation\\n                       sentence = gsub(\\\"[[:punct:]]\\\", \\\"\\\", sentence)\\n                       # remove control characters\\n                       sentence = gsub(\\\"[[:cntrl:]]\\\", \\\"\\\", sentence)\\n                       # remove digits?\\n                       sentence = gsub('\\\\\\\\d+', '', sentence)\\n                       \\n                       # define error handling function when trying tolower\\n                       tryTolower = function(x)\\n                       {\\n                           # create missing value\\n                           y = NA\\n                           # tryCatch error\\n                           try_error = tryCatch(tolower(x), error=function(e) e)\\n                           # if not an error\\n                           if (!inherits(try_error, \\\"error\\\"))\\n                               y = tolower(x)\\n                           # result\\n                           return(y)\\n                       }\\n                       # use tryTolower with sapply \\n                       sentence = sapply(sentence, tryTolower)\\n                       \\n                       # split sentence into words with str_split (stringr package)\\n                       word.list = str_split(sentence, \\\"\\\\\\\\s+\\\")\\n                       words = unlist(word.list)\\n                       \\n                       # compare words to the dictionaries of positive & negative terms\\n                       pos.matches = match(words, pos.words)\\n                       neg.matches = match(words, neg.words)\\n                       \\n                       # get the position of the matched term or NA\\n                       # we just want a TRUE/FALSE\\n                       pos.matches = !is.na(pos.matches)\\n                       neg.matches = !is.na(neg.matches)\\n                       \\n                       # final score\\n                       score = sum(pos.matches) - sum(neg.matches)\\n                       return(score)\\n                   }, pos.words, neg.words, .progress=.progress )\\n    \\n    # data frame with scores for each sentence\\n    scores.df = data.frame(text=sentences, score=scores)\\n    return(scores.df)\\n}\\n\\n# import positive and negative words\\npos = readLines(\\\"positive_words.txt\\\")\\nneg = readLines(\\\"negative_words.txt\\\")\\n\\n# apply function score.sentiment\\nscores = score.sentiment(text, pos, neg, .progress='text')\\n\\n# add variables to data frame\\nscores$very.pos = as.numeric(scores$score >= 2)\\nscores$very.neg = as.numeric(scores$score <= -2)\\n\\n# how many very positives and very negatives\\nnumpos = sum(scores$very.pos)\\nnumneg = sum(scores$very.neg)\\n\\n# global score\\nglobal_score = round( 100 * numpos / (numpos + numneg) )\\n\\n# boxplot\\nggplot(scores, aes(y=score))+\\n    geom_boxplot(aes(fill=drink))\\n\\n# write csv / view comments and score\\nwrite.csv(scores, \\\"censusCommentsSentiment.csv\\\", row.names = F)\\n\\n# Sentiment Analysis (Bayesian approach - not yet tested) ####\\nlibrary(sentiment)\\n\\nsome_txt <- text\\n\\n# remove retweet entities\\nsome_txt = gsub(\\\"(RT|via)((?:\\\\\\\\b\\\\\\\\W*@\\\\\\\\w+)+)\\\", \\\"\\\", some_txt)\\n# remove at people\\nsome_txt = gsub(\\\"@\\\\\\\\w+\\\", \\\"\\\", some_txt)\\n# remove punctuation\\nsome_txt = gsub(\\\"[[:punct:]]\\\", \\\"\\\", some_txt)\\n# remove numbers\\nsome_txt = gsub(\\\"[[:digit:]]\\\", \\\"\\\", some_txt)\\n# remove html links\\nsome_txt = gsub(\\\"http\\\\\\\\w+\\\", \\\"\\\", some_txt)\\n# remove unnecessary spaces\\nsome_txt = gsub(\\\"[ \\\\t]{2,}\\\", \\\"\\\", some_txt)\\nsome_txt = gsub(\\\"^\\\\\\\\s+|\\\\\\\\s+$\\\", \\\"\\\", some_txt)\\n\\n# define \\\"tolower error handling\\\" function \\ntry.error = function(x)\\n{\\n    # create missing value\\n    y = NA\\n    # tryCatch error\\n    try_error = tryCatch(tolower(x), error=function(e) e)\\n    # if not an error\\n    if (!inherits(try_error, \\\"error\\\"))\\n        y = tolower(x)\\n    # result\\n    return(y)\\n}\\n# lower case using try.error with sapply \\nsome_txt = sapply(some_txt, try.error)\\n\\n# remove NAs in some_txt\\nsome_txt = some_txt[!is.na(some_txt)]\\nnames(some_txt) = NULL\\n\\n# classify emotion\\nclass_emo = classify_emotion(some_txt, algorithm=\\\"bayes\\\", prior=1.0)\\n# get emotion best fit\\nemotion = class_emo[,7]\\n# substitute NA's by \\\"unknown\\\"\\nemotion[is.na(emotion)] = \\\"unknown\\\"\\n\\n# classify polarity\\nclass_pol = classify_polarity(some_txt, algorithm=\\\"bayes\\\")\\n# get polarity best fit\\npolarity = class_pol[,4]\\n\\n```\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>have a go on mac using the apis wrapped in this package<br>\n<a href=\"https://cran.r-project.org/web/packages/indicoio/index.html\">https://cran.r-project.org/web/packages/indicoio/index.html</a></p>\n<pre><code>#helpful links\n# http://web.stanford.edu/class/cs124/lec/sentiment.pdf\n#\nlibrary(tm)         \nlibrary(wordcloud)\nlibrary(SnowballC)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(RWeka)\n\nsetwd(&quot;/home/ucg8j/workforce/ScriptsMisc/Wordcloud/&quot;)\n\n# load text ####\ntext &lt;- readLines(&quot;/home/ucg8j/workforce/ScriptsMisc/Wordcloud/Corpora/2015CensusComments.txt&quot;)\ncensusComments &lt;- Corpus(VectorSource(text)) #vector source required in order to findAssoc() \n\n# CLEANING COMMANDS ####\n# stripWhitespace\ncensusComments &lt;- tm_map(censusComments, stripWhitespace)  \n# command to lower case\ncensusComments &lt;- tm_map(censusComments, tolower)\n# remove common english words\ncensusComments &lt;- tm_map(censusComments, removeWords, stopwords(&quot;english&quot;))\n# rule below useful for removing suffixes for the common 'stem'.\ncensusComments &lt;- tm_map(censusComments, stemDocument)\n# remove punctuation\ncensusComments &lt;- tm_map(censusComments, removePunctuation)\n# error fix...\ncensusComments &lt;- tm_map(censusComments, PlainTextDocument)\n\n# word cloud ####\nset.seed(123) #to replicate the same cloud\n#png(&quot;2015CensusCloud3.png&quot;, width = 12, height = 8, res = 300)\nwordcloud(censusComments\n          , scale=c(5,0.1)\n          , max.words=180\n          , random.order=FALSE\n          , rot.per=0.35 #rot[ate].per[centage] the amt of vertical words\n          , use.r.layout=FALSE\n          , colors=brewer.pal(8, &quot;Dark2&quot;))\n#dev.off() \n\n# document-term matrix / frequency of terms ####\ndtm &lt;- TermDocumentMatrix(censusComments)\n# tt &lt;- findFreqTerms(dtm, lowfreq = 450)         #min of 450x mentioned\n# termFrequency &lt;- rowSums(as.matrix(dtm[tt,]))\n\n# BAR PLOT w. some data manipulation ####\nm &lt;- as.matrix(dtm)\nv &lt;- sort(rowSums(m), decreasing = T)\nd &lt;- data.frame(word = names(v), freq=v)\nd &lt;- transform(d,word = reorder(word, freq))\nqplot(d[1:15,]$word, d[1:15,]$freq, geom = &quot;bar&quot;, stat = &quot;identity&quot;, ylab = &quot;Mentions&quot;, xlab = &quot;Words&quot;) +\n   coord_flip()\nggsave(&quot;termFreqBarPlotOrdered.png&quot;)\n\n# word associations ( not working)\n# assoc &lt;- findAssocs(dtm, terms = &quot;management&quot;, corlimit = 0.2)\n\n# Bi-Gram Frequency ####\nBigramTokenizer &lt;- function(x) NGramTokenizer(x,\n                                              Weka_control(min=2,max=2))\ndtm2 &lt;- DocumentTermMatrix(censusComments, control = list(tokenize = BigramTokenizer))\nfreq &lt;- sort(colSums(as.matrix(dtm2)), decreasing = T)\nwof &lt;- data.frame(word=names(freq), freq=freq)\nwof &lt;- transform(wof,word = reorder(word, freq))\nqplot(wof[1:15,]$word, wof[1:15,]$freq, geom = &quot;bar&quot;, stat = &quot;identity&quot;, ylab = &quot;Mentions&quot;, xlab = &quot;Words&quot;) +\n    coord_flip()\nggsave(&quot;BigramFreq.png&quot;)\n\n# Tri-Gram Frequency ####\nTrigramTokenizer &lt;- function(x) NGramTokenizer(x,\n                                              Weka_control(min=3,max=3))\ndtm3 &lt;- DocumentTermMatrix(censusComments, control = list(tokenize = TrigramTokenizer))\nfreq &lt;- sort(colSums(as.matrix(dtm3)), decreasing = T)\nwof &lt;- data.frame(word=names(freq), freq=freq)\nwof &lt;- transform(wof,word = reorder(word, freq))\nqplot(wof[1:15,]$word, wof[1:15,]$freq, geom = &quot;bar&quot;, stat = &quot;identity&quot;, ylab = &quot;Mentions&quot;, xlab = &quot;Words&quot;) +\n    coord_flip()\nggsave(&quot;TrigramFreq.png&quot;)\n\n# Quad-Gram Frequency ####\nQuadgramTokenizer &lt;- function(x) NGramTokenizer(x,\n                                               Weka_control(min=4,max=4))\ndtm4 &lt;- DocumentTermMatrix(censusComments, control = list(tokenize = QuadgramTokenizer))\nfreq &lt;- sort(colSums(as.matrix(dtm4)), decreasing = T)\nwof &lt;- data.frame(word=names(freq), freq=freq)\nwof &lt;- transform(wof,word = reorder(word, freq))\nqplot(wof[1:15,]$word, wof[1:15,]$freq, geom = &quot;bar&quot;, stat = &quot;identity&quot;, ylab = &quot;Mentions&quot;, xlab = &quot;Words&quot;) +\n    coord_flip()\nggsave(&quot;QuadgramFreq.png&quot;)\n\n#wordcloud...\n\n# correlation plot (NOT SO GOOD)\n# plot(dtm, \n#      terms = findFreqTerms(dtm, lowfreq = 40)[1:25],\n#      corThreshold = 0.2)\n\n#Word Clustering Dendogram\n# tdmat &lt;- as.matrix(\n#     removeSparseTerms(dtm, sparse = 0.8)\n# )\n# distMatrix &lt;- dist(scale(dtm))\n# fit &lt;- hclust(distMatrix, method=&quot;ward.D2&quot;)\n# plot(fit)\n\n# Sentiment Analysis (Jeffrey Breen attempt) ####\n#notes - improve the pos/neg txt inputs\n# - match in Groups/BSLs\n\nlibrary(plyr)\nlibrary(stringr)\n\n\n# function score.sentiment\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\n{\n    # Parameters\n    # sentences: vector of text to score\n    # pos.words: vector of words of postive sentiment\n    # neg.words: vector of words of negative sentiment\n    # .progress: passed to laply() to control of progress bar\n    \n    # create simple array of scores with laply\n    scores = laply(sentences,\n                   function(sentence, pos.words, neg.words)\n                   {\n                       # remove punctuation\n                       sentence = gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, sentence)\n                       # remove control characters\n                       sentence = gsub(&quot;[[:cntrl:]]&quot;, &quot;&quot;, sentence)\n                       # remove digits?\n                       sentence = gsub('\\\\d+', '', sentence)\n                       \n                       # define error handling function when trying tolower\n                       tryTolower = function(x)\n                       {\n                           # create missing value\n                           y = NA\n                           # tryCatch error\n                           try_error = tryCatch(tolower(x), error=function(e) e)\n                           # if not an error\n                           if (!inherits(try_error, &quot;error&quot;))\n                               y = tolower(x)\n                           # result\n                           return(y)\n                       }\n                       # use tryTolower with sapply \n                       sentence = sapply(sentence, tryTolower)\n                       \n                       # split sentence into words with str_split (stringr package)\n                       word.list = str_split(sentence, &quot;\\\\s+&quot;)\n                       words = unlist(word.list)\n                       \n                       # compare words to the dictionaries of positive &amp; negative terms\n                       pos.matches = match(words, pos.words)\n                       neg.matches = match(words, neg.words)\n                       \n                       # get the position of the matched term or NA\n                       # we just want a TRUE/FALSE\n                       pos.matches = !is.na(pos.matches)\n                       neg.matches = !is.na(neg.matches)\n                       \n                       # final score\n                       score = sum(pos.matches) - sum(neg.matches)\n                       return(score)\n                   }, pos.words, neg.words, .progress=.progress )\n    \n    # data frame with scores for each sentence\n    scores.df = data.frame(text=sentences, score=scores)\n    return(scores.df)\n}\n\n# import positive and negative words\npos = readLines(&quot;positive_words.txt&quot;)\nneg = readLines(&quot;negative_words.txt&quot;)\n\n# apply function score.sentiment\nscores = score.sentiment(text, pos, neg, .progress='text')\n\n# add variables to data frame\nscores$very.pos = as.numeric(scores$score &gt;= 2)\nscores$very.neg = as.numeric(scores$score &lt;= -2)\n\n# how many very positives and very negatives\nnumpos = sum(scores$very.pos)\nnumneg = sum(scores$very.neg)\n\n# global score\nglobal_score = round( 100 * numpos / (numpos + numneg) )\n\n# boxplot\nggplot(scores, aes(y=score))+\n    geom_boxplot(aes(fill=drink))\n\n# write csv / view comments and score\nwrite.csv(scores, &quot;censusCommentsSentiment.csv&quot;, row.names = F)\n\n# Sentiment Analysis (Bayesian approach - not yet tested) ####\nlibrary(sentiment)\n\nsome_txt &lt;- text\n\n# remove retweet entities\nsome_txt = gsub(&quot;(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)&quot;, &quot;&quot;, some_txt)\n# remove at people\nsome_txt = gsub(&quot;@\\\\w+&quot;, &quot;&quot;, some_txt)\n# remove punctuation\nsome_txt = gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, some_txt)\n# remove numbers\nsome_txt = gsub(&quot;[[:digit:]]&quot;, &quot;&quot;, some_txt)\n# remove html links\nsome_txt = gsub(&quot;http\\\\w+&quot;, &quot;&quot;, some_txt)\n# remove unnecessary spaces\nsome_txt = gsub(&quot;[ \\t]{2,}&quot;, &quot;&quot;, some_txt)\nsome_txt = gsub(&quot;^\\\\s+|\\\\s+$&quot;, &quot;&quot;, some_txt)\n\n# define &quot;tolower error handling&quot; function \ntry.error = function(x)\n{\n    # create missing value\n    y = NA\n    # tryCatch error\n    try_error = tryCatch(tolower(x), error=function(e) e)\n    # if not an error\n    if (!inherits(try_error, &quot;error&quot;))\n        y = tolower(x)\n    # result\n    return(y)\n}\n# lower case using try.error with sapply \nsome_txt = sapply(some_txt, try.error)\n\n# remove NAs in some_txt\nsome_txt = some_txt[!is.na(some_txt)]\nnames(some_txt) = NULL\n\n# classify emotion\nclass_emo = classify_emotion(some_txt, algorithm=&quot;bayes&quot;, prior=1.0)\n# get emotion best fit\nemotion = class_emo[,7]\n# substitute NA's by &quot;unknown&quot;\nemotion[is.na(emotion)] = &quot;unknown&quot;\n\n# classify polarity\nclass_pol = classify_polarity(some_txt, algorithm=&quot;bayes&quot;)\n# get polarity best fit\npolarity = class_pol[,4]\n\n</code></pre>\n<!--kg-card-end: markdown-->","comment_id":"8","plaintext":"have a go on mac using the apis wrapped in this package\nhttps://cran.r-project.org/web/packages/indicoio/index.html\n\n#helpful links\n# http://web.stanford.edu/class/cs124/lec/sentiment.pdf\n#\nlibrary(tm)         \nlibrary(wordcloud)\nlibrary(SnowballC)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(RWeka)\n\nsetwd(\"/home/ucg8j/workforce/ScriptsMisc/Wordcloud/\")\n\n# load text ####\ntext <- readLines(\"/home/ucg8j/workforce/ScriptsMisc/Wordcloud/Corpora/2015CensusComments.txt\")\ncensusComments <- Corpus(VectorSource(text)) #vector source required in order to findAssoc() \n\n# CLEANING COMMANDS ####\n# stripWhitespace\ncensusComments <- tm_map(censusComments, stripWhitespace)  \n# command to lower case\ncensusComments <- tm_map(censusComments, tolower)\n# remove common english words\ncensusComments <- tm_map(censusComments, removeWords, stopwords(\"english\"))\n# rule below useful for removing suffixes for the common 'stem'.\ncensusComments <- tm_map(censusComments, stemDocument)\n# remove punctuation\ncensusComments <- tm_map(censusComments, removePunctuation)\n# error fix...\ncensusComments <- tm_map(censusComments, PlainTextDocument)\n\n# word cloud ####\nset.seed(123) #to replicate the same cloud\n#png(\"2015CensusCloud3.png\", width = 12, height = 8, res = 300)\nwordcloud(censusComments\n          , scale=c(5,0.1)\n          , max.words=180\n          , random.order=FALSE\n          , rot.per=0.35 #rot[ate].per[centage] the amt of vertical words\n          , use.r.layout=FALSE\n          , colors=brewer.pal(8, \"Dark2\"))\n#dev.off() \n\n# document-term matrix / frequency of terms ####\ndtm <- TermDocumentMatrix(censusComments)\n# tt <- findFreqTerms(dtm, lowfreq = 450)         #min of 450x mentioned\n# termFrequency <- rowSums(as.matrix(dtm[tt,]))\n\n# BAR PLOT w. some data manipulation ####\nm <- as.matrix(dtm)\nv <- sort(rowSums(m), decreasing = T)\nd <- data.frame(word = names(v), freq=v)\nd <- transform(d,word = reorder(word, freq))\nqplot(d[1:15,]$word, d[1:15,]$freq, geom = \"bar\", stat = \"identity\", ylab = \"Mentions\", xlab = \"Words\") +\n   coord_flip()\nggsave(\"termFreqBarPlotOrdered.png\")\n\n# word associations ( not working)\n# assoc <- findAssocs(dtm, terms = \"management\", corlimit = 0.2)\n\n# Bi-Gram Frequency ####\nBigramTokenizer <- function(x) NGramTokenizer(x,\n                                              Weka_control(min=2,max=2))\ndtm2 <- DocumentTermMatrix(censusComments, control = list(tokenize = BigramTokenizer))\nfreq <- sort(colSums(as.matrix(dtm2)), decreasing = T)\nwof <- data.frame(word=names(freq), freq=freq)\nwof <- transform(wof,word = reorder(word, freq))\nqplot(wof[1:15,]$word, wof[1:15,]$freq, geom = \"bar\", stat = \"identity\", ylab = \"Mentions\", xlab = \"Words\") +\n    coord_flip()\nggsave(\"BigramFreq.png\")\n\n# Tri-Gram Frequency ####\nTrigramTokenizer <- function(x) NGramTokenizer(x,\n                                              Weka_control(min=3,max=3))\ndtm3 <- DocumentTermMatrix(censusComments, control = list(tokenize = TrigramTokenizer))\nfreq <- sort(colSums(as.matrix(dtm3)), decreasing = T)\nwof <- data.frame(word=names(freq), freq=freq)\nwof <- transform(wof,word = reorder(word, freq))\nqplot(wof[1:15,]$word, wof[1:15,]$freq, geom = \"bar\", stat = \"identity\", ylab = \"Mentions\", xlab = \"Words\") +\n    coord_flip()\nggsave(\"TrigramFreq.png\")\n\n# Quad-Gram Frequency ####\nQuadgramTokenizer <- function(x) NGramTokenizer(x,\n                                               Weka_control(min=4,max=4))\ndtm4 <- DocumentTermMatrix(censusComments, control = list(tokenize = QuadgramTokenizer))\nfreq <- sort(colSums(as.matrix(dtm4)), decreasing = T)\nwof <- data.frame(word=names(freq), freq=freq)\nwof <- transform(wof,word = reorder(word, freq))\nqplot(wof[1:15,]$word, wof[1:15,]$freq, geom = \"bar\", stat = \"identity\", ylab = \"Mentions\", xlab = \"Words\") +\n    coord_flip()\nggsave(\"QuadgramFreq.png\")\n\n#wordcloud...\n\n# correlation plot (NOT SO GOOD)\n# plot(dtm, \n#      terms = findFreqTerms(dtm, lowfreq = 40)[1:25],\n#      corThreshold = 0.2)\n\n#Word Clustering Dendogram\n# tdmat <- as.matrix(\n#     removeSparseTerms(dtm, sparse = 0.8)\n# )\n# distMatrix <- dist(scale(dtm))\n# fit <- hclust(distMatrix, method=\"ward.D2\")\n# plot(fit)\n\n# Sentiment Analysis (Jeffrey Breen attempt) ####\n#notes - improve the pos/neg txt inputs\n# - match in Groups/BSLs\n\nlibrary(plyr)\nlibrary(stringr)\n\n\n# function score.sentiment\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\n{\n    # Parameters\n    # sentences: vector of text to score\n    # pos.words: vector of words of postive sentiment\n    # neg.words: vector of words of negative sentiment\n    # .progress: passed to laply() to control of progress bar\n    \n    # create simple array of scores with laply\n    scores = laply(sentences,\n                   function(sentence, pos.words, neg.words)\n                   {\n                       # remove punctuation\n                       sentence = gsub(\"[[:punct:]]\", \"\", sentence)\n                       # remove control characters\n                       sentence = gsub(\"[[:cntrl:]]\", \"\", sentence)\n                       # remove digits?\n                       sentence = gsub('\\\\d+', '', sentence)\n                       \n                       # define error handling function when trying tolower\n                       tryTolower = function(x)\n                       {\n                           # create missing value\n                           y = NA\n                           # tryCatch error\n                           try_error = tryCatch(tolower(x), error=function(e) e)\n                           # if not an error\n                           if (!inherits(try_error, \"error\"))\n                               y = tolower(x)\n                           # result\n                           return(y)\n                       }\n                       # use tryTolower with sapply \n                       sentence = sapply(sentence, tryTolower)\n                       \n                       # split sentence into words with str_split (stringr package)\n                       word.list = str_split(sentence, \"\\\\s+\")\n                       words = unlist(word.list)\n                       \n                       # compare words to the dictionaries of positive & negative terms\n                       pos.matches = match(words, pos.words)\n                       neg.matches = match(words, neg.words)\n                       \n                       # get the position of the matched term or NA\n                       # we just want a TRUE/FALSE\n                       pos.matches = !is.na(pos.matches)\n                       neg.matches = !is.na(neg.matches)\n                       \n                       # final score\n                       score = sum(pos.matches) - sum(neg.matches)\n                       return(score)\n                   }, pos.words, neg.words, .progress=.progress )\n    \n    # data frame with scores for each sentence\n    scores.df = data.frame(text=sentences, score=scores)\n    return(scores.df)\n}\n\n# import positive and negative words\npos = readLines(\"positive_words.txt\")\nneg = readLines(\"negative_words.txt\")\n\n# apply function score.sentiment\nscores = score.sentiment(text, pos, neg, .progress='text')\n\n# add variables to data frame\nscores$very.pos = as.numeric(scores$score >= 2)\nscores$very.neg = as.numeric(scores$score <= -2)\n\n# how many very positives and very negatives\nnumpos = sum(scores$very.pos)\nnumneg = sum(scores$very.neg)\n\n# global score\nglobal_score = round( 100 * numpos / (numpos + numneg) )\n\n# boxplot\nggplot(scores, aes(y=score))+\n    geom_boxplot(aes(fill=drink))\n\n# write csv / view comments and score\nwrite.csv(scores, \"censusCommentsSentiment.csv\", row.names = F)\n\n# Sentiment Analysis (Bayesian approach - not yet tested) ####\nlibrary(sentiment)\n\nsome_txt <- text\n\n# remove retweet entities\nsome_txt = gsub(\"(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)\", \"\", some_txt)\n# remove at people\nsome_txt = gsub(\"@\\\\w+\", \"\", some_txt)\n# remove punctuation\nsome_txt = gsub(\"[[:punct:]]\", \"\", some_txt)\n# remove numbers\nsome_txt = gsub(\"[[:digit:]]\", \"\", some_txt)\n# remove html links\nsome_txt = gsub(\"http\\\\w+\", \"\", some_txt)\n# remove unnecessary spaces\nsome_txt = gsub(\"[ \\t]{2,}\", \"\", some_txt)\nsome_txt = gsub(\"^\\\\s+|\\\\s+$\", \"\", some_txt)\n\n# define \"tolower error handling\" function \ntry.error = function(x)\n{\n    # create missing value\n    y = NA\n    # tryCatch error\n    try_error = tryCatch(tolower(x), error=function(e) e)\n    # if not an error\n    if (!inherits(try_error, \"error\"))\n        y = tolower(x)\n    # result\n    return(y)\n}\n# lower case using try.error with sapply \nsome_txt = sapply(some_txt, try.error)\n\n# remove NAs in some_txt\nsome_txt = some_txt[!is.na(some_txt)]\nnames(some_txt) = NULL\n\n# classify emotion\nclass_emo = classify_emotion(some_txt, algorithm=\"bayes\", prior=1.0)\n# get emotion best fit\nemotion = class_emo[,7]\n# substitute NA's by \"unknown\"\nemotion[is.na(emotion)] = \"unknown\"\n\n# classify polarity\nclass_pol = classify_polarity(some_txt, algorithm=\"bayes\")\n# get polarity best fit\npolarity = class_pol[,4]","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2015-07-31 05:28:59","created_by":"1","updated_at":"2015-07-31 05:59:18","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dce1","uuid":"0cba53a2-c47d-4eca-973e-00d9a1c87571","title":"rate of marriage across aussie cohort","slug":"rate-of-marriage-across-aussie-cohort","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"do some analysis\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>do some analysis</p>\n<!--kg-card-end: markdown-->","comment_id":"9","plaintext":"do some analysis","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2015-08-11 10:58:26","created_by":"1","updated_at":"2015-08-11 10:58:31","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dce2","uuid":"f7323f7a-3e3d-44ee-ae1b-49806a28371d","title":"BI, JG, Automation","slug":"bi-jg-automation","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Luxoury Communism\\nhttp://www.theguardian.com/sustainable-business/2015/mar/18/fully-automated-luxury-communism-robots-employment\\n\\n\\nhttps://medium.com/@simon.sarris/after-universal-basic-income-the-flood-217db9889c07\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Luxoury Communism<br>\n<a href=\"http://www.theguardian.com/sustainable-business/2015/mar/18/fully-automated-luxury-communism-robots-employment\">http://www.theguardian.com/sustainable-business/2015/mar/18/fully-automated-luxury-communism-robots-employment</a></p>\n<p><a href=\"https://medium.com/@simon.sarris/after-universal-basic-income-the-flood-217db9889c07\">https://medium.com/@simon.sarris/after-universal-basic-income-the-flood-217db9889c07</a></p>\n<!--kg-card-end: markdown-->","comment_id":"10","plaintext":"Luxoury Communism\nhttp://www.theguardian.com/sustainable-business/2015/mar/18/fully-automated-luxury-communism-robots-employment\n\nhttps://medium.com/@simon.sarris/after-universal-basic-income-the-flood-217db9889c07","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2015-08-13 00:07:33","created_by":"1","updated_at":"2017-10-23 08:01:49","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dce3","uuid":"0875c00b-00b6-45c2-84e8-eb724ed059ee","title":"Automated trading platforms","slug":"automated-trading-platforms","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}","html":null,"comment_id":"11","plaintext":null,"feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2015-10-23 04:10:36","created_by":"1","updated_at":"2015-10-23 04:10:36","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dce4","uuid":"25436ad5-f9bc-466c-81b8-97d202738fca","title":"Micromorts","slug":"micromorts","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}","html":null,"comment_id":"12","plaintext":null,"feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2015-11-24 04:19:43","created_by":"1","updated_at":"2015-11-24 04:19:43","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dce5","uuid":"d2bfc2fe-f452-498b-89b2-a1d6bc5ed220","title":"A Review of Coursera's Data Science Specialisation","slug":"data-science-specialisation-coursera-a-short-review","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"After using `R` for a couple of years, I decided it was time to learn in a more systematic way and hopefully take my skills to the next level. As thinking _'Coding, where the hell do I start?'_ is not too distant a memory for me, I will keep the begginer in mind.\\n\\nA question I'll answer at the start, _\\\"Is it worth paying for the specialisation?\\\"_ Yes! The course material is free and you can (as I have previously) view the videos and lectures (you can even use [coursera-dl](https://github.com/coursera-dl/coursera-dl) to automatically download all the material). All the benefits without the costs! Well, I am human and my approach has \\n\\n###1. The Data Scientist’s Toolbox\\nIf you have used `git` or another version control system (e.g. `bzr`) before this course can literally be completed within an hour - perhaps explaining why it is the cheapest course of the specialisation. For beginners however, version control can be a hard and abstract concept to get your head around, this course does a good job of teaching the fundamentals and making the user familiar with the basics of creating a repository of code and `commit`/`push` process. These fundamentals will be used when submitting your assignments for the rest of this specialisation.\\n\\nTwo extra resources I found incredibly useful: Firstly, this diagram of common git commands and workflow:\\n{<1>}![](http://assets.osteele.com/images/2008/git-transport.png)\\n\\nAnd secondly, this XKCD accurate portrayal of git. I experienced a git error, spent way too long trying to solve it when really the best thing to do is save elsewhere -> delete local repo -> re-clone remote repo.\\n{<2>}![](http://imgs.xkcd.com/comics/git.png)\\n\\n###2. R Programming\\nThe difficulty level increases for this course. Even though I have been using R for some time now, this course pushed my understanding of R as a brilliant data analysis language to treating it more like a _programming_ language. If you aren't familiar with programming this course will catapault you through common programming concepts like data structures, boolean logic and control structures. Perhaps the most challenging and abstract concept covered is lexical scoping. Googling \\\"How to do what you're trying to do*** in r?\\\"\\n\\n###3. Getting and Cleaning data\\n\\n###4. Exploratory Data Analysis\\n###5. Reproducible Research\\n###6. Statistical Inference\\nBrian Caffo enters the fold and, unfortunately I have to go quite hard on him. He is incredibly monotone, clearly is reading straight from lecture slides with little in the way of value add and generally falls into the 'how statistics is generally taught badly' category.\\n\\n###7. Regression Models\\nBrian Caffo again! Ok, so his performance picks up in this course where he frequently switches from a slightly more lively delivery of slides c.f. the Statistical Inference course to practical code models. Mathmatical proofs/technical details are included for those inclined and clearly identified as optional. \\nSomething I've picked up particularly in this course, is Brian Caffo's code style, it's poor. He frequently uses `=` rather than `<-` as the assignment operator and includes what should be mulitple lines of code on the one line. I have three sources I'd recommend for R code style. First, [Google's R style](https://google.github.io/styleguide/Rguide.xml) which is very comprehensive. Second, the wise sage of `R`, Hadley Wickham, has [a 'few tweaks'  on Google's guide](http://adv-r.had.co.nz/Style.html). The third, is Data Scientist [Graham William's guide to style](http://handsondatascience.com/StyleO.pdf) which is less prescriptive and offers 'contentious' alternatives that logically aren't *bad* they just aren't the most used in the R community.  \\n\\n###8. Practical Machine Learning\\nThis course does a great job at introducing machine learning and keeps the delivery focused on the practice which I think if anyone is put off complexity assumed in the term 'machine learning' this is the course to take. Jeff Leek is the lecturer and his delivery is consistently engaging throughout the course. As the name suggests, the course is focused on the practice of machine learning. The lectures take you through worked examples and explain the reasons behind the methods used in machine learning. There are some mathmatical explanations of the methods but if you have sat through Brian Caffo's Statistical Inference, this course will seem like a breeze. There are no `swirl()` courses for machine learning which I think would be a helpful addition. Otherwise, this course is a great introduction into machine learning and should make anyone dangerous when it comes to supervised learning!\\n\\n###9. Data Products\\nBack to Brian Caffo for this course, though he makes use of [goanimate](https://goanimate.com/) videos which are perhaps a little less stilted. Week 1 starts off by introducing Shiny along with [`rCharts`](http://rcharts.io/) and [`googlVis`](https://cran.r-project.org/web/packages/googleVis/vignettes/googleVis_examples.html). I have used shiny a lot and as a consequence have a few criticisms of the first week. The former, despite having being around for at least a couple of years never made it on to [CRAN](https://cran.r-project.org/). Ramnath the talented author behind rCharts seems to have shifted his efforts towards htmlwidgets. In fact, a quick look at the [github stats for rChart](https://github.com/ramnathv/rCharts/graphs/contributors) reveals there hasn't been active development on rCharts for at least two years. The plotly lecture is also out of date, with reference to content that does't exist anymore and advice to install the package from github rather than CRAN.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>After using <code>R</code> for a couple of years, I decided it was time to learn in a more systematic way and hopefully take my skills to the next level. As thinking <em>'Coding, where the hell do I start?'</em> is not too distant a memory for me, I will keep the begginer in mind.</p>\n<p>A question I'll answer at the start, <em>&quot;Is it worth paying for the specialisation?&quot;</em> Yes! The course material is free and you can (as I have previously) view the videos and lectures (you can even use <a href=\"https://github.com/coursera-dl/coursera-dl\">coursera-dl</a> to automatically download all the material). All the benefits without the costs! Well, I am human and my approach has</p>\n<h3 id=\"1thedatascientiststoolbox\">1. The Data Scientist’s Toolbox</h3>\n<p>If you have used <code>git</code> or another version control system (e.g. <code>bzr</code>) before this course can literally be completed within an hour - perhaps explaining why it is the cheapest course of the specialisation. For beginners however, version control can be a hard and abstract concept to get your head around, this course does a good job of teaching the fundamentals and making the user familiar with the basics of creating a repository of code and <code>commit</code>/<code>push</code> process. These fundamentals will be used when submitting your assignments for the rest of this specialisation.</p>\n<p>Two extra resources I found incredibly useful: Firstly, this diagram of common git commands and workflow:<br>\n{&lt;1&gt;}<img src=\"http://assets.osteele.com/images/2008/git-transport.png\" alt=\"\"></p>\n<p>And secondly, this XKCD accurate portrayal of git. I experienced a git error, spent way too long trying to solve it when really the best thing to do is save elsewhere -&gt; delete local repo -&gt; re-clone remote repo.<br>\n{&lt;2&gt;}<img src=\"http://imgs.xkcd.com/comics/git.png\" alt=\"\"></p>\n<h3 id=\"2rprogramming\">2. R Programming</h3>\n<p>The difficulty level increases for this course. Even though I have been using R for some time now, this course pushed my understanding of R as a brilliant data analysis language to treating it more like a <em>programming</em> language. If you aren't familiar with programming this course will catapault you through common programming concepts like data structures, boolean logic and control structures. Perhaps the most challenging and abstract concept covered is lexical scoping. Googling &quot;How to do what you're trying to do*** in r?&quot;</p>\n<h3 id=\"3gettingandcleaningdata\">3. Getting and Cleaning data</h3>\n<h3 id=\"4exploratorydataanalysis\">4. Exploratory Data Analysis</h3>\n<h3 id=\"5reproducibleresearch\">5. Reproducible Research</h3>\n<h3 id=\"6statisticalinference\">6. Statistical Inference</h3>\n<p>Brian Caffo enters the fold and, unfortunately I have to go quite hard on him. He is incredibly monotone, clearly is reading straight from lecture slides with little in the way of value add and generally falls into the 'how statistics is generally taught badly' category.</p>\n<h3 id=\"7regressionmodels\">7. Regression Models</h3>\n<p>Brian Caffo again! Ok, so his performance picks up in this course where he frequently switches from a slightly more lively delivery of slides c.f. the Statistical Inference course to practical code models. Mathmatical proofs/technical details are included for those inclined and clearly identified as optional.<br>\nSomething I've picked up particularly in this course, is Brian Caffo's code style, it's poor. He frequently uses <code>=</code> rather than <code>&lt;-</code> as the assignment operator and includes what should be mulitple lines of code on the one line. I have three sources I'd recommend for R code style. First, <a href=\"https://google.github.io/styleguide/Rguide.xml\">Google's R style</a> which is very comprehensive. Second, the wise sage of <code>R</code>, Hadley Wickham, has <a href=\"http://adv-r.had.co.nz/Style.html\">a 'few tweaks'  on Google's guide</a>. The third, is Data Scientist <a href=\"http://handsondatascience.com/StyleO.pdf\">Graham William's guide to style</a> which is less prescriptive and offers 'contentious' alternatives that logically aren't <em>bad</em> they just aren't the most used in the R community.</p>\n<h3 id=\"8practicalmachinelearning\">8. Practical Machine Learning</h3>\n<p>This course does a great job at introducing machine learning and keeps the delivery focused on the practice which I think if anyone is put off complexity assumed in the term 'machine learning' this is the course to take. Jeff Leek is the lecturer and his delivery is consistently engaging throughout the course. As the name suggests, the course is focused on the practice of machine learning. The lectures take you through worked examples and explain the reasons behind the methods used in machine learning. There are some mathmatical explanations of the methods but if you have sat through Brian Caffo's Statistical Inference, this course will seem like a breeze. There are no <code>swirl()</code> courses for machine learning which I think would be a helpful addition. Otherwise, this course is a great introduction into machine learning and should make anyone dangerous when it comes to supervised learning!</p>\n<h3 id=\"9dataproducts\">9. Data Products</h3>\n<p>Back to Brian Caffo for this course, though he makes use of <a href=\"https://goanimate.com/\">goanimate</a> videos which are perhaps a little less stilted. Week 1 starts off by introducing Shiny along with <a href=\"http://rcharts.io/\"><code>rCharts</code></a> and <a href=\"https://cran.r-project.org/web/packages/googleVis/vignettes/googleVis_examples.html\"><code>googlVis</code></a>. I have used shiny a lot and as a consequence have a few criticisms of the first week. The former, despite having being around for at least a couple of years never made it on to <a href=\"https://cran.r-project.org/\">CRAN</a>. Ramnath the talented author behind rCharts seems to have shifted his efforts towards htmlwidgets. In fact, a quick look at the <a href=\"https://github.com/ramnathv/rCharts/graphs/contributors\">github stats for rChart</a> reveals there hasn't been active development on rCharts for at least two years. The plotly lecture is also out of date, with reference to content that does't exist anymore and advice to install the package from github rather than CRAN.</p>\n<!--kg-card-end: markdown-->","comment_id":"13","plaintext":"After using R for a couple of years, I decided it was time to learn in a more\nsystematic way and hopefully take my skills to the next level. As thinking \n'Coding, where the hell do I start?' is not too distant a memory for me, I will\nkeep the begginer in mind.\n\nA question I'll answer at the start, \"Is it worth paying for the\nspecialisation?\" Yes! The course material is free and you can (as I have\npreviously) view the videos and lectures (you can even use coursera-dl\n[https://github.com/coursera-dl/coursera-dl] to automatically download all the\nmaterial). All the benefits without the costs! Well, I am human and my approach\nhas\n\n1. The Data Scientist’s Toolbox\nIf you have used git or another version control system (e.g. bzr) before this\ncourse can literally be completed within an hour - perhaps explaining why it is\nthe cheapest course of the specialisation. For beginners however, version\ncontrol can be a hard and abstract concept to get your head around, this course\ndoes a good job of teaching the fundamentals and making the user familiar with\nthe basics of creating a repository of code and commit/push process. These\nfundamentals will be used when submitting your assignments for the rest of this\nspecialisation.\n\nTwo extra resources I found incredibly useful: Firstly, this diagram of common\ngit commands and workflow:\n{<1>}\n\nAnd secondly, this XKCD accurate portrayal of git. I experienced a git error,\nspent way too long trying to solve it when really the best thing to do is save\nelsewhere -> delete local repo -> re-clone remote repo.\n{<2>}\n\n2. R Programming\nThe difficulty level increases for this course. Even though I have been using R\nfor some time now, this course pushed my understanding of R as a brilliant data\nanalysis language to treating it more like a programming language. If you aren't\nfamiliar with programming this course will catapault you through common\nprogramming concepts like data structures, boolean logic and control structures.\nPerhaps the most challenging and abstract concept covered is lexical scoping.\nGoogling \"How to do what you're trying to do*** in r?\"\n\n3. Getting and Cleaning data\n4. Exploratory Data Analysis\n5. Reproducible Research\n6. Statistical Inference\nBrian Caffo enters the fold and, unfortunately I have to go quite hard on him.\nHe is incredibly monotone, clearly is reading straight from lecture slides with\nlittle in the way of value add and generally falls into the 'how statistics is\ngenerally taught badly' category.\n\n7. Regression Models\nBrian Caffo again! Ok, so his performance picks up in this course where he\nfrequently switches from a slightly more lively delivery of slides c.f. the\nStatistical Inference course to practical code models. Mathmatical\nproofs/technical details are included for those inclined and clearly identified\nas optional.\nSomething I've picked up particularly in this course, is Brian Caffo's code\nstyle, it's poor. He frequently uses = rather than <- as the assignment operator\nand includes what should be mulitple lines of code on the one line. I have three\nsources I'd recommend for R code style. First, Google's R style\n[https://google.github.io/styleguide/Rguide.xml] which is very comprehensive.\nSecond, the wise sage of R, Hadley Wickham, has a 'few tweaks' on Google's guide\n[http://adv-r.had.co.nz/Style.html]. The third, is Data Scientist Graham\nWilliam's guide to style [http://handsondatascience.com/StyleO.pdf] which is\nless prescriptive and offers 'contentious' alternatives that logically aren't \nbad they just aren't the most used in the R community.\n\n8. Practical Machine Learning\nThis course does a great job at introducing machine learning and keeps the\ndelivery focused on the practice which I think if anyone is put off complexity\nassumed in the term 'machine learning' this is the course to take. Jeff Leek is\nthe lecturer and his delivery is consistently engaging throughout the course. As\nthe name suggests, the course is focused on the practice of machine learning.\nThe lectures take you through worked examples and explain the reasons behind the\nmethods used in machine learning. There are some mathmatical explanations of the\nmethods but if you have sat through Brian Caffo's Statistical Inference, this\ncourse will seem like a breeze. There are no swirl() courses for machine\nlearning which I think would be a helpful addition. Otherwise, this course is a\ngreat introduction into machine learning and should make anyone dangerous when\nit comes to supervised learning!\n\n9. Data Products\nBack to Brian Caffo for this course, though he makes use of goanimate\n[https://goanimate.com/] videos which are perhaps a little less stilted. Week 1\nstarts off by introducing Shiny along with rCharts [http://rcharts.io/] and \ngooglVis\n[https://cran.r-project.org/web/packages/googleVis/vignettes/googleVis_examples.html]\n. I have used shiny a lot and as a consequence have a few criticisms of the\nfirst week. The former, despite having being around for at least a couple of\nyears never made it on to CRAN [https://cran.r-project.org/]. Ramnath the\ntalented author behind rCharts seems to have shifted his efforts towards\nhtmlwidgets. In fact, a quick look at the github stats for rChart\n[https://github.com/ramnathv/rCharts/graphs/contributors] reveals there hasn't\nbeen active development on rCharts for at least two years. The plotly lecture is\nalso out of date, with reference to content that does't exist anymore and advice\nto install the package from github rather than CRAN.","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2016-03-01 00:35:17","created_by":"1","updated_at":"2016-07-17 04:20:52","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dce6","uuid":"65f34085-e32d-4851-a80b-afc18b470a03","title":"Map of Australia Using OpenStreetMaps, PSMA, R and Leaflet.js","slug":"map-of-australia-using-osm-psma-and-shiny","mobiledoc":"{\"version\":\"0.3.2\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"**Aim:** To use GNAF (Geocoded National Address File) data to display the adminstrative boundaries (e.g. States) on top of Open Street Maps. Additionally, I want the user when hovering over a state to see summary statistics. I want an Australian version of [this](#leafletjs).\\n\\n## Data \\nG-NAF, is data containing all Australian addresses with geocoding, shape files for boundaries of states, electorates, municipals. It is the geocoding of a nation. And finally was released on data.gov.au this year. [Here's a nice GNAF data overview.](https://docs.google.com/viewer?url=http%3A%2F%2Fminus34.com%2Fopendata%2Fintro-to-gnaf.pptx&embedded=true&chrome=false&dov=1) N.B. One set of data that is missing from the release is the [postcode boundaries](https://www.psma.com.au/products/postcode-boundaries). \\n\\n### State Boundaries\\nMy initial geojson to get started was sourced from this [github repo](https://raw.githubusercontent.com/edwinsteele/d3-projects/master/data/au-states.geojson). \\n\\n### Postcode Boundaries\\nI obtained [the shape file for Australian postcodes](http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.003July%202011?OpenDocument) from the Australian Bureau of Statistics. I then converted the shapefile to geojson (helpful tutorial [here](http://ben.balter.com/2013/06/26/how-to-convert-shapefiles-to-geojson-for-use-on-github/)):\\n\\n```bash\\n# Convert shp file to geojson\\n$ ogr2ogr -f GeoJSON -t_srs crs:84 au-postcodes.geojson POA_2011_AUST.shp\\n```\\n\\nThe resulting geojson is a whooping 201.1mb. There are two more processes to consider. Firstly, simplification of the polygons to reduce the load on the browser. The most popular algorithm to use for this is the [Ramer–Douglas–Peucker algorithm](https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm). In R have a look at [`gSimplify()`](http://www.inside-r.org/packages/cran/rgeos/docs/gSimplify) however I am going to use the [mapshaper command line tool](https://github.com/mbloch/mapshaper) as I prefer command line tools and mapshaper has a couple of implementations of Visvalingam's polygon simplification algorithm.\\n\\n```bash\\n# Install package\\n$ npm install -g mapshaper\\n\\n# Run Visvalingam simplification retaining 10% of polygons\\n$ mapshaper -i au-postcodes.geojson -simplify 10% -o au-postcodes-Visvalingam-0.1.geojson\\n```\\n\\nThe size of the `au-postcodes-Visvalingam-0.1.geojson` file is about 10% of the size of `au-postcodes.geojson` file.\\n\\n![Polygon Simplification](/content/images/2017/08/polygon-simplification.gif)\\n\\nSecondly, we may want to reduce the file size by 'minifying' or 'packing' the data by removing all spaces and new lines. This can be done using [geojson-minify](https://github.com/igorti/geojson-minifier). Packing the `au-postcodes.geojson` file from above with the command below reduced the file size to 19.1mb.\\n\\n```bash\\n$ npm install -g geojson-minifier\\n$ geojson-minifier -o pack -f au-postcodes.geojson\\n```\\n\\n## Maps\\nSo `library(rgdal)` seems pretty essential but if you're using a mac like I am, here you run into some pain. If you try running `install.packages('rgdal')` it will exit with a status of 0. Installing the underlying `gdal` dependency fixes the problem.\\n\\n```bash\\n$ brew install gdal\\n```\\n\\nAfter installing gdal, `install.packages('rgdal')` ran just fine.\\n\\n## Adding Data to the Shapefile\\nI used R for this process, `rgdal` allows you to do joins on shape files so you can attribute data to the geographical areas.\\n\\n```r\\nlibrary(rgdal)      # Spatial data processing\\nlibrary(jsonlite)   # Read json files\\nlibrary(readr)      # Fast I/O\\n\\n# Postcode shape json\\ngdal.postcodes <- readOGR(\\\"au-postcodes-Visvalingam-0.1-density.geojson\\\", \\\"OGRGeoJSON\\\")\\n\\n# Read data you want to join to the shape file\\ndf <- read_csv(\\\"your_data.csv\\\")\\n\\n# Change to character for leading 0s in NT postcodes\\ndf$cleaned_postcode <- sprintf(\\\"%04d\\\", df$postcode)\\n\\n# Join agg values onto postcode (returns)\\ngdal.postcodes@data <- sp::merge(gdal.postcodes@data, df , by.x = \\\"POA_NAME\\\", by.y = \\\"cleaned_postcode\\\", all.x = TRUE)\\n\\n# Write to a new file\\nwriteOGR(gdal.postcodes, 'au-postcodes-Visvalingam-0.1-density.geojson','spDf', driver = 'GeoJSON', check_exists = FALSE)\\n```\\n\\n## Leaflet.js\\nTo display the information in a standalone single page app I started with this [leaflet choropleth example](http://leafletjs.com/examples/choropleth-example.html).\\n\\n<iframe\\nwidth=\\\"820\\\" height=\\\"520\\\"\\nsrc=\\\"https://leafletjs.com/examples/choropleth/example.html#map\\\" >\\n</iframe>\\nThere's a few things to customise to an Australian version:\\n\\n### 1. Getting the Data into the Webpage\\nI modified the page to read `au-postcodes-Visvalingam-0.1-density.js`. You can't just read a `.json` file in the head of your page, for options on how to get data to load on your page see this [helpful SO answer](http://stackoverflow.com/a/13576588/3691003). I went with option two, I opened the `au-postcodes-Visvalingam-0.1-density.geosjson` and inserted `var postcodesData =` at the beginning of the data array and put a `;` to end the variable declaration. \\n\\n```js\\nvar postcodesData = {\\n...\\n};\\n```\\n\\nThen save as `.js`. Now you can read the `au-postcodes-Visvalingam-0.1-density.js` file in the `head` of your html.\\n\\n```html\\n<head>\\n<!-- load geojson data as \\\"postcodeData\\\" -->\\n<script src=\\\"au-postcodes-Visvalingam-0.1-density.js\\\"></script>\\n...\\n</head>\\n```\\n\\nAt the bottom of your page you will need to ensure the `postcodesData` is referenced in the declaration of the `geojson` variable. This is also when the leaflet functions defining style and hover events are pointed to the postcode shapes.\\n\\n```js\\n// add all options to the polygons\\ngeojson = L.geoJson(postcodesData, {\\n\\tstyle: style,\\n\\tonEachFeature: onEachFeature\\n}).addTo(map);\\n```\\n\\n### 2. Setting Initial Location and Zoom on Page Load\\nTo open on Australia instead of the US the key piece of code you need to set is the latitude and longitude coordinates along with the zoom level. This required a little bit of playing around to get the desired look. N.b I set the zoom level to 4 in the code below, don't try finely tuning with decimal places as tiles are only available for integers. I learnt the hard way!\\n\\n```js\\n// set up the map\\nvar map = L.map('map').setView([-27.833, 133.583], 4);\\n```\\n\\n### 3. Dynamically Fit the Map to Fill the Device Screen Size\\nThere is one issue with setting the zoom, since the zoom of the map is fixed and the device size is variable depending on the device being used. For example, if you set the zoom using your desktop screen, then loaded the page on a mobile device with a fraction of the screen real estate you will only see a fraction of Australia.\\n\\n#### a. Set the Viewport\\n[Check out W3's intro to the viewport](http://www.w3schools.com/css/css_rwd_viewport.asp). Included the following in the head of your html.\\n```\\n<meta name=\\\"viewport\\\" content=\\\"width=device-width, height=device-height, initial-scale=1.0\\\"/>\\n```\\n\\n#### b. Change the #map Styling\\nThe leaflet choropleth example has the following styling applied to the `div` map:\\n```\\n#map { width: 800px; height: 500px; }\\n```\\nThis is a fixed size, we want to change this to be relative. The following will fit to any screensize\\n```\\n#map { position: absolute; top: 0; right: 0; bottom: 0; left: 0; }\\n```\\n\\n#### c. Set Relative Zoom\\nThe steps before fix the filling of the screen. So now we have variable screen fill and fixed zoom. What we want is variable screen fill and variable zoom. That way, leaflet will automatically zoom to the maximum it can whilst keeping all of Australia visible on the device. Thankfully, leaflet provides a function for this, `fitbounds`. We need to change this:\\n\\n```js\\n// set up the map\\nvar map = L.map('map').setView([-27.833, 133.583], 4);\\n```\\n\\nIf you imagine a box containing Australia, then draw a diagonal through that box, the latitude and longitude of the diagnoal line's endpoints are what the `fitBounds()` constructs as a container from. I picked a point at the south easter tip of Tasmania and the other to the north west of Western Australia. Here's the code:\\n\\n```js\\n// set up the map\\nvar map = L.map('map').setView([-27.833, 133.583]).fitBounds(\\n[ [-43.21, 147.8], [-12.5, 117.2] ]\\n);\\n```\\n\\nIf you've found this content helpful why not...\\n\\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\\n  <link href=\\\"https://fonts.googleapis.com/css?family=Cookie\\\" rel=\\\"stylesheet\\\"><a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\\n\\n### Other Useful Resources Encountered\\n\\n- [Shapefiles for all countries](http://gadm.org/country)\\n\\n- [Creating maps in R](https://github.com/Robinlovelace/Creating-maps-in-R/blob/master/vignettes/geoJSON.Rmd)\\n\\n- [Useful leaflet Github comments](https://github.com/rstudio/leaflet/issues/55)\\n\\n- [Joe Cheng, as always a pioneeR](http://rpubs.com/jcheng/us-states-2)\\n\\n- [Convert shape file formats](https://github.com/mbostock/topojson/wiki/Command-Line-Reference)\"}]],\"markups\":[],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><strong>Aim:</strong> To use GNAF (Geocoded National Address File) data to display the adminstrative boundaries (e.g. States) on top of Open Street Maps. Additionally, I want the user when hovering over a state to see summary statistics. I want an Australian version of <a href=\"#leafletjs\">this</a>.</p>\n<h2 id=\"data\">Data</h2>\n<p>G-NAF, is data containing all Australian addresses with geocoding, shape files for boundaries of states, electorates, municipals. It is the geocoding of a nation. And finally was released on data.gov.au this year. <a href=\"https://docs.google.com/viewer?url=http%3A%2F%2Fminus34.com%2Fopendata%2Fintro-to-gnaf.pptx&amp;embedded=true&amp;chrome=false&amp;dov=1\">Here's a nice GNAF data overview.</a> N.B. One set of data that is missing from the release is the <a href=\"https://www.psma.com.au/products/postcode-boundaries\">postcode boundaries</a>.</p>\n<h3 id=\"stateboundaries\">State Boundaries</h3>\n<p>My initial geojson to get started was sourced from this <a href=\"https://raw.githubusercontent.com/edwinsteele/d3-projects/master/data/au-states.geojson\">github repo</a>.</p>\n<h3 id=\"postcodeboundaries\">Postcode Boundaries</h3>\n<p>I obtained <a href=\"http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.003July%202011?OpenDocument\">the shape file for Australian postcodes</a> from the Australian Bureau of Statistics. I then converted the shapefile to geojson (helpful tutorial <a href=\"http://ben.balter.com/2013/06/26/how-to-convert-shapefiles-to-geojson-for-use-on-github/\">here</a>):</p>\n<pre><code class=\"language-bash\"># Convert shp file to geojson\n$ ogr2ogr -f GeoJSON -t_srs crs:84 au-postcodes.geojson POA_2011_AUST.shp\n</code></pre>\n<p>The resulting geojson is a whooping 201.1mb. There are two more processes to consider. Firstly, simplification of the polygons to reduce the load on the browser. The most popular algorithm to use for this is the <a href=\"https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm\">Ramer–Douglas–Peucker algorithm</a>. In R have a look at <a href=\"http://www.inside-r.org/packages/cran/rgeos/docs/gSimplify\"><code>gSimplify()</code></a> however I am going to use the <a href=\"https://github.com/mbloch/mapshaper\">mapshaper command line tool</a> as I prefer command line tools and mapshaper has a couple of implementations of Visvalingam's polygon simplification algorithm.</p>\n<pre><code class=\"language-bash\"># Install package\n$ npm install -g mapshaper\n\n# Run Visvalingam simplification retaining 10% of polygons\n$ mapshaper -i au-postcodes.geojson -simplify 10% -o au-postcodes-Visvalingam-0.1.geojson\n</code></pre>\n<p>The size of the <code>au-postcodes-Visvalingam-0.1.geojson</code> file is about 10% of the size of <code>au-postcodes.geojson</code> file.</p>\n<p><img src=\"/content/images/2017/08/polygon-simplification.gif\" alt=\"Polygon Simplification\"></p>\n<p>Secondly, we may want to reduce the file size by 'minifying' or 'packing' the data by removing all spaces and new lines. This can be done using <a href=\"https://github.com/igorti/geojson-minifier\">geojson-minify</a>. Packing the <code>au-postcodes.geojson</code> file from above with the command below reduced the file size to 19.1mb.</p>\n<pre><code class=\"language-bash\">$ npm install -g geojson-minifier\n$ geojson-minifier -o pack -f au-postcodes.geojson\n</code></pre>\n<h2 id=\"maps\">Maps</h2>\n<p>So <code>library(rgdal)</code> seems pretty essential but if you're using a mac like I am, here you run into some pain. If you try running <code>install.packages('rgdal')</code> it will exit with a status of 0. Installing the underlying <code>gdal</code> dependency fixes the problem.</p>\n<pre><code class=\"language-bash\">$ brew install gdal\n</code></pre>\n<p>After installing gdal, <code>install.packages('rgdal')</code> ran just fine.</p>\n<h2 id=\"addingdatatotheshapefile\">Adding Data to the Shapefile</h2>\n<p>I used R for this process, <code>rgdal</code> allows you to do joins on shape files so you can attribute data to the geographical areas.</p>\n<pre><code class=\"language-r\">library(rgdal)      # Spatial data processing\nlibrary(jsonlite)   # Read json files\nlibrary(readr)      # Fast I/O\n\n# Postcode shape json\ngdal.postcodes &lt;- readOGR(&quot;au-postcodes-Visvalingam-0.1-density.geojson&quot;, &quot;OGRGeoJSON&quot;)\n\n# Read data you want to join to the shape file\ndf &lt;- read_csv(&quot;your_data.csv&quot;)\n\n# Change to character for leading 0s in NT postcodes\ndf$cleaned_postcode &lt;- sprintf(&quot;%04d&quot;, df$postcode)\n\n# Join agg values onto postcode (returns)\ngdal.postcodes@data &lt;- sp::merge(gdal.postcodes@data, df , by.x = &quot;POA_NAME&quot;, by.y = &quot;cleaned_postcode&quot;, all.x = TRUE)\n\n# Write to a new file\nwriteOGR(gdal.postcodes, 'au-postcodes-Visvalingam-0.1-density.geojson','spDf', driver = 'GeoJSON', check_exists = FALSE)\n</code></pre>\n<h2 id=\"leafletjs\">Leaflet.js</h2>\n<p>To display the information in a standalone single page app I started with this <a href=\"http://leafletjs.com/examples/choropleth-example.html\">leaflet choropleth example</a>.</p>\n<iframe\nwidth=\"820\" height=\"520\"\nsrc=\"https://leafletjs.com/examples/choropleth/example.html#map\" >\n</iframe>\nThere's a few things to customise to an Australian version:\n<h3 id=\"1gettingthedataintothewebpage\">1. Getting the Data into the Webpage</h3>\n<p>I modified the page to read <code>au-postcodes-Visvalingam-0.1-density.js</code>. You can't just read a <code>.json</code> file in the head of your page, for options on how to get data to load on your page see this <a href=\"http://stackoverflow.com/a/13576588/3691003\">helpful SO answer</a>. I went with option two, I opened the <code>au-postcodes-Visvalingam-0.1-density.geosjson</code> and inserted <code>var postcodesData =</code> at the beginning of the data array and put a <code>;</code> to end the variable declaration.</p>\n<pre><code class=\"language-js\">var postcodesData = {\n...\n};\n</code></pre>\n<p>Then save as <code>.js</code>. Now you can read the <code>au-postcodes-Visvalingam-0.1-density.js</code> file in the <code>head</code> of your html.</p>\n<pre><code class=\"language-html\">&lt;head&gt;\n&lt;!-- load geojson data as &quot;postcodeData&quot; --&gt;\n&lt;script src=&quot;au-postcodes-Visvalingam-0.1-density.js&quot;&gt;&lt;/script&gt;\n...\n&lt;/head&gt;\n</code></pre>\n<p>At the bottom of your page you will need to ensure the <code>postcodesData</code> is referenced in the declaration of the <code>geojson</code> variable. This is also when the leaflet functions defining style and hover events are pointed to the postcode shapes.</p>\n<pre><code class=\"language-js\">// add all options to the polygons\ngeojson = L.geoJson(postcodesData, {\n\tstyle: style,\n\tonEachFeature: onEachFeature\n}).addTo(map);\n</code></pre>\n<h3 id=\"2settinginitiallocationandzoomonpageload\">2. Setting Initial Location and Zoom on Page Load</h3>\n<p>To open on Australia instead of the US the key piece of code you need to set is the latitude and longitude coordinates along with the zoom level. This required a little bit of playing around to get the desired look. N.b I set the zoom level to 4 in the code below, don't try finely tuning with decimal places as tiles are only available for integers. I learnt the hard way!</p>\n<pre><code class=\"language-js\">// set up the map\nvar map = L.map('map').setView([-27.833, 133.583], 4);\n</code></pre>\n<h3 id=\"3dynamicallyfitthemaptofillthedevicescreensize\">3. Dynamically Fit the Map to Fill the Device Screen Size</h3>\n<p>There is one issue with setting the zoom, since the zoom of the map is fixed and the device size is variable depending on the device being used. For example, if you set the zoom using your desktop screen, then loaded the page on a mobile device with a fraction of the screen real estate you will only see a fraction of Australia.</p>\n<h4 id=\"asettheviewport\">a. Set the Viewport</h4>\n<p><a href=\"http://www.w3schools.com/css/css_rwd_viewport.asp\">Check out W3's intro to the viewport</a>. Included the following in the head of your html.</p>\n<pre><code>&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, height=device-height, initial-scale=1.0&quot;/&gt;\n</code></pre>\n<h4 id=\"bchangethemapstyling\">b. Change the #map Styling</h4>\n<p>The leaflet choropleth example has the following styling applied to the <code>div</code> map:</p>\n<pre><code>#map { width: 800px; height: 500px; }\n</code></pre>\n<p>This is a fixed size, we want to change this to be relative. The following will fit to any screensize</p>\n<pre><code>#map { position: absolute; top: 0; right: 0; bottom: 0; left: 0; }\n</code></pre>\n<h4 id=\"csetrelativezoom\">c. Set Relative Zoom</h4>\n<p>The steps before fix the filling of the screen. So now we have variable screen fill and fixed zoom. What we want is variable screen fill and variable zoom. That way, leaflet will automatically zoom to the maximum it can whilst keeping all of Australia visible on the device. Thankfully, leaflet provides a function for this, <code>fitbounds</code>. We need to change this:</p>\n<pre><code class=\"language-js\">// set up the map\nvar map = L.map('map').setView([-27.833, 133.583], 4);\n</code></pre>\n<p>If you imagine a box containing Australia, then draw a diagonal through that box, the latitude and longitude of the diagnoal line's endpoints are what the <code>fitBounds()</code> constructs as a container from. I picked a point at the south easter tip of Tasmania and the other to the north west of Western Australia. Here's the code:</p>\n<pre><code class=\"language-js\">// set up the map\nvar map = L.map('map').setView([-27.833, 133.583]).fitBounds(\n[ [-43.21, 147.8], [-12.5, 117.2] ]\n);\n</code></pre>\n<p>If you've found this content helpful why not...</p>\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\n  <link href=\"https://fonts.googleapis.com/css?family=Cookie\" rel=\"stylesheet\"><a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a>\n<h3 id=\"otherusefulresourcesencountered\">Other Useful Resources Encountered</h3>\n<ul>\n<li>\n<p><a href=\"http://gadm.org/country\">Shapefiles for all countries</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/Robinlovelace/Creating-maps-in-R/blob/master/vignettes/geoJSON.Rmd\">Creating maps in R</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/rstudio/leaflet/issues/55\">Useful leaflet Github comments</a></p>\n</li>\n<li>\n<p><a href=\"http://rpubs.com/jcheng/us-states-2\">Joe Cheng, as always a pioneeR</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/mbostock/topojson/wiki/Command-Line-Reference\">Convert shape file formats</a></p>\n</li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"14","plaintext":"Aim: To use GNAF (Geocoded National Address File) data to display the\nadminstrative boundaries (e.g. States) on top of Open Street Maps. Additionally,\nI want the user when hovering over a state to see summary statistics. I want an\nAustralian version of this.\n\nData\nG-NAF, is data containing all Australian addresses with geocoding, shape files\nfor boundaries of states, electorates, municipals. It is the geocoding of a\nnation. And finally was released on data.gov.au this year. Here's a nice GNAF\ndata overview.\n[https://docs.google.com/viewer?url=http%3A%2F%2Fminus34.com%2Fopendata%2Fintro-to-gnaf.pptx&embedded=true&chrome=false&dov=1] \nN.B. One set of data that is missing from the release is the postcode boundaries\n[https://www.psma.com.au/products/postcode-boundaries].\n\nState Boundaries\nMy initial geojson to get started was sourced from this github repo\n[https://raw.githubusercontent.com/edwinsteele/d3-projects/master/data/au-states.geojson]\n.\n\nPostcode Boundaries\nI obtained the shape file for Australian postcodes\n[http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.003July%202011?OpenDocument] \nfrom the Australian Bureau of Statistics. I then converted the shapefile to\ngeojson (helpful tutorial here\n[http://ben.balter.com/2013/06/26/how-to-convert-shapefiles-to-geojson-for-use-on-github/]\n):\n\n# Convert shp file to geojson\n$ ogr2ogr -f GeoJSON -t_srs crs:84 au-postcodes.geojson POA_2011_AUST.shp\n\n\nThe resulting geojson is a whooping 201.1mb. There are two more processes to\nconsider. Firstly, simplification of the polygons to reduce the load on the\nbrowser. The most popular algorithm to use for this is the \nRamer–Douglas–Peucker\nalgorithm\n[https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm].\nIn R have a look at gSimplify()\n[http://www.inside-r.org/packages/cran/rgeos/docs/gSimplify] however I am going\nto use the mapshaper command line tool [https://github.com/mbloch/mapshaper] as\nI prefer command line tools and mapshaper has a couple of implementations of\nVisvalingam's polygon simplification algorithm.\n\n# Install package\n$ npm install -g mapshaper\n\n# Run Visvalingam simplification retaining 10% of polygons\n$ mapshaper -i au-postcodes.geojson -simplify 10% -o au-postcodes-Visvalingam-0.1.geojson\n\n\nThe size of the au-postcodes-Visvalingam-0.1.geojson file is about 10% of the\nsize of au-postcodes.geojson file.\n\n\n\nSecondly, we may want to reduce the file size by 'minifying' or 'packing' the\ndata by removing all spaces and new lines. This can be done using geojson-minify\n[https://github.com/igorti/geojson-minifier]. Packing the au-postcodes.geojson \nfile from above with the command below reduced the file size to 19.1mb.\n\n$ npm install -g geojson-minifier\n$ geojson-minifier -o pack -f au-postcodes.geojson\n\n\nMaps\nSo library(rgdal) seems pretty essential but if you're using a mac like I am,\nhere you run into some pain. If you try running install.packages('rgdal') it\nwill exit with a status of 0. Installing the underlying gdal dependency fixes\nthe problem.\n\n$ brew install gdal\n\n\nAfter installing gdal, install.packages('rgdal') ran just fine.\n\nAdding Data to the Shapefile\nI used R for this process, rgdal allows you to do joins on shape files so you\ncan attribute data to the geographical areas.\n\nlibrary(rgdal)      # Spatial data processing\nlibrary(jsonlite)   # Read json files\nlibrary(readr)      # Fast I/O\n\n# Postcode shape json\ngdal.postcodes <- readOGR(\"au-postcodes-Visvalingam-0.1-density.geojson\", \"OGRGeoJSON\")\n\n# Read data you want to join to the shape file\ndf <- read_csv(\"your_data.csv\")\n\n# Change to character for leading 0s in NT postcodes\ndf$cleaned_postcode <- sprintf(\"%04d\", df$postcode)\n\n# Join agg values onto postcode (returns)\ngdal.postcodes@data <- sp::merge(gdal.postcodes@data, df , by.x = \"POA_NAME\", by.y = \"cleaned_postcode\", all.x = TRUE)\n\n# Write to a new file\nwriteOGR(gdal.postcodes, 'au-postcodes-Visvalingam-0.1-density.geojson','spDf', driver = 'GeoJSON', check_exists = FALSE)\n\n\nLeaflet.js\nTo display the information in a standalone single page app I started with this \nleaflet choropleth example\n[http://leafletjs.com/examples/choropleth-example.html].\n\nThere's a few things to customise to an Australian version:1. Getting the Data\ninto the Webpage\nI modified the page to read au-postcodes-Visvalingam-0.1-density.js. You can't\njust read a .json file in the head of your page, for options on how to get data\nto load on your page see this helpful SO answer\n[http://stackoverflow.com/a/13576588/3691003]. I went with option two, I opened\nthe au-postcodes-Visvalingam-0.1-density.geosjson and inserted var postcodesData\n= at the beginning of the data array and put a ; to end the variable\ndeclaration.\n\nvar postcodesData = {\n...\n};\n\n\nThen save as .js. Now you can read the au-postcodes-Visvalingam-0.1-density.js \nfile in the head of your html.\n\n<head>\n<!-- load geojson data as \"postcodeData\" -->\n<script src=\"au-postcodes-Visvalingam-0.1-density.js\"></script>\n...\n</head>\n\n\nAt the bottom of your page you will need to ensure the postcodesData is\nreferenced in the declaration of the geojson variable. This is also when the\nleaflet functions defining style and hover events are pointed to the postcode\nshapes.\n\n// add all options to the polygons\ngeojson = L.geoJson(postcodesData, {\n\tstyle: style,\n\tonEachFeature: onEachFeature\n}).addTo(map);\n\n\n2. Setting Initial Location and Zoom on Page Load\nTo open on Australia instead of the US the key piece of code you need to set is\nthe latitude and longitude coordinates along with the zoom level. This required\na little bit of playing around to get the desired look. N.b I set the zoom level\nto 4 in the code below, don't try finely tuning with decimal places as tiles are\nonly available for integers. I learnt the hard way!\n\n// set up the map\nvar map = L.map('map').setView([-27.833, 133.583], 4);\n\n\n3. Dynamically Fit the Map to Fill the Device Screen Size\nThere is one issue with setting the zoom, since the zoom of the map is fixed and\nthe device size is variable depending on the device being used. For example, if\nyou set the zoom using your desktop screen, then loaded the page on a mobile\ndevice with a fraction of the screen real estate you will only see a fraction of\nAustralia.\n\na. Set the Viewport\nCheck out W3's intro to the viewport\n[http://www.w3schools.com/css/css_rwd_viewport.asp]. Included the following in\nthe head of your html.\n\n<meta name=\"viewport\" content=\"width=device-width, height=device-height, initial-scale=1.0\"/>\n\n\nb. Change the #map Styling\nThe leaflet choropleth example has the following styling applied to the div map:\n\n#map { width: 800px; height: 500px; }\n\n\nThis is a fixed size, we want to change this to be relative. The following will\nfit to any screensize\n\n#map { position: absolute; top: 0; right: 0; bottom: 0; left: 0; }\n\n\nc. Set Relative Zoom\nThe steps before fix the filling of the screen. So now we have variable screen\nfill and fixed zoom. What we want is variable screen fill and variable zoom.\nThat way, leaflet will automatically zoom to the maximum it can whilst keeping\nall of Australia visible on the device. Thankfully, leaflet provides a function\nfor this, fitbounds. We need to change this:\n\n// set up the map\nvar map = L.map('map').setView([-27.833, 133.583], 4);\n\n\nIf you imagine a box containing Australia, then draw a diagonal through that\nbox, the latitude and longitude of the diagnoal line's endpoints are what the \nfitBounds() constructs as a container from. I picked a point at the south easter\ntip of Tasmania and the other to the north west of Western Australia. Here's the\ncode:\n\n// set up the map\nvar map = L.map('map').setView([-27.833, 133.583]).fitBounds(\n[ [-43.21, 147.8], [-12.5, 117.2] ]\n);\n\n\nIf you've found this content helpful why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]Other Useful Resources\nEncountered\n * Shapefiles for all countries [http://gadm.org/country]\n   \n   \n * Creating maps in R\n   [https://github.com/Robinlovelace/Creating-maps-in-R/blob/master/vignettes/geoJSON.Rmd]\n   \n   \n * Useful leaflet Github comments [https://github.com/rstudio/leaflet/issues/55]\n   \n   \n * Joe Cheng, as always a pioneeR [http://rpubs.com/jcheng/us-states-2]\n   \n   \n * Convert shape file formats\n   [https://github.com/mbostock/topojson/wiki/Command-Line-Reference]","feature_image":null,"featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2016-06-03 04:52:23","created_by":"1","updated_at":"2020-04-21 21:22:32","updated_by":null,"published_at":"2016-08-10 05:35:16","published_by":"1","custom_excerpt":null,"codeinjection_head":"<script async custom-element=\"amp-iframe\" src=\"https://cdn.ampproject.org/v0/amp-iframe-0.1.js\"></script>","codeinjection_foot":"<script async custom-element=\"amp-iframe\" src=\"https://cdn.ampproject.org/v0/amp-iframe-0.1.js\"></script>","custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dce7","uuid":"996fb0e7-d947-4fc4-b5c6-7815049330a1","title":"How to Stop R Projects and Scripts Breaking","slug":"using-package-management-in-r","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*A how-to in getting started with Package Management in R*\\n\\n**The problem:** Let's say you coded a project in `R` in 2014 using the package [`dplyr`](http://dplyr.tidyverse.org/) and you called the project `2014project`. Over time packages get updated, sometimes changing syntax and function names, let's say some major syntax changes occurred in 2015 to `dplyr` and you used the latest version of [`dplyr`](http://dplyr.tidyverse.org/) in other projects in 2015. Now you come to 2016 and you want to re-run the code in the `2014project`, but you find a bunch of errors because in 2015 dplyr had those syntax changes, such that the syntax you used in 2014 no longer computes. Now for the **solution**...\\n\\nR scripts breaking over time is annoying for anyone, but it's worth highlighting that organisations using proper package management will significantly decrease their costs in the form of maintenance time spent by developers, data scientists and data analysts fixing projects broken due to changes in package dependencies. In contrast to the Python community who seem very familiar with their [`virtualenv` package management](https://virtualenv.pypa.io/en/stable/), the R community doesn't seem to have widely adopted the practice. In the data science world, I attribute this difference between the R and Python communities to Python coming out of the computer science discipline and R coming from the statistics discipline. If you are an organisation heavily using R, you have a lot of productivity to gain from the adoption of package management by your R developers.\\n\\n## Packrat\\n```r\\n> readLines(system.file(\\\"DESCRIPTION\\\", package = \\\"packrat\\\"))[c(3,9,10)]\\n[1] \\\"Title: A Dependency Management System for Projects and their R Package\\\"\\n[2] \\\"Description: Manage the R packages your project depends on in an\\\"      \\n[3] \\\"isolated, portable, and reproducible way.\\\"  \\n```\\n\\n## Getting Started\\nTo get the RStudio niceties/integrations it seems neccessary to treat your project folder as an R Project. I've created a new project `/testpackrat2` and with the latest RStudio you should be able to select both `git` and `packrat` to be initialised. This runs the `packrat::init()` function to enter packrat mode, takes a snapshot of the package dependencies (of which there are currently none) and places the binaries in the project folder under `testpackrat2/packrat/src/`; finally `init()` runs `restore()` to apply the latest snapshot to the project folder. `R` will restart once this process is finished.\\n\\n![New Packrat Project RStudio](/content/images/2016/11/Screen-Shot-2016-11-08-at-09-19-09.png)\\nTo double check packrat has been initialised properly run:\\n\\n```r\\n# Check I am in the right directory\\n> getwd()\\n[1] \\\"/Users/lukesingham/Projects/testpackrat2\\\"\\n\\n# See if packrat is working\\n> packrat::status()\\nUp to date.\\n```\\n\\nYou should also notice the RStudio integration in the packages area, where it will show your project version against that which Packrat has the binary for and the source of that binary:\\n\\n![packages window RStudio ](/content/images/2016/11/Screen-Shot-2016-11-07-at-10-03-25.png)\\n\\nFor test purposes I created `script.R` with only one package.\\n\\n```r\\n# Create script\\n> system(\\\"echo 'library(stringr)' >| script.R\\\")\\n\\n# Confirm project contents\\n> system(\\\"ls\\\")\\npackrat\\nscript.R\\ntestpackrat.Rproj\\n```\\n\\nIf I open `script.R` in RStudio and try and load the library this is the expected behaviour:\\n\\n```r\\n> library(stringr)\\nError in library(stringr) : there is no package called ‘stringr’\\n```\\n\\nI have `stringr` installed on my system, but now I am in an isolated packrat project which is only reading from the project folders list of packages. Now to run the packrat commands to `snapshot()` the addition of `stringr` to the project and install it ready for use in the project.\\n\\n```r\\n> packrat::status()\\n\\nThe following packages are referenced in your code, but are not present\\nin your library nor in packrat:\\n\\n    stringr\\n\\nYou will need to install these packages manually, then use\\npackrat::snapshot() to record these packages in packrat.\\n\\n# Well I do have stringr in my library, anyway...\\n# now to take a snapshot\\n> packrat::snapshot()\\n\\nAdding these packages to packrat:\\n             _      \\n    magrittr   1.5  \\n    stringi    1.1.2\\n    stringr    1.1.0\\n\\nFetching sources for magrittr (1.5) ... OK (CRAN current)\\nFetching sources for stringi (1.1.2) ... OK (CRAN current)\\nFetching sources for stringr (1.1.0) ... OK (CRAN current)\\nSnapshot written to '/Users/lukesingham/Projects/testpackrat2/packrat/packrat.lock'\\n\\n# Now to install/'restore' those sources to the snapshot just taken\\n> packrat::restore()\\nInstalling magrittr (1.5) ... \\n\\tOK (built source)\\nInstalling stringi (1.1.2) ... \\n\\tOK (built source)\\nInstalling stringr (1.1.0) ... \\n\\tOK (built source)\\n```\\n\\nIt's probably worth pointing out what packrat is doing to your library path, particularly when you go to install new packages in your project.\\n\\n```r\\n# Turn packrat off\\n> packrat::packrat_mode()\\nPackrat mode off. Resetting library paths to:\\n- \\\"/usr/local/lib/R/3.2/site-library\\\"\\n- \\\"/usr/local/Cellar/r/3.2.3/R.framework/Versions/3.2/Resources/library\\\"\\n\\n# Turn packrat back on\\n> packrat::packrat_mode()\\nPackrat mode on. Using library in directory:\\n- \\\"~/Projects/testpackrat2/packrat/lib\\\"\\n```\\n\\nSo as long as you are in `packrat_mode`, when you run `install.packages()` or remove `remove.packages()` it will only be modifying the project `testpackrat2/` folder.\\n\\n## Existing Project with Out-Of-Date Packages\\nI want to test the scenario where I have existing projects using old versions of packages. To do this I ran:\\n\\n```r\\n# Find old packages\\nold.packages()\\n>             Installed     Built   ReposVer\\nlubridate     \\\"1.5.6\\\"       \\\"3.2.3\\\" \\\"1.6.0\\\"\\nhtmlwidgets   \\\"0.6\\\"         \\\"3.2.3\\\" \\\"0.7\\\"\\n```\\n\\nI then created a folder with these packages, to simulate an old project that I want `packrat` to manage.\\n\\n```bash\\n$ mkdir testpackratOutofDatePackages\\n$ echo \\\"library(lubridate); library(htmlwidgets)\\\" >| testpackratOutofDatePackages/oldScript.R\\n```\\n\\nNow go to RStudio and open the existing project `testpackratOutofDatePackages`. Click on `File > New Project > Existing Directory`\\n\\n![New project Rstudio](/content/images/2016/11/Screen-Shot-2016-11-07-at-19-58-24.png)\\n\\nUnfortunately there is not an option to `init` packrat as part the process. So...\\n\\n```\\n> packrat::init()\\nInitializing packrat project in directory:\\n- \\\"~/Projects/testpackratOutofDatePackages\\\"\\n... (edited down the consolte printout)\\nFetching sources for htmlwidgets (0.6) ... OK (CRAN archived)\\nFetching sources for lubridate (1.5.6) ... OK (CRAN archived)\\n```\\n\\nOk great, it fetches the old binaries. Now let's test upgrading these packages and then reverting back to the initial project state.\\n\\n```r\\n> install.packages(\\\"htmlwidgets\\\") # v0.7\\nInstalling package into ‘/Users/lukesingham/Projects/testpackratOutofDatePackages/packrat/lib/x86_64-apple-darwin15.2.0/3.2.3’ ...\\n* DONE (htmlwidgets)\\n```\\n\\nSo it seems packrat auto snapshots the project, such that I couldn't run `restore()` and return to htmlwidgets v0.6. Let's try that again without autosnapshot and with my out-of-date `lubridate`:\\n\\n```r\\n# Turn off auto.snapshot\\n> packrat::set_opts(auto.snapshot=FALSE)\\n\\n# Check status\\n> packrat::status()\\nUp to date.\\n\\n# Now try updating lubridate\\n> install.packages(\\\"lubridate\\\")\\nDONE\\n\\n> packrat::status()\\n\\nThe following packages are out of sync between packrat and your current library:\\n                packrat   library\\n    lubridate     1.5.6     1.6.0\\n\\nUse packrat::snapshot() to set packrat to use the current library, or use\\npackrat::restore() to reset the library to the last snapshot.\\n\\n# Return to old version of lubridate\\n> packrat::restore(overwrite.dirty=T)\\n\\nDowngrading these packages in your library:\\n                 from      to\\n    lubridate   1.6.0   1.5.6\\n\\nDo you want to continue? [Y/n]: Y\\nReplacing lubridate (downgrade 1.6.0 to 1.5.6) ... \\tOK (built source)\\n```\\n\\nIf you've found this content helpful why not...\\n\\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\\n  <link href=\\\"https://fonts.googleapis.com/css?family=Cookie\\\" rel=\\\"stylesheet\\\"><a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\\n\\n## Conclusion \\nBy default packrat `auto.snapshot` my project when I updated a package. Crucially, this meant I could not use `packrat::restore()` because I was `already up to date`. Of course, if I had been using `git` and making commits before and after, then there wouldn't have been a problem. However, without `git`, running `packrat::restore()` to downgrade a package requires the `overwrite.dirty=TRUE` option and only seems possible if you switched off `auto.snapshot`. I'd highly recommend turning it off anyway, as the manual update will allow for a better understanding and control over packrat.\\n\\n## Other Resources\\n\\n* [RStudio Video presentation of Packrat](https://www.rstudio.com/resources/webinars/managing-package-dependencies-in-r-with-packrat/)\\n* [Packrat Package Documentation](https://cran.r-project.org/web/packages/packrat/packrat.pdf)\\n* [Packrat website](https://rstudio.github.io/packrat/)\\n* [An alternative to packrat, Microsoft's MRAN and Enhanced R](https://mran.microsoft.com/rro/#repos)\\n* [Packrat discussion board](https://groups.google.com/forum/#!forum/packrat-discuss)\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><em>A how-to in getting started with Package Management in R</em></p>\n<p><strong>The problem:</strong> Let's say you coded a project in <code>R</code> in 2014 using the package <a href=\"http://dplyr.tidyverse.org/\"><code>dplyr</code></a> and you called the project <code>2014project</code>. Over time packages get updated, sometimes changing syntax and function names, let's say some major syntax changes occurred in 2015 to <code>dplyr</code> and you used the latest version of <a href=\"http://dplyr.tidyverse.org/\"><code>dplyr</code></a> in other projects in 2015. Now you come to 2016 and you want to re-run the code in the <code>2014project</code>, but you find a bunch of errors because in 2015 dplyr had those syntax changes, such that the syntax you used in 2014 no longer computes. Now for the <strong>solution</strong>...</p>\n<p>R scripts breaking over time is annoying for anyone, but it's worth highlighting that organisations using proper package management will significantly decrease their costs in the form of maintenance time spent by developers, data scientists and data analysts fixing projects broken due to changes in package dependencies. In contrast to the Python community who seem very familiar with their <a href=\"https://virtualenv.pypa.io/en/stable/\"><code>virtualenv</code> package management</a>, the R community doesn't seem to have widely adopted the practice. In the data science world, I attribute this difference between the R and Python communities to Python coming out of the computer science discipline and R coming from the statistics discipline. If you are an organisation heavily using R, you have a lot of productivity to gain from the adoption of package management by your R developers.</p>\n<h2 id=\"packrat\">Packrat</h2>\n<pre><code class=\"language-r\">&gt; readLines(system.file(&quot;DESCRIPTION&quot;, package = &quot;packrat&quot;))[c(3,9,10)]\n[1] &quot;Title: A Dependency Management System for Projects and their R Package&quot;\n[2] &quot;Description: Manage the R packages your project depends on in an&quot;      \n[3] &quot;isolated, portable, and reproducible way.&quot;  \n</code></pre>\n<h2 id=\"gettingstarted\">Getting Started</h2>\n<p>To get the RStudio niceties/integrations it seems neccessary to treat your project folder as an R Project. I've created a new project <code>/testpackrat2</code> and with the latest RStudio you should be able to select both <code>git</code> and <code>packrat</code> to be initialised. This runs the <code>packrat::init()</code> function to enter packrat mode, takes a snapshot of the package dependencies (of which there are currently none) and places the binaries in the project folder under <code>testpackrat2/packrat/src/</code>; finally <code>init()</code> runs <code>restore()</code> to apply the latest snapshot to the project folder. <code>R</code> will restart once this process is finished.</p>\n<p><img src=\"/content/images/2016/11/Screen-Shot-2016-11-08-at-09-19-09.png\" alt=\"New Packrat Project RStudio\"><br>\nTo double check packrat has been initialised properly run:</p>\n<pre><code class=\"language-r\"># Check I am in the right directory\n&gt; getwd()\n[1] &quot;/Users/lukesingham/Projects/testpackrat2&quot;\n\n# See if packrat is working\n&gt; packrat::status()\nUp to date.\n</code></pre>\n<p>You should also notice the RStudio integration in the packages area, where it will show your project version against that which Packrat has the binary for and the source of that binary:</p>\n<p><img src=\"/content/images/2016/11/Screen-Shot-2016-11-07-at-10-03-25.png\" alt=\"packages window RStudio \"></p>\n<p>For test purposes I created <code>script.R</code> with only one package.</p>\n<pre><code class=\"language-r\"># Create script\n&gt; system(&quot;echo 'library(stringr)' &gt;| script.R&quot;)\n\n# Confirm project contents\n&gt; system(&quot;ls&quot;)\npackrat\nscript.R\ntestpackrat.Rproj\n</code></pre>\n<p>If I open <code>script.R</code> in RStudio and try and load the library this is the expected behaviour:</p>\n<pre><code class=\"language-r\">&gt; library(stringr)\nError in library(stringr) : there is no package called ‘stringr’\n</code></pre>\n<p>I have <code>stringr</code> installed on my system, but now I am in an isolated packrat project which is only reading from the project folders list of packages. Now to run the packrat commands to <code>snapshot()</code> the addition of <code>stringr</code> to the project and install it ready for use in the project.</p>\n<pre><code class=\"language-r\">&gt; packrat::status()\n\nThe following packages are referenced in your code, but are not present\nin your library nor in packrat:\n\n    stringr\n\nYou will need to install these packages manually, then use\npackrat::snapshot() to record these packages in packrat.\n\n# Well I do have stringr in my library, anyway...\n# now to take a snapshot\n&gt; packrat::snapshot()\n\nAdding these packages to packrat:\n             _      \n    magrittr   1.5  \n    stringi    1.1.2\n    stringr    1.1.0\n\nFetching sources for magrittr (1.5) ... OK (CRAN current)\nFetching sources for stringi (1.1.2) ... OK (CRAN current)\nFetching sources for stringr (1.1.0) ... OK (CRAN current)\nSnapshot written to '/Users/lukesingham/Projects/testpackrat2/packrat/packrat.lock'\n\n# Now to install/'restore' those sources to the snapshot just taken\n&gt; packrat::restore()\nInstalling magrittr (1.5) ... \n\tOK (built source)\nInstalling stringi (1.1.2) ... \n\tOK (built source)\nInstalling stringr (1.1.0) ... \n\tOK (built source)\n</code></pre>\n<p>It's probably worth pointing out what packrat is doing to your library path, particularly when you go to install new packages in your project.</p>\n<pre><code class=\"language-r\"># Turn packrat off\n&gt; packrat::packrat_mode()\nPackrat mode off. Resetting library paths to:\n- &quot;/usr/local/lib/R/3.2/site-library&quot;\n- &quot;/usr/local/Cellar/r/3.2.3/R.framework/Versions/3.2/Resources/library&quot;\n\n# Turn packrat back on\n&gt; packrat::packrat_mode()\nPackrat mode on. Using library in directory:\n- &quot;~/Projects/testpackrat2/packrat/lib&quot;\n</code></pre>\n<p>So as long as you are in <code>packrat_mode</code>, when you run <code>install.packages()</code> or remove <code>remove.packages()</code> it will only be modifying the project <code>testpackrat2/</code> folder.</p>\n<h2 id=\"existingprojectwithoutofdatepackages\">Existing Project with Out-Of-Date Packages</h2>\n<p>I want to test the scenario where I have existing projects using old versions of packages. To do this I ran:</p>\n<pre><code class=\"language-r\"># Find old packages\nold.packages()\n&gt;             Installed     Built   ReposVer\nlubridate     &quot;1.5.6&quot;       &quot;3.2.3&quot; &quot;1.6.0&quot;\nhtmlwidgets   &quot;0.6&quot;         &quot;3.2.3&quot; &quot;0.7&quot;\n</code></pre>\n<p>I then created a folder with these packages, to simulate an old project that I want <code>packrat</code> to manage.</p>\n<pre><code class=\"language-bash\">$ mkdir testpackratOutofDatePackages\n$ echo &quot;library(lubridate); library(htmlwidgets)&quot; &gt;| testpackratOutofDatePackages/oldScript.R\n</code></pre>\n<p>Now go to RStudio and open the existing project <code>testpackratOutofDatePackages</code>. Click on <code>File &gt; New Project &gt; Existing Directory</code></p>\n<p><img src=\"/content/images/2016/11/Screen-Shot-2016-11-07-at-19-58-24.png\" alt=\"New project Rstudio\"></p>\n<p>Unfortunately there is not an option to <code>init</code> packrat as part the process. So...</p>\n<pre><code>&gt; packrat::init()\nInitializing packrat project in directory:\n- &quot;~/Projects/testpackratOutofDatePackages&quot;\n... (edited down the consolte printout)\nFetching sources for htmlwidgets (0.6) ... OK (CRAN archived)\nFetching sources for lubridate (1.5.6) ... OK (CRAN archived)\n</code></pre>\n<p>Ok great, it fetches the old binaries. Now let's test upgrading these packages and then reverting back to the initial project state.</p>\n<pre><code class=\"language-r\">&gt; install.packages(&quot;htmlwidgets&quot;) # v0.7\nInstalling package into ‘/Users/lukesingham/Projects/testpackratOutofDatePackages/packrat/lib/x86_64-apple-darwin15.2.0/3.2.3’ ...\n* DONE (htmlwidgets)\n</code></pre>\n<p>So it seems packrat auto snapshots the project, such that I couldn't run <code>restore()</code> and return to htmlwidgets v0.6. Let's try that again without autosnapshot and with my out-of-date <code>lubridate</code>:</p>\n<pre><code class=\"language-r\"># Turn off auto.snapshot\n&gt; packrat::set_opts(auto.snapshot=FALSE)\n\n# Check status\n&gt; packrat::status()\nUp to date.\n\n# Now try updating lubridate\n&gt; install.packages(&quot;lubridate&quot;)\nDONE\n\n&gt; packrat::status()\n\nThe following packages are out of sync between packrat and your current library:\n                packrat   library\n    lubridate     1.5.6     1.6.0\n\nUse packrat::snapshot() to set packrat to use the current library, or use\npackrat::restore() to reset the library to the last snapshot.\n\n# Return to old version of lubridate\n&gt; packrat::restore(overwrite.dirty=T)\n\nDowngrading these packages in your library:\n                 from      to\n    lubridate   1.6.0   1.5.6\n\nDo you want to continue? [Y/n]: Y\nReplacing lubridate (downgrade 1.6.0 to 1.5.6) ... \tOK (built source)\n</code></pre>\n<p>If you've found this content helpful why not...</p>\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\n  <link href=\"https://fonts.googleapis.com/css?family=Cookie\" rel=\"stylesheet\"><a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>By default packrat <code>auto.snapshot</code> my project when I updated a package. Crucially, this meant I could not use <code>packrat::restore()</code> because I was <code>already up to date</code>. Of course, if I had been using <code>git</code> and making commits before and after, then there wouldn't have been a problem. However, without <code>git</code>, running <code>packrat::restore()</code> to downgrade a package requires the <code>overwrite.dirty=TRUE</code> option and only seems possible if you switched off <code>auto.snapshot</code>. I'd highly recommend turning it off anyway, as the manual update will allow for a better understanding and control over packrat.</p>\n<h2 id=\"otherresources\">Other Resources</h2>\n<ul>\n<li><a href=\"https://www.rstudio.com/resources/webinars/managing-package-dependencies-in-r-with-packrat/\">RStudio Video presentation of Packrat</a></li>\n<li><a href=\"https://cran.r-project.org/web/packages/packrat/packrat.pdf\">Packrat Package Documentation</a></li>\n<li><a href=\"https://rstudio.github.io/packrat/\">Packrat website</a></li>\n<li><a href=\"https://mran.microsoft.com/rro/#repos\">An alternative to packrat, Microsoft's MRAN and Enhanced R</a></li>\n<li><a href=\"https://groups.google.com/forum/#!forum/packrat-discuss\">Packrat discussion board</a></li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"15","plaintext":"A how-to in getting started with Package Management in R\n\nThe problem: Let's say you coded a project in R in 2014 using the package dplyr\n[http://dplyr.tidyverse.org/] and you called the project 2014project. Over time\npackages get updated, sometimes changing syntax and function names, let's say\nsome major syntax changes occurred in 2015 to dplyr and you used the latest\nversion of dplyr [http://dplyr.tidyverse.org/] in other projects in 2015. Now\nyou come to 2016 and you want to re-run the code in the 2014project, but you\nfind a bunch of errors because in 2015 dplyr had those syntax changes, such that\nthe syntax you used in 2014 no longer computes. Now for the solution...\n\nR scripts breaking over time is annoying for anyone, but it's worth highlighting\nthat organisations using proper package management will significantly decrease\ntheir costs in the form of maintenance time spent by developers, data scientists\nand data analysts fixing projects broken due to changes in package dependencies.\nIn contrast to the Python community who seem very familiar with their virtualenv\npackage management [https://virtualenv.pypa.io/en/stable/], the R community\ndoesn't seem to have widely adopted the practice. In the data science world, I\nattribute this difference between the R and Python communities to Python coming\nout of the computer science discipline and R coming from the statistics\ndiscipline. If you are an organisation heavily using R, you have a lot of\nproductivity to gain from the adoption of package management by your R\ndevelopers.\n\nPackrat\n> readLines(system.file(\"DESCRIPTION\", package = \"packrat\"))[c(3,9,10)]\n[1] \"Title: A Dependency Management System for Projects and their R Package\"\n[2] \"Description: Manage the R packages your project depends on in an\"      \n[3] \"isolated, portable, and reproducible way.\"  \n\n\nGetting Started\nTo get the RStudio niceties/integrations it seems neccessary to treat your\nproject folder as an R Project. I've created a new project /testpackrat2 and\nwith the latest RStudio you should be able to select both git and packrat to be\ninitialised. This runs the packrat::init() function to enter packrat mode, takes\na snapshot of the package dependencies (of which there are currently none) and\nplaces the binaries in the project folder under testpackrat2/packrat/src/;\nfinally init() runs restore() to apply the latest snapshot to the project\nfolder. R will restart once this process is finished.\n\n\nTo double check packrat has been initialised properly run:\n\n# Check I am in the right directory\n> getwd()\n[1] \"/Users/lukesingham/Projects/testpackrat2\"\n\n# See if packrat is working\n> packrat::status()\nUp to date.\n\n\nYou should also notice the RStudio integration in the packages area, where it\nwill show your project version against that which Packrat has the binary for and\nthe source of that binary:\n\n\n\nFor test purposes I created script.R with only one package.\n\n# Create script\n> system(\"echo 'library(stringr)' >| script.R\")\n\n# Confirm project contents\n> system(\"ls\")\npackrat\nscript.R\ntestpackrat.Rproj\n\n\nIf I open script.R in RStudio and try and load the library this is the expected\nbehaviour:\n\n> library(stringr)\nError in library(stringr) : there is no package called ‘stringr’\n\n\nI have stringr installed on my system, but now I am in an isolated packrat\nproject which is only reading from the project folders list of packages. Now to\nrun the packrat commands to snapshot() the addition of stringr to the project\nand install it ready for use in the project.\n\n> packrat::status()\n\nThe following packages are referenced in your code, but are not present\nin your library nor in packrat:\n\n    stringr\n\nYou will need to install these packages manually, then use\npackrat::snapshot() to record these packages in packrat.\n\n# Well I do have stringr in my library, anyway...\n# now to take a snapshot\n> packrat::snapshot()\n\nAdding these packages to packrat:\n             _      \n    magrittr   1.5  \n    stringi    1.1.2\n    stringr    1.1.0\n\nFetching sources for magrittr (1.5) ... OK (CRAN current)\nFetching sources for stringi (1.1.2) ... OK (CRAN current)\nFetching sources for stringr (1.1.0) ... OK (CRAN current)\nSnapshot written to '/Users/lukesingham/Projects/testpackrat2/packrat/packrat.lock'\n\n# Now to install/'restore' those sources to the snapshot just taken\n> packrat::restore()\nInstalling magrittr (1.5) ... \n\tOK (built source)\nInstalling stringi (1.1.2) ... \n\tOK (built source)\nInstalling stringr (1.1.0) ... \n\tOK (built source)\n\n\nIt's probably worth pointing out what packrat is doing to your library path,\nparticularly when you go to install new packages in your project.\n\n# Turn packrat off\n> packrat::packrat_mode()\nPackrat mode off. Resetting library paths to:\n- \"/usr/local/lib/R/3.2/site-library\"\n- \"/usr/local/Cellar/r/3.2.3/R.framework/Versions/3.2/Resources/library\"\n\n# Turn packrat back on\n> packrat::packrat_mode()\nPackrat mode on. Using library in directory:\n- \"~/Projects/testpackrat2/packrat/lib\"\n\n\nSo as long as you are in packrat_mode, when you run install.packages() or remove \nremove.packages() it will only be modifying the project testpackrat2/ folder.\n\nExisting Project with Out-Of-Date Packages\nI want to test the scenario where I have existing projects using old versions of\npackages. To do this I ran:\n\n# Find old packages\nold.packages()\n>             Installed     Built   ReposVer\nlubridate     \"1.5.6\"       \"3.2.3\" \"1.6.0\"\nhtmlwidgets   \"0.6\"         \"3.2.3\" \"0.7\"\n\n\nI then created a folder with these packages, to simulate an old project that I\nwant packrat to manage.\n\n$ mkdir testpackratOutofDatePackages\n$ echo \"library(lubridate); library(htmlwidgets)\" >| testpackratOutofDatePackages/oldScript.R\n\n\nNow go to RStudio and open the existing project testpackratOutofDatePackages.\nClick on File > New Project > Existing Directory\n\n\n\nUnfortunately there is not an option to init packrat as part the process. So...\n\n> packrat::init()\nInitializing packrat project in directory:\n- \"~/Projects/testpackratOutofDatePackages\"\n... (edited down the consolte printout)\nFetching sources for htmlwidgets (0.6) ... OK (CRAN archived)\nFetching sources for lubridate (1.5.6) ... OK (CRAN archived)\n\n\nOk great, it fetches the old binaries. Now let's test upgrading these packages\nand then reverting back to the initial project state.\n\n> install.packages(\"htmlwidgets\") # v0.7\nInstalling package into ‘/Users/lukesingham/Projects/testpackratOutofDatePackages/packrat/lib/x86_64-apple-darwin15.2.0/3.2.3’ ...\n* DONE (htmlwidgets)\n\n\nSo it seems packrat auto snapshots the project, such that I couldn't run \nrestore() and return to htmlwidgets v0.6. Let's try that again without\nautosnapshot and with my out-of-date lubridate:\n\n# Turn off auto.snapshot\n> packrat::set_opts(auto.snapshot=FALSE)\n\n# Check status\n> packrat::status()\nUp to date.\n\n# Now try updating lubridate\n> install.packages(\"lubridate\")\nDONE\n\n> packrat::status()\n\nThe following packages are out of sync between packrat and your current library:\n                packrat   library\n    lubridate     1.5.6     1.6.0\n\nUse packrat::snapshot() to set packrat to use the current library, or use\npackrat::restore() to reset the library to the last snapshot.\n\n# Return to old version of lubridate\n> packrat::restore(overwrite.dirty=T)\n\nDowngrading these packages in your library:\n                 from      to\n    lubridate   1.6.0   1.5.6\n\nDo you want to continue? [Y/n]: Y\nReplacing lubridate (downgrade 1.6.0 to 1.5.6) ... \tOK (built source)\n\n\nIf you've found this content helpful why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]Conclusion\nBy default packrat auto.snapshot my project when I updated a package. Crucially,\nthis meant I could not use packrat::restore() because I was already up to date.\nOf course, if I had been using git and making commits before and after, then\nthere wouldn't have been a problem. However, without git, running \npackrat::restore() to downgrade a package requires the overwrite.dirty=TRUE \noption and only seems possible if you switched off auto.snapshot. I'd highly\nrecommend turning it off anyway, as the manual update will allow for a better\nunderstanding and control over packrat.\n\nOther Resources\n * RStudio Video presentation of Packrat\n   [https://www.rstudio.com/resources/webinars/managing-package-dependencies-in-r-with-packrat/]\n * Packrat Package Documentation\n   [https://cran.r-project.org/web/packages/packrat/packrat.pdf]\n * Packrat website [https://rstudio.github.io/packrat/]\n * An alternative to packrat, Microsoft's MRAN and Enhanced R\n   [https://mran.microsoft.com/rro/#repos]\n * Packrat discussion board\n   [https://groups.google.com/forum/#!forum/packrat-discuss]","feature_image":null,"featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2016-11-02 00:03:40","created_by":"1","updated_at":"2019-08-04 13:07:36","updated_by":null,"published_at":"2016-11-07 23:17:25","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dce8","uuid":"061aee20-645e-4fbf-b205-1d6f41ab545a","title":"Intro to React.js and Redux","slug":"intro-to-react-js-and-redux","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"This post aims to give a high level overview of key concepts, principles and the structure of how React.js and [Redux](http://redux.js.org/) work.\\n\\n# Redux\\n\\n>The whole state of your app is stored in an object tree inside a single store.\\nThe only way to change the state tree is to emit an action, an object describing what happened.\\nTo specify how the actions transform the state tree, you write pure reducers.\\n\\nRedux was motivated by a common problem in complicated web applications of multiple web components interacting with one or more models that then flow on to update one or more web components, which can then update one or more models, which can then... I think you get the point! Essentially, for a developer debugging the state of an application can be very hard. Enter Redux, which brings a design pattern where the whole state of the application is in a single read only store.\\n\\nRedux has a uni-directional data flow.\\ncurrent state -> ui component -> action -> reducer -> next state\\n\\n## Redux folder Structure\\n```\\ncore\\n├── actions\\n│   └── actions-ui.js\\n├── reducers\\n│   ├── index.js\\n│   └── reducer-ui.js\\n├── store\\n│   └── configureStore.js\\n└── types.js\\n```\\n**actions**\\n\\n\\n**Reducers** - this folder contains definitions of reducers. A reducer changes the state of the application as a result of an action. Effectively there is only one *reducer* however modularised code is much easier to manage (e.g. reducer-ui.js). Which means in the `reducer/index.js` file each discrete reducer needs to be referenced.\\n\\n**store** - `configureStore.js` enables the configuration of middleware or extensions to redux.\\n\\n**types.js** - common but not compulsory, acts as a main configuration file that contains the names of the actions and exports them as constants, which reduces typing elsewhere in your code. \\n\\n#Routing\\n\\n# Firebase Deployment Process\\n```\\nnpm build\\nfirebase deploy\\n```\\n\\n#Manifest.json\\nNot strictly related to React.js or Redux, but handy to know about.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>This post aims to give a high level overview of key concepts, principles and the structure of how React.js and <a href=\"http://redux.js.org/\">Redux</a> work.</p>\n<h1 id=\"redux\">Redux</h1>\n<blockquote>\n<p>The whole state of your app is stored in an object tree inside a single store.<br>\nThe only way to change the state tree is to emit an action, an object describing what happened.<br>\nTo specify how the actions transform the state tree, you write pure reducers.</p>\n</blockquote>\n<p>Redux was motivated by a common problem in complicated web applications of multiple web components interacting with one or more models that then flow on to update one or more web components, which can then update one or more models, which can then... I think you get the point! Essentially, for a developer debugging the state of an application can be very hard. Enter Redux, which brings a design pattern where the whole state of the application is in a single read only store.</p>\n<p>Redux has a uni-directional data flow.<br>\ncurrent state -&gt; ui component -&gt; action -&gt; reducer -&gt; next state</p>\n<h2 id=\"reduxfolderstructure\">Redux folder Structure</h2>\n<pre><code>core\n├── actions\n│   └── actions-ui.js\n├── reducers\n│   ├── index.js\n│   └── reducer-ui.js\n├── store\n│   └── configureStore.js\n└── types.js\n</code></pre>\n<p><strong>actions</strong></p>\n<p><strong>Reducers</strong> - this folder contains definitions of reducers. A reducer changes the state of the application as a result of an action. Effectively there is only one <em>reducer</em> however modularised code is much easier to manage (e.g. reducer-ui.js). Which means in the <code>reducer/index.js</code> file each discrete reducer needs to be referenced.</p>\n<p><strong>store</strong> - <code>configureStore.js</code> enables the configuration of middleware or extensions to redux.</p>\n<p><strong>types.js</strong> - common but not compulsory, acts as a main configuration file that contains the names of the actions and exports them as constants, which reduces typing elsewhere in your code.</p>\n<h1 id=\"routing\">Routing</h1>\n<h1 id=\"firebasedeploymentprocess\">Firebase Deployment Process</h1>\n<pre><code>npm build\nfirebase deploy\n</code></pre>\n<h1 id=\"manifestjson\">Manifest.json</h1>\n<p>Not strictly related to React.js or Redux, but handy to know about.</p>\n<!--kg-card-end: markdown-->","comment_id":"16","plaintext":"This post aims to give a high level overview of key concepts, principles and the\nstructure of how React.js and Redux [http://redux.js.org/] work.\n\nRedux\n> The whole state of your app is stored in an object tree inside a single store.\nThe only way to change the state tree is to emit an action, an object describing\nwhat happened.\nTo specify how the actions transform the state tree, you write pure reducers.\n\n\nRedux was motivated by a common problem in complicated web applications of\nmultiple web components interacting with one or more models that then flow on to\nupdate one or more web components, which can then update one or more models,\nwhich can then... I think you get the point! Essentially, for a developer\ndebugging the state of an application can be very hard. Enter Redux, which\nbrings a design pattern where the whole state of the application is in a single\nread only store.\n\nRedux has a uni-directional data flow.\ncurrent state -> ui component -> action -> reducer -> next state\n\nRedux folder Structure\ncore\n├── actions\n│   └── actions-ui.js\n├── reducers\n│   ├── index.js\n│   └── reducer-ui.js\n├── store\n│   └── configureStore.js\n└── types.js\n\n\nactions\n\nReducers - this folder contains definitions of reducers. A reducer changes the\nstate of the application as a result of an action. Effectively there is only one \nreducer however modularised code is much easier to manage (e.g. reducer-ui.js).\nWhich means in the reducer/index.js file each discrete reducer needs to be\nreferenced.\n\nstore - configureStore.js enables the configuration of middleware or extensions\nto redux.\n\ntypes.js - common but not compulsory, acts as a main configuration file that\ncontains the names of the actions and exports them as constants, which reduces\ntyping elsewhere in your code.\n\nRouting\nFirebase Deployment Process\nnpm build\nfirebase deploy\n\n\nManifest.json\nNot strictly related to React.js or Redux, but handy to know about.","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-05-06 21:45:56","created_by":"1","updated_at":"2017-05-13 09:27:51","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dce9","uuid":"21ffe90a-9b65-4bdf-be6c-d52fff981271","title":"Benchmarking the Performance of R Code","slug":"benchmarking-the-performance-of-r-code","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"To assess the performance  of R code there's a great little package called [microbenchmark](https://cran.r-project.org/web/packages/microbenchmark/index.html).\\n\\n```r\\ninstall.packages('microbenchmark')\\nlibrary(microbenchmark)\\n```\\n\\nI was particularly interested in the performance increase of a [shiny application](https://shiny.rstudio.com/) that reads in a bunch of `.csv` data. To test the performance increase I ran the following code:\\n\\n```r\\n# For all *.csv files in \\\"data\\\" directory\\ncsv.files <- list.files(path = \\\"data\\\", pattern = \\\"*.csv\\\", full.names = TRUE)\\n\\n# How many csvs are there?\\nlength(csv.files)\\n#[1] 13\\n\\n# Benchmark\\nresults <- microbenchmark(\\n  read.csv2.test = lapply(csv.files, read.csv2),\\n  read_csv.test  = lapply(csv.files, read_csv),\\n  times = 50)\\n```\\n\\nSo that's 50 iterations of reading 13 files, or 650 reads for each function. Now to have a look at the results:\\n\\n```r\\n> results\\nUnit: milliseconds\\n           expr       min        lq      mean    median        uq      max neval\\n read.csv2.test 111.18612 118.28930 124.71949 123.38192 131.20016 144.3691    50\\n  read_csv.test  67.56371  71.28666  76.22763  74.19215  78.06555 105.8433    50\\n```\\n\\nNot the most impressive gain from the `readr` package, where the [documentation purports up to 10x faster reads](https://cran.r-project.org/web/packages/readr/README.html), but still substantial and worth implementing. For a simple guide on UX and the implications of lag check out this [stackexchange answer](https://ux.stackexchange.com/a/3836/78865). And if you want even faster read/write operations have a look at [data.table](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html)'s `fread()` function.\\n\\n## Examine the Distribution of Evaluations\\nThe [microbenchmark](https://cran.r-project.org/web/packages/microbenchmark/index.html) package includes a `boxplot()` function which is a really good idea to look at prior to implementing a particular change in your code base. Here's an example of a comparison between a custom function and the same function compiled using the [`compiler::cmpfun()`]() function.\\n\\nHere's the a print out of the results:\\n```R\\nprint(results)\\n## Unit: microseconds\\n##      expr    min      lq      mean  median      uq       max neval\\n##     plain 87.550 90.5355 227.71949 91.8995 94.5685 13308.578   100\\n##  compiled 87.254 90.9845  95.71651 92.4605 96.1630   194.724   100\\n```\\n\\nIf I only compared the mean of the two functions, then I would expect to get a considerable 58% speed increase. \\n\\n![Microbenchmark Boxplot](/content/images/2017/08/microbenchmark_compiled_function.png)\\n\\nHowever, the boxplot of the results `boxplot(results)` quickly reveals an outlier in the 100 evaluations performed that heavily impacts the mean. In this case, the median actually shows a negligible difference between the function and the byte compiled function.\\n\\nIf you prefer a fancier looking plot, microbenchmark has included a [ggplot2](http://ggplot2.org/) based `autoplot()` function. \\n\\n```\\nlibrary(ggplot2)\\nautoplot(results)\\n```\\n\\n![Microbenchmark Autoplot](/content/images/2017/08/microbenchmark_autoplot.png)\\n\\n# When Should You Use Microbenchmarking?\\n\\nI use the microbenchmark package whenever I am concerned about speeding up an application. Old hands know the obligatory computer science mantra and the Knuth quote that must go along with this microbenchmarking blog post. \\n\\n> Make it work, make it right, make it fast. \\n\\n-- <cite>Kent Beck</cite>\\n\\n> ... premature optimization is the root of all evil... \\n\\n-- <cite>Donald Knuth</cite>\\n\\nKnuth goes on to say in [his paper](http://web.archive.org/web/20130731202547/http://pplab.snu.ac.kr/courses/adv_pl05/papers/p261-knuth.pdf), *'It is often a mistake to make a priori judgments about what parts of a program are really critical'*. Which is why you should first profile your code to identify where your efforts are best spent.\\n\\nIf you've found this content helpful why not...\\n\\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\\n  <link href=\\\"https://fonts.googleapis.com/css?family=Cookie\\\" rel=\\\"stylesheet\\\"><a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\\n\\n**Postscript** - [After a few nights of sleep I realised](https://m.signalvnoise.com/cant-crack-that-programming-problem-go-to-sleep-or-take-a-walk-930c767e1119) why the compiled vs. non-compiled functions had the distributions they did. [Version 3.4 of R](https://stat.ethz.ch/pipermail/r-announce/2017/000612.html) has [Just-In-Time (JIT) compiling](https://en.wikipedia.org/wiki/Just-in-time_compilation) at level 3 by default. The function I was comparing contains a `for` loop. Therefore, the plots show the difference between the new `JIT(3)` speed of R vs pre-compiling your functions that contain for loops. The first run of the non-compiled function is not byte-compiled, however all subsequent runs are byte-compiled. Hence one outlier but otherwise very similar distributions.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>To assess the performance  of R code there's a great little package called <a href=\"https://cran.r-project.org/web/packages/microbenchmark/index.html\">microbenchmark</a>.</p>\n<pre><code class=\"language-r\">install.packages('microbenchmark')\nlibrary(microbenchmark)\n</code></pre>\n<p>I was particularly interested in the performance increase of a <a href=\"https://shiny.rstudio.com/\">shiny application</a> that reads in a bunch of <code>.csv</code> data. To test the performance increase I ran the following code:</p>\n<pre><code class=\"language-r\"># For all *.csv files in &quot;data&quot; directory\ncsv.files &lt;- list.files(path = &quot;data&quot;, pattern = &quot;*.csv&quot;, full.names = TRUE)\n\n# How many csvs are there?\nlength(csv.files)\n#[1] 13\n\n# Benchmark\nresults &lt;- microbenchmark(\n  read.csv2.test = lapply(csv.files, read.csv2),\n  read_csv.test  = lapply(csv.files, read_csv),\n  times = 50)\n</code></pre>\n<p>So that's 50 iterations of reading 13 files, or 650 reads for each function. Now to have a look at the results:</p>\n<pre><code class=\"language-r\">&gt; results\nUnit: milliseconds\n           expr       min        lq      mean    median        uq      max neval\n read.csv2.test 111.18612 118.28930 124.71949 123.38192 131.20016 144.3691    50\n  read_csv.test  67.56371  71.28666  76.22763  74.19215  78.06555 105.8433    50\n</code></pre>\n<p>Not the most impressive gain from the <code>readr</code> package, where the <a href=\"https://cran.r-project.org/web/packages/readr/README.html\">documentation purports up to 10x faster reads</a>, but still substantial and worth implementing. For a simple guide on UX and the implications of lag check out this <a href=\"https://ux.stackexchange.com/a/3836/78865\">stackexchange answer</a>. And if you want even faster read/write operations have a look at <a href=\"https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html\">data.table</a>'s <code>fread()</code> function.</p>\n<h2 id=\"examinethedistributionofevaluations\">Examine the Distribution of Evaluations</h2>\n<p>The <a href=\"https://cran.r-project.org/web/packages/microbenchmark/index.html\">microbenchmark</a> package includes a <code>boxplot()</code> function which is a really good idea to look at prior to implementing a particular change in your code base. Here's an example of a comparison between a custom function and the same function compiled using the <a href=\"\"><code>compiler::cmpfun()</code></a> function.</p>\n<p>Here's the a print out of the results:</p>\n<pre><code class=\"language-R\">print(results)\n## Unit: microseconds\n##      expr    min      lq      mean  median      uq       max neval\n##     plain 87.550 90.5355 227.71949 91.8995 94.5685 13308.578   100\n##  compiled 87.254 90.9845  95.71651 92.4605 96.1630   194.724   100\n</code></pre>\n<p>If I only compared the mean of the two functions, then I would expect to get a considerable 58% speed increase.</p>\n<p><img src=\"/content/images/2017/08/microbenchmark_compiled_function.png\" alt=\"Microbenchmark Boxplot\"></p>\n<p>However, the boxplot of the results <code>boxplot(results)</code> quickly reveals an outlier in the 100 evaluations performed that heavily impacts the mean. In this case, the median actually shows a negligible difference between the function and the byte compiled function.</p>\n<p>If you prefer a fancier looking plot, microbenchmark has included a <a href=\"http://ggplot2.org/\">ggplot2</a> based <code>autoplot()</code> function.</p>\n<pre><code>library(ggplot2)\nautoplot(results)\n</code></pre>\n<p><img src=\"/content/images/2017/08/microbenchmark_autoplot.png\" alt=\"Microbenchmark Autoplot\"></p>\n<h1 id=\"whenshouldyouusemicrobenchmarking\">When Should You Use Microbenchmarking?</h1>\n<p>I use the microbenchmark package whenever I am concerned about speeding up an application. Old hands know the obligatory computer science mantra and the Knuth quote that must go along with this microbenchmarking blog post.</p>\n<blockquote>\n<p>Make it work, make it right, make it fast.</p>\n</blockquote>\n<p>-- <cite>Kent Beck</cite></p>\n<blockquote>\n<p>... premature optimization is the root of all evil...</p>\n</blockquote>\n<p>-- <cite>Donald Knuth</cite></p>\n<p>Knuth goes on to say in <a href=\"http://web.archive.org/web/20130731202547/http://pplab.snu.ac.kr/courses/adv_pl05/papers/p261-knuth.pdf\">his paper</a>, <em>'It is often a mistake to make a priori judgments about what parts of a program are really critical'</em>. Which is why you should first profile your code to identify where your efforts are best spent.</p>\n<p>If you've found this content helpful why not...</p>\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\n  <link href=\"https://fonts.googleapis.com/css?family=Cookie\" rel=\"stylesheet\"><a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a>\n<p><strong>Postscript</strong> - <a href=\"https://m.signalvnoise.com/cant-crack-that-programming-problem-go-to-sleep-or-take-a-walk-930c767e1119\">After a few nights of sleep I realised</a> why the compiled vs. non-compiled functions had the distributions they did. <a href=\"https://stat.ethz.ch/pipermail/r-announce/2017/000612.html\">Version 3.4 of R</a> has <a href=\"https://en.wikipedia.org/wiki/Just-in-time_compilation\">Just-In-Time (JIT) compiling</a> at level 3 by default. The function I was comparing contains a <code>for</code> loop. Therefore, the plots show the difference between the new <code>JIT(3)</code> speed of R vs pre-compiling your functions that contain for loops. The first run of the non-compiled function is not byte-compiled, however all subsequent runs are byte-compiled. Hence one outlier but otherwise very similar distributions.</p>\n<!--kg-card-end: markdown-->","comment_id":"17","plaintext":"To assess the performance of R code there's a great little package called \nmicrobenchmark\n[https://cran.r-project.org/web/packages/microbenchmark/index.html].\n\ninstall.packages('microbenchmark')\nlibrary(microbenchmark)\n\n\nI was particularly interested in the performance increase of a shiny application\n[https://shiny.rstudio.com/] that reads in a bunch of .csv data. To test the\nperformance increase I ran the following code:\n\n# For all *.csv files in \"data\" directory\ncsv.files <- list.files(path = \"data\", pattern = \"*.csv\", full.names = TRUE)\n\n# How many csvs are there?\nlength(csv.files)\n#[1] 13\n\n# Benchmark\nresults <- microbenchmark(\n  read.csv2.test = lapply(csv.files, read.csv2),\n  read_csv.test  = lapply(csv.files, read_csv),\n  times = 50)\n\n\nSo that's 50 iterations of reading 13 files, or 650 reads for each function. Now\nto have a look at the results:\n\n> results\nUnit: milliseconds\n           expr       min        lq      mean    median        uq      max neval\n read.csv2.test 111.18612 118.28930 124.71949 123.38192 131.20016 144.3691    50\n  read_csv.test  67.56371  71.28666  76.22763  74.19215  78.06555 105.8433    50\n\n\nNot the most impressive gain from the readr package, where the documentation\npurports up to 10x faster reads\n[https://cran.r-project.org/web/packages/readr/README.html], but still\nsubstantial and worth implementing. For a simple guide on UX and the\nimplications of lag check out this stackexchange answer\n[https://ux.stackexchange.com/a/3836/78865]. And if you want even faster\nread/write operations have a look at data.table\n[https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html]\n's fread() function.\n\nExamine the Distribution of Evaluations\nThe microbenchmark\n[https://cran.r-project.org/web/packages/microbenchmark/index.html] package\nincludes a boxplot() function which is a really good idea to look at prior to\nimplementing a particular change in your code base. Here's an example of a\ncomparison between a custom function and the same function compiled using the \ncompiler::cmpfun() function.\n\nHere's the a print out of the results:\n\nprint(results)\n## Unit: microseconds\n##      expr    min      lq      mean  median      uq       max neval\n##     plain 87.550 90.5355 227.71949 91.8995 94.5685 13308.578   100\n##  compiled 87.254 90.9845  95.71651 92.4605 96.1630   194.724   100\n\n\nIf I only compared the mean of the two functions, then I would expect to get a\nconsiderable 58% speed increase.\n\n\n\nHowever, the boxplot of the results boxplot(results) quickly reveals an outlier\nin the 100 evaluations performed that heavily impacts the mean. In this case,\nthe median actually shows a negligible difference between the function and the\nbyte compiled function.\n\nIf you prefer a fancier looking plot, microbenchmark has included a ggplot2\n[http://ggplot2.org/] based autoplot() function.\n\nlibrary(ggplot2)\nautoplot(results)\n\n\n\n\nWhen Should You Use Microbenchmarking?\nI use the microbenchmark package whenever I am concerned about speeding up an\napplication. Old hands know the obligatory computer science mantra and the Knuth\nquote that must go along with this microbenchmarking blog post.\n\n> Make it work, make it right, make it fast.\n\n\n-- Kent Beck\n\n> ... premature optimization is the root of all evil...\n\n\n-- Donald Knuth\n\nKnuth goes on to say in his paper\n[http://web.archive.org/web/20130731202547/http://pplab.snu.ac.kr/courses/adv_pl05/papers/p261-knuth.pdf]\n, 'It is often a mistake to make a priori judgments about what parts of a\nprogram are really critical'. Which is why you should first profile your code to\nidentify where your efforts are best spent.\n\nIf you've found this content helpful why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]Postscript - After a few nights of sleep I realised\n[https://m.signalvnoise.com/cant-crack-that-programming-problem-go-to-sleep-or-take-a-walk-930c767e1119] \nwhy the compiled vs. non-compiled functions had the distributions they did. \nVersion 3.4 of R [https://stat.ethz.ch/pipermail/r-announce/2017/000612.html] \nhas Just-In-Time (JIT) compiling\n[https://en.wikipedia.org/wiki/Just-in-time_compilation] at level 3 by default.\nThe function I was comparing contains a for loop. Therefore, the plots show the\ndifference between the new JIT(3) speed of R vs pre-compiling your functions\nthat contain for loops. The first run of the non-compiled function is not\nbyte-compiled, however all subsequent runs are byte-compiled. Hence one outlier\nbut otherwise very similar distributions.","feature_image":null,"featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-06-17 11:30:01","created_by":"1","updated_at":"2019-08-04 13:07:08","updated_by":null,"published_at":"2017-06-18 03:19:21","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcea","uuid":"3e886023-4f02-4a1b-82af-a70e5f03d1e5","title":"Binary Classification in Python - Who's Going to Leave Next?","slug":"whos-going-to-leave-next","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"This post goes through a binary classification problem with Python's machine learning library [scikit-learn](http://scikit-learn.org/). \\n\\n## Aim\\n>Create a model that predicts who is going to leave the organisation next. Commonly known as churn modelling.\\n\\nTo follow along, I breakdown each piece of the coding journey in this post. Alternatively, you can find a [complete copy of the code on github](https://github.com/ucg8j/kaggle_HR). You'll need the following packages loaded:\\n\\n```python\\nimport os\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom random import sample\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn import tree\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.cross_validation import cross_val_score\\nfrom sklearn import metrics\\nfrom IPython.display import Image  \\nfrom pydotplus import graph_from_dot_data\\n```\\n\\nThe dataset for this exercise was found on [kaggle](https://www.kaggle.com/ludobenistant/hr-analytics/downloads/human-resources-analytics.zip). Once unzipped, I read in the data:\\n\\n```python\\n# Set working directory\\npath = os.path.expanduser('~/Projects/kaggle_HR/')\\nos.chdir(path)\\n# Read in the data\\ndf = pd.read_csv('HR_comma_sep.csv')\\n```\\n\\nIt contains data of 14,999 employees who are either in the organisation or have left, and 10 columns.\\n\\n```\\n>>> df.shape\\n(14999, 10)\\n>>> df.columns.tolist()\\n['satisfaction_level',\\n 'last_evaluation',\\n 'number_project',\\n 'average_montly_hours',\\n 'time_spend_company',\\n 'Work_accident',\\n 'left',\\n 'promotion_last_5years',\\n 'sales',\\n 'salary']\\n```\\n\\nWe need to get some sense of how balanced our dataset is...\\n\\n```python\\n# Some basic stats on the target variable\\nprint '# people left = {}'.format(len(df[df['left'] == 1]))\\n# people left = 3571\\n\\nprint '# people stayed = {}'.format(len(df[df['left'] == 0]))\\n# people stayed = 11428\\n\\nprint '% people left = {}%'.format(round(float(len(df[df['left'] == 1])) / len(df) * 100), 3)\\n# % people left = 24.0%\\n```\\n\\nKnowing that 76% of the workforce will stay helps us not be excited by machine learning diagnostics that aren't much better than 76%. If I predicted that **all** staff would remain in the organisation, I would be 76% accurate!\\n\\nI next check for missing values.\\n\\n```python\\n>>> df.apply(lambda x: sum(x.isnull()), axis=0)\\nsatisfaction_level       0\\nlast_evaluation          0\\nnumber_project           0\\naverage_montly_hours     0\\ntime_spend_company       0\\nWork_accident            0\\nleft                     0\\npromotion_last_5years    0\\nsales                    0\\nsalary                   0\\n```\\n\\nNo missing values, that's great. Next I look for correlations in the data.\\n\\n```python\\n# Correlation heatmap\\ncorrelation = df.corr()\\nplt.figure(figsize=(10, 10))\\nsns.heatmap(correlation, vmax=1, square=True, annot=True, cmap='cubehelix')\\n```\\n![](/content/images/2017/07/heatmap_correl.png)\\n\\nUnsurprisingly, `satisfaction_level` has the largest correlation with the decision to stay or leave. Next up, pairwise plots provide a lot information for one diagram.\\n\\n```python\\n# take a 5% sample as this is computationally expensive\\ndf_sample = df.sample(frac=0.05)\\n# Pairwise plots\\npplot = sns.pairplot(df_sample, hue=\\\"left\\\")\\n```\\n![](/content/images/2017/07/pairwise.png)\\n\\nOn the diagonal we can see the distribution of each variable, including the target variable of who has left (this visualises the balance). There are some interesting groupings of those who leave (Green dots) when looking at the plots of `satisfaction_level` by `last_evaluation` and by `average_montly_hours` - they look to be the same sub-populations. E.g. Low satisfaction but high evaluation and monthly hours, there's also a grouping of a high number of projects completed with low satisfaction levels by those who leave. Perhaps these are *high performers* that easily leave when they are not satisfied by their job. There are other interesting groupings so it's worth spending some looking at this output examining these relationships. It's great to have these groupings, as our classifier should be able to differentiate between those who leave and those that remain. If there were no relationships revealed in this plot, we would face a very difficult problem and perhaps have to go back to examine what data we could collect to help predict the outcome.\\n\\nTwo variables were left out of the above plots, `Salary` and `Sales`. I need to deal with these separately prior to modelling, as our machine learning classifier will expect numbers to crunch rather than text. \\n\\n```python\\n>>> df.salary.unique()\\narray(['low', 'medium', 'high'], dtype=object)\\n>>> df.sales.unique()\\narray(['sales', 'accounting', 'hr', 'technical', 'support', 'management', 'IT', 'product_mng', 'marketing', 'RandD'], dtype=object)\\n```\\n\\n`Salary` has 3 values *low*, *medium* and *high*. These factor variables have an order. I'm going to go use dummy encoding as a simple and common method to handle factor variables. The downsides to this is I am not handling the *ordering* and will introduce extra dimensions to the data (see [Curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) for why this is bad). Helmert, Forward/Backward differencing or Orthogonal encoding would *probably* be better.\\n\\n---\\n\\n#### As an aside...\\n`R` makes factors really easy to handle. For example, let's say we have data with an unordered factor variable `df$unordered.variable`:\\n\\n```r\\n# Order a factor variable in R\\ndf$ordered.var <- factor(df$unordered.var\\n                        , levels = c('low', 'medium', 'high')\\n```\\n---\\n## Dummy Encoding\\nDummy encoding, or *one hot encoding*, transforms categorical variables into a series of binary columns. Before I one hot encode the `sales` and `salary` I prepend the column names to the categories, that way I know later which column each new column came from. This helps particularly in cases where the columns use the same category names e.g. 'high' could apply to `sales` and `salary`.\\n\\n```python\\n# Prepend string prior to encoding\\ndf['salary'] = '$_' + df['salary'].astype(str)\\ndf['sales'] = 'sales_' + df['sales'].astype(str)\\n\\n# Create 'salary' dummies and join\\none_hot_salary = pd.get_dummies(df['salary'])\\ndf = df.join(one_hot_salary)\\n\\n# Create 'sales' dummies and join\\none_hot_sales = pd.get_dummies(df['sales'])\\ndf = df.join(one_hot_sales)\\n```\\n\\nIf we proceeded to modelling without dropping columns, we would run into the *dummy variable trap*. To illustrate what the dummy variable trap is, let's say that our data has a column for `Gender` in the form `['male','female','male','female'...]`. If we use `pd.get_dummies()` \\n\\n```python\\n# Generate 'Gender' variable\\nGender = np.random.choice(['male','female'], 4)\\n# Create DataFrame\\ndf1 = pd.DataFrame(Gender, columns=['Gender'])\\n# One hot encode Gender\\ndummies = pd.get_dummies(df1['Gender'])\\n# Join dummies to 'df1'\\ndf1 = df1.join(dummies)\\nprint df1\\n   Gender  female  male\\n0    male       0     1\\n1  female       1     0\\n2    male       0     1\\n3  female       1     0\\n```\\n\\nWe can see that the dummy columns of `female` and `male` reflect the `Gender` column, a row with column `Gender='male'` has `female=0` and `male=1`. In modelling, we are looking for *independent* variables to predict a *dependent* outcome. One of our dummy variables is completely *dependent* on the other ([see Multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)). We can naturally drop one of the dummy variables to avoid this trap whilst not losing any valuable information.\\n\\nTo avoid the dummy variable trap in the Kaggle HR dataset, I'm dropping the columns, `sales_IT` and low salary `$_low`.\\n\\n```python\\n# Drop unnecessary columns to avoid the dummy variable trap\\ndf = df.drop(['salary', 'sales', '$_low', 'sales_IT'], axis=1)\\n```\\n\\nIf you're finding this content helpful why not...\\n\\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\\n  <link href=\\\"https://fonts.googleapis.com/css?family=Cookie\\\" rel=\\\"stylesheet\\\"><a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\\n\\n## Split Into Test and Training Sets\\nNow to split the dataset up into our training, testing and *optionally* our validation sets. This is best done randomly to avoid having one subgroup of the data overrepresented in either our training or testing datasets, hence `df.sample()`.\\n\\n```python\\n# Randomly, split the data into test/training/validation sets\\ntrain, test, validate = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\\nprint train.shape, test.shape, validate.shape\\n# (8999, 20) (3000, 20) (3000, 20)\\n\\n# Separate target and predictors\\ny_train = train['left']\\nx_train = train.drop(['left'], axis=1)\\ny_test = test['left']\\nx_test = test.drop(['left'], axis=1)\\ny_validate = validate['left']\\nx_validate = validate.drop(['left'], axis=1)\\n\\n# Check the balance of the splits on y_\\n>>> y_test.mean()\\n0.23933333333333334\\n>>> y_train.mean()\\n0.24091565729525502\\n```\\n\\nFeature importance can be ascertained from a random forest, alternatively [Principle Components Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) can be used.\\n\\n```python\\n# Variable importance\\nrf = RandomForestClassifier()\\nrf.fit(x_train, y_train)\\nprint \\\"Features sorted by their score:\\\"\\nprint sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), x_train), reverse=True)\\n#[(0.3523, 'satisfaction_level'), (0.1738, 'time_spend_company'), (0.1705, 'number_project'), (0.158, 'average_montly_hours'), (0.1043, 'last_evaluation'), (0.0077, '$_high'), (0.0065, 'Work_accident'), (0.0059, '$_medium'), (0.0042, 'sales_technical'), (0.0034, 'sales_sales'), (0.0028, 'sales_support'), (0.0024, 'sales_hr'), (0.0023, 'sales_accounting'), (0.0019, 'sales_management'), (0.0014, 'sales_RandD'), (0.001, 'sales_product_mng'), (0.001, 'sales_marketing'), (0.0006, 'promotion_last_5years')]\\n```\\n\\nA very rough plot of `importances` reveals that past the first 5 columns, we really don't get that much more variance explained by the additional columns. \\n![](/content/images/2017/07/rf_variable_importance-1.png)\\n\\nFrom hereon in I'll only use the first 5 variables.\\n\\n```python\\n# Create variable lists and drop\\nall_vars = x_train.columns.tolist()\\ntop_5_vars = ['satisfaction_level', 'number_project', 'time_spend_company', 'average_montly_hours', 'last_evaluation']\\nbottom_vars = [cols for cols in all_vars if cols not in top_5_vars]\\n\\n# Drop less important variables leaving the top_5\\nx_train    = x_train.drop(bottom_vars, axis=1)\\nx_test     = x_test.drop(bottom_vars, axis=1)\\nx_validate = x_validate.drop(bottom_vars, axis=1)\\n```\\n\\n## Model Evaluation Basics\\nThe formula for **accuracy** is:\\n![](/content/images/2017/07/2786000a854e3b49c4fb9d6f41a0d7c503469b77.svg)\\n\\n- **TP** is the number of true positives\\n\\n    *Predicted as leaving and they do*\\n    \\n- **TN** is the number of true negatives\\n\\n  *Predicted as remaining and they do*\\n\\n- **FP** is the number of false positives\\n\\n   *Predicted as leaving, but they aren't - whoops!*\\n\\n- **FN** is the number of false negatives\\n   \\n   *Predicted as remaining and they don't - eek!*\\n\\n\\nAs mentioned before, if we were aiming to predict those who remain (76%) then if we predicted 100%, we would have `(0 + 11428) / 14999` or 76% accurracy. As the [Accuracy Paradox wiki](https://en.wikipedia.org/wiki/Accuracy_paradox) states precision and recall are probably more appropriate evaluation metrics. Here's a really quick definition of sensitivity (AKA recall), specificity and precision:\\n\\n- **Sensitivity/Recall** - `TP / (TP + FN)` how well the model *recalls*/identifies those that will leave. AKA the true positive rate.\\n- **Specificity** - `TN / (TN + FP)` how well the model identifies those that will stay. \\n- **Precision** `TP / (TP + FP)` how believable is the model? A low precision model will alarm you to those who are leaving that are actually staying.\\n- **F1 score** `2 * (precision * recall)/(precision + recall)`is the harmonic mean betwen precision and recall or the *balance*.\\n\\nFor this problem, we are perhaps most interested in knowing who is going to leave next. That way an organisation can respond with workforce planning and recruitment activities. Therefore, precision and recall will be the metrics we focus on.\\n\\n>Intuitively, precision is the ability of the classifier not to label as positive a sample that is negative, and recall is the ability of the classifier to find all the positive samples.\\n\\nSource: [scikit-learn](http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-and-f-measures)\\n\\n## Modelling\\n### 1. Logistic Regression\\nStarting off with a very simple model, logistic regression.\\n\\n```python\\n# Instantiate\\nlogit_model = LogisticRegression()\\n# Fit\\nlogit_model = logit_model.fit(x_train, y_train)\\n# How accurate?\\nlogit_model.score(x_train, y_train)\\n#0.7874\\n\\n# How does it perform on the test dataset?\\n\\n# Predictions on the test dataset\\npredicted = pd.DataFrame(logit_model.predict(x_test))\\n# Probabilities on the test dataset\\nprobs = pd.DataFrame(logit_model.predict_proba(x_test))\\nprint metrics.accuracy_score(y_test, predicted)\\n0.785\\n```\\n\\nNow let's look at the confusion matrix and the metrics we care about.\\n\\n```python\\n>>> print metrics.confusion_matrix(y_test, predicted) \\n[[2100  182]\\n [ 463  255]]\\n>>> print metrics.classification_report(y_test, predicted)\\n             precision    recall  f1-score   support\\n\\n          0       0.82      0.92      0.87      2282\\n          1       0.58      0.36      0.44       718\\n\\navg / total       0.76      0.79      0.77      3000\\n```\\n\\nLet's double check the `classification_report`. Focusing on those  who will leave.\\n\\n>Precision = TP/(TP+FP) = 255/(255+182) = 0.58\\n\\n>Recall = TP/(TP+FN) = 255/(255+463) = 0.36\\n\\nPerhaps we could optimise this model, however there are a range of models to test that may perform better out-of-the-box. I'll quickly run through a range of models...\\n\\n### 2. Decision Tree\\n\\nDecision trees are highly interpretable and tend to perform well on classification problems.\\n\\n```python\\n# Instantiate with a max depth of 3\\ntree_model = tree.DecisionTreeClassifier(max_depth=3) \\n# Fit a decision tree\\ntree_model = tree_model.fit(x_train, y_train)\\n# Training accuracy\\ntree_model.score(x_train, y_train)\\n\\n# Predictions/probs on the test dataset\\npredicted = pd.DataFrame(tree_model.predict(x_test))\\nprobs = pd.DataFrame(tree_model.predict_proba(x_test))\\n\\n# Store metrics\\ntree_accuracy = metrics.accuracy_score(y_test, predicted)     \\ntree_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \\ntree_confus_matrix = metrics.confusion_matrix(y_test, predicted) \\ntree_classification_report = metrics.classification_report(y_test, predicted)\\ntree_precision = metrics.precision_score(y_test, predicted, pos_label=1)\\ntree_recall = metrics.recall_score(y_test, predicted, pos_label=1)\\ntree_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\\n\\n# evaluate the model using 10-fold cross-validation\\ntree_cv_scores = cross_val_score(tree.DecisionTreeClassifier(max_depth=3), x_test, y_test, scoring='precision', cv=10)\\n\\n# output decision plot\\ndot_data = tree.export_graphviz(tree_model, out_file=None, \\n                     feature_names=x_test.columns.tolist(),\\n                     class_names=['remain', 'left'],\\n                     filled=True, rounded=True,  \\n                     special_characters=True)  \\ngraph = graph_from_dot_data(dot_data)\\ngraph.write_png(\\\"images/decision_tree.png\\\")\\n```\\n\\n![](/content/images/2017/07/decision_tree.png)\\n\\n### 3. Random Forest\\n\\n```python\\n# Instantiate\\nrf = RandomForestClassifier()\\t   \\n# Fit\\nrf_model = rf.fit(x_train, y_train)\\n# training accuracy 99.74%\\nrf_model.score(x_train, y_train)\\n\\n# Predictions/probs on the test dataset\\npredicted = pd.DataFrame(rf_model.predict(x_test))\\nprobs = pd.DataFrame(rf_model.predict_proba(x_test))\\n\\n# Store metrics\\nrf_accuracy = metrics.accuracy_score(y_test, predicted)     \\nrf_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \\nrf_confus_matrix = metrics.confusion_matrix(y_test, predicted) \\nrf_classification_report = metrics.classification_report(y_test, predicted)\\nrf_precision = metrics.precision_score(y_test, predicted, pos_label=1)\\nrf_recall = metrics.recall_score(y_test, predicted, pos_label=1)\\nrf_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\\n\\n# Evaluate the model using 10-fold cross-validation\\nrf_cv_scores = cross_val_score(RandomForestClassifier(), x_test, y_test, scoring='precision', cv=10)\\nrf_cv_mean = np.mean(rf_cv_scores)\\n```\\n\\nThis has performed possibly a little too well! \\n\\n### 4. SUPPORT VECTOR MACHINE\\n```python\\n# Instantiate\\nsvm_model = SVC(probability=True)\\n# Fit\\nsvm_model = svm_model.fit(x_train, y_train)\\n# Accuracy\\nsvm_model.score(x_train, y_train)\\n\\n# Predictions/probs on the test dataset\\npredicted = pd.DataFrame(svm_model.predict(x_test))\\nprobs = pd.DataFrame(svm_model.predict_proba(x_test))\\n\\n# Store metrics\\nsvm_accuracy = metrics.accuracy_score(y_test, predicted)     \\nsvm_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \\nsvm_confus_matrix = metrics.confusion_matrix(y_test, predicted) \\nsvm_classification_report = metrics.classification_report(y_test, predicted)\\nsvm_precision = metrics.precision_score(y_test, predicted, pos_label=1)\\nsvm_recall = metrics.recall_score(y_test, predicted, pos_label=1)\\nsvm_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\\n\\n# Evaluate the model using 10-fold cross-validation\\nsvm_cv_scores = cross_val_score(SVC(probability=True), x_test, y_test, scoring='precision', cv=10)\\nsvm_cv_mean = np.mean(svm_cv_scores)\\n```\\n\\n### 5. KNN\\n\\n```python\\n# instantiate learning model (k = 3)\\nknn_model = KNeighborsClassifier(n_neighbors=3)\\n# fit the model\\nknn_model.fit(x_train, y_train)\\n# Accuracy\\nknn_model.score(x_train, y_train)\\n\\n# Predictions/probs on the test dataset\\npredicted = pd.DataFrame(knn_model.predict(x_test))\\nprobs = pd.DataFrame(knn_model.predict_proba(x_test))\\n\\n# Store metrics\\nknn_accuracy = metrics.accuracy_score(y_test, predicted)     \\nknn_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \\nknn_confus_matrix = metrics.confusion_matrix(y_test, predicted) \\nknn_classification_report = metrics.classification_report(y_test, predicted)\\nknn_precision = metrics.precision_score(y_test, predicted, pos_label=1)\\nknn_recall = metrics.recall_score(y_test, predicted, pos_label=1)\\nknn_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\\n\\n# Evaluate the model using 10-fold cross-validation\\nknn_cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=3), x_test, y_test, scoring='precision', cv=10)\\nknn_cv_mean = np.mean(knn_cv_scores)\\n```\\n\\n### 6. TWO CLASS BAYES\\n\\n```python\\n# Instantiate\\nbayes_model = GaussianNB()\\n# Fit the model\\nbayes_model.fit(x_train, y_train)\\n# Accuracy\\nbayes_model.score(x_train, y_train)\\n\\n# Predictions/probs on the test dataset\\npredicted = pd.DataFrame(bayes_model.predict(x_test))\\nprobs = pd.DataFrame(bayes_model.predict_proba(x_test))\\n\\n# Store metrics\\nbayes_accuracy = metrics.accuracy_score(y_test, predicted)     \\nbayes_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \\nbayes_confus_matrix = metrics.confusion_matrix(y_test, predicted) \\nbayes_classification_report = metrics.classification_report(y_test, predicted)\\nbayes_precision = metrics.precision_score(y_test, predicted, pos_label=1)\\nbayes_recall = metrics.recall_score(y_test, predicted, pos_label=1)\\nbayes_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\\n\\n# Evaluate the model using 10-fold cross-validation\\nbayes_cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=3), x_test, y_test, scoring='precision', cv=10)\\nbayes_cv_mean = np.mean(bayes_cv_scores)\\n```\\n\\n## Results\\n\\n```python\\n# Model comparison\\nmodels = pd.DataFrame({\\n  'Model': ['Logistic', 'd.Tree', 'r.f.', 'SVM', 'kNN',  'Bayes'],\\n  'Accuracy' : [logit_accuracy, tree_accuracy, rf_accuracy, svm_accuracy, knn_accuracy, bayes_accuracy],\\n  'Precision': [logit_precision, tree_precision, rf_precision, svm_precision, knn_precision, bayes_precision],\\n  'recall' : [logit_recall, tree_recall, rf_recall, svm_recall, knn_recall, bayes_recall],\\n  'F1' : [logit_f1, tree_f1, rf_f1, svm_f1, knn_f1, bayes_f1],\\n  'cv_precision' : [logit_cv_mean, tree_cv_mean, rf_cv_mean, svm_cv_mean, knn_cv_mean, bayes_cv_mean]\\n})\\n# Print table and sort by test precision\\nmodels.sort_values(by='Precision', ascending=False)\\n\\n   Model         F1 Accuracy Precision  cv_precision    recall\\n    r.f.   0.976812 0.989333  0.994100      0.988294  0.960114\\n  d.Tree   0.915088 0.959667  0.901798      0.899632  0.928775\\n     SVM   0.902098 0.953333  0.885989      0.863300  0.918803\\n     kNN   0.902959 0.953000  0.873502      0.831590  0.934473\\n   Bayes   0.530179 0.808000  0.620229      0.831590  0.462963\\nLogistic   0.327684 0.762000  0.483333      0.526624  0.247863\\n```\\n\\nFrom the above results table, it is clear the Random Forest is the best model based on Accuracy, Precision and Recall metrics. The performance is so good, that I would be concerned about overfitting and a lack of generalisation to future periods of data. However, the classifier was evaluated with K-folds cross-validation (K=10) and also evaluated on the test dataset.\\n\\nTo use the trained model later, it's first best practice to re-train on the entire dataset to get the best trained model. Then save the pickle i.e. serialised model.\\n```python\\n# Create x and y from all data\\ny = df['left']\\nx = df.drop(['left'], axis=1)\\n\\n# Re-train model on all data\\nrf_model = rf.fit(x, y)\\n\\n# Save model\\nimport cPickle\\nwith open('churn_classifier.pkl', 'wb') as fid:\\n    cPickle.dump(rf_model, fid)\\n```\\n\\nThe unabridged version of the above code can be found [here on github](https://github.com/ucg8j/kaggle_HR).\\n\\nIf you've found this content helpful why not...\\n\\n<a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\\n\\n## Good Resources\\n\\n- [Contrasts/Encoding in Python](http://www.statsmodels.org/dev/contrasts.html)\\n\\n- [UCLA's Contrasts/Encoding in SAS](https://stats.idre.ucla.edu/sas/webbooks/reg/chapter5/regression-with-saschapter-5-additional-coding-systems-for-categorical-variables-in-regressionanalysis/)\\n\\n- [Contrasts/Encoding in R](http://rstudio-pubs-static.s3.amazonaws.com/65059_586f394d8eb84f84b1baaf56ffb6b47f.html)\\n\\n- [Sci-kit Learn's \\\"Preprocessing\\\" Doc](http://scikit-learn.org/stable/modules/preprocessing.html)\\n\\n- [Simple Explanation of the Dummy Variable Trap](http://www.algosome.com/articles/dummy-variable-trap-regression.html)\\n\\n- [Cross Validated: How can machine learning models be used for survival analysis?](https://stats.stackexchange.com/questions/199549/how-can-machine-learning-models-gbm-nn-etc-be-used-for-survival-analysis/263946#263946)\\n\\n- [Healthy criticism of the approach I took](https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/)\\n\\n- [A very well written (human speak) blog on accuracy vs. recall & precision](https://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/)\\n\\n- [Model Performance Cheat Sheet](http://www.damienfrancois.be/blog/files/modelperfcheatsheet.pdf)\\n\\n- [Very simple explanation of ML evaluation metrics with nice visuals](https://classeval.wordpress.com/introduction/)\\n\\n- [A good blog on KNN using Python](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/)\\n\\n- [Good process schema](http://www.datasciencecentral.com/profiles/blogs/customer-churn-logistic-regression-with-r)\\n\\n- [High level overview of using transactional data for churn modelling](https://www.capgemini.com/wp-content/uploads/2017/07/Predictive_Modeling_Using_Transactional_Data.pdf)\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>This post goes through a binary classification problem with Python's machine learning library <a href=\"http://scikit-learn.org/\">scikit-learn</a>.</p>\n<h2 id=\"aim\">Aim</h2>\n<blockquote>\n<p>Create a model that predicts who is going to leave the organisation next. Commonly known as churn modelling.</p>\n</blockquote>\n<p>To follow along, I breakdown each piece of the coding journey in this post. Alternatively, you can find a <a href=\"https://github.com/ucg8j/kaggle_HR\">complete copy of the code on github</a>. You'll need the following packages loaded:</p>\n<pre><code class=\"language-python\">import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom random import sample\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn import metrics\nfrom IPython.display import Image  \nfrom pydotplus import graph_from_dot_data\n</code></pre>\n<p>The dataset for this exercise was found on <a href=\"https://www.kaggle.com/ludobenistant/hr-analytics/downloads/human-resources-analytics.zip\">kaggle</a>. Once unzipped, I read in the data:</p>\n<pre><code class=\"language-python\"># Set working directory\npath = os.path.expanduser('~/Projects/kaggle_HR/')\nos.chdir(path)\n# Read in the data\ndf = pd.read_csv('HR_comma_sep.csv')\n</code></pre>\n<p>It contains data of 14,999 employees who are either in the organisation or have left, and 10 columns.</p>\n<pre><code>&gt;&gt;&gt; df.shape\n(14999, 10)\n&gt;&gt;&gt; df.columns.tolist()\n['satisfaction_level',\n 'last_evaluation',\n 'number_project',\n 'average_montly_hours',\n 'time_spend_company',\n 'Work_accident',\n 'left',\n 'promotion_last_5years',\n 'sales',\n 'salary']\n</code></pre>\n<p>We need to get some sense of how balanced our dataset is...</p>\n<pre><code class=\"language-python\"># Some basic stats on the target variable\nprint '# people left = {}'.format(len(df[df['left'] == 1]))\n# people left = 3571\n\nprint '# people stayed = {}'.format(len(df[df['left'] == 0]))\n# people stayed = 11428\n\nprint '% people left = {}%'.format(round(float(len(df[df['left'] == 1])) / len(df) * 100), 3)\n# % people left = 24.0%\n</code></pre>\n<p>Knowing that 76% of the workforce will stay helps us not be excited by machine learning diagnostics that aren't much better than 76%. If I predicted that <strong>all</strong> staff would remain in the organisation, I would be 76% accurate!</p>\n<p>I next check for missing values.</p>\n<pre><code class=\"language-python\">&gt;&gt;&gt; df.apply(lambda x: sum(x.isnull()), axis=0)\nsatisfaction_level       0\nlast_evaluation          0\nnumber_project           0\naverage_montly_hours     0\ntime_spend_company       0\nWork_accident            0\nleft                     0\npromotion_last_5years    0\nsales                    0\nsalary                   0\n</code></pre>\n<p>No missing values, that's great. Next I look for correlations in the data.</p>\n<pre><code class=\"language-python\"># Correlation heatmap\ncorrelation = df.corr()\nplt.figure(figsize=(10, 10))\nsns.heatmap(correlation, vmax=1, square=True, annot=True, cmap='cubehelix')\n</code></pre>\n<p><img src=\"/content/images/2017/07/heatmap_correl.png\" alt=\"\"></p>\n<p>Unsurprisingly, <code>satisfaction_level</code> has the largest correlation with the decision to stay or leave. Next up, pairwise plots provide a lot information for one diagram.</p>\n<pre><code class=\"language-python\"># take a 5% sample as this is computationally expensive\ndf_sample = df.sample(frac=0.05)\n# Pairwise plots\npplot = sns.pairplot(df_sample, hue=&quot;left&quot;)\n</code></pre>\n<p><img src=\"/content/images/2017/07/pairwise.png\" alt=\"\"></p>\n<p>On the diagonal we can see the distribution of each variable, including the target variable of who has left (this visualises the balance). There are some interesting groupings of those who leave (Green dots) when looking at the plots of <code>satisfaction_level</code> by <code>last_evaluation</code> and by <code>average_montly_hours</code> - they look to be the same sub-populations. E.g. Low satisfaction but high evaluation and monthly hours, there's also a grouping of a high number of projects completed with low satisfaction levels by those who leave. Perhaps these are <em>high performers</em> that easily leave when they are not satisfied by their job. There are other interesting groupings so it's worth spending some looking at this output examining these relationships. It's great to have these groupings, as our classifier should be able to differentiate between those who leave and those that remain. If there were no relationships revealed in this plot, we would face a very difficult problem and perhaps have to go back to examine what data we could collect to help predict the outcome.</p>\n<p>Two variables were left out of the above plots, <code>Salary</code> and <code>Sales</code>. I need to deal with these separately prior to modelling, as our machine learning classifier will expect numbers to crunch rather than text.</p>\n<pre><code class=\"language-python\">&gt;&gt;&gt; df.salary.unique()\narray(['low', 'medium', 'high'], dtype=object)\n&gt;&gt;&gt; df.sales.unique()\narray(['sales', 'accounting', 'hr', 'technical', 'support', 'management', 'IT', 'product_mng', 'marketing', 'RandD'], dtype=object)\n</code></pre>\n<p><code>Salary</code> has 3 values <em>low</em>, <em>medium</em> and <em>high</em>. These factor variables have an order. I'm going to go use dummy encoding as a simple and common method to handle factor variables. The downsides to this is I am not handling the <em>ordering</em> and will introduce extra dimensions to the data (see <a href=\"https://en.wikipedia.org/wiki/Curse_of_dimensionality\">Curse of dimensionality</a> for why this is bad). Helmert, Forward/Backward differencing or Orthogonal encoding would <em>probably</em> be better.</p>\n<hr>\n<h4 id=\"asanaside\">As an aside...</h4>\n<p><code>R</code> makes factors really easy to handle. For example, let's say we have data with an unordered factor variable <code>df$unordered.variable</code>:</p>\n<pre><code class=\"language-r\"># Order a factor variable in R\ndf$ordered.var &lt;- factor(df$unordered.var\n                        , levels = c('low', 'medium', 'high')\n</code></pre>\n<hr>\n<h2 id=\"dummyencoding\">Dummy Encoding</h2>\n<p>Dummy encoding, or <em>one hot encoding</em>, transforms categorical variables into a series of binary columns. Before I one hot encode the <code>sales</code> and <code>salary</code> I prepend the column names to the categories, that way I know later which column each new column came from. This helps particularly in cases where the columns use the same category names e.g. 'high' could apply to <code>sales</code> and <code>salary</code>.</p>\n<pre><code class=\"language-python\"># Prepend string prior to encoding\ndf['salary'] = '$_' + df['salary'].astype(str)\ndf['sales'] = 'sales_' + df['sales'].astype(str)\n\n# Create 'salary' dummies and join\none_hot_salary = pd.get_dummies(df['salary'])\ndf = df.join(one_hot_salary)\n\n# Create 'sales' dummies and join\none_hot_sales = pd.get_dummies(df['sales'])\ndf = df.join(one_hot_sales)\n</code></pre>\n<p>If we proceeded to modelling without dropping columns, we would run into the <em>dummy variable trap</em>. To illustrate what the dummy variable trap is, let's say that our data has a column for <code>Gender</code> in the form <code>['male','female','male','female'...]</code>. If we use <code>pd.get_dummies()</code></p>\n<pre><code class=\"language-python\"># Generate 'Gender' variable\nGender = np.random.choice(['male','female'], 4)\n# Create DataFrame\ndf1 = pd.DataFrame(Gender, columns=['Gender'])\n# One hot encode Gender\ndummies = pd.get_dummies(df1['Gender'])\n# Join dummies to 'df1'\ndf1 = df1.join(dummies)\nprint df1\n   Gender  female  male\n0    male       0     1\n1  female       1     0\n2    male       0     1\n3  female       1     0\n</code></pre>\n<p>We can see that the dummy columns of <code>female</code> and <code>male</code> reflect the <code>Gender</code> column, a row with column <code>Gender='male'</code> has <code>female=0</code> and <code>male=1</code>. In modelling, we are looking for <em>independent</em> variables to predict a <em>dependent</em> outcome. One of our dummy variables is completely <em>dependent</em> on the other (<a href=\"https://en.wikipedia.org/wiki/Multicollinearity\">see Multicollinearity</a>). We can naturally drop one of the dummy variables to avoid this trap whilst not losing any valuable information.</p>\n<p>To avoid the dummy variable trap in the Kaggle HR dataset, I'm dropping the columns, <code>sales_IT</code> and low salary <code>$_low</code>.</p>\n<pre><code class=\"language-python\"># Drop unnecessary columns to avoid the dummy variable trap\ndf = df.drop(['salary', 'sales', '$_low', 'sales_IT'], axis=1)\n</code></pre>\n<p>If you're finding this content helpful why not...</p>\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\n  <link href=\"https://fonts.googleapis.com/css?family=Cookie\" rel=\"stylesheet\"><a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a>\n<h2 id=\"splitintotestandtrainingsets\">Split Into Test and Training Sets</h2>\n<p>Now to split the dataset up into our training, testing and <em>optionally</em> our validation sets. This is best done randomly to avoid having one subgroup of the data overrepresented in either our training or testing datasets, hence <code>df.sample()</code>.</p>\n<pre><code class=\"language-python\"># Randomly, split the data into test/training/validation sets\ntrain, test, validate = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\nprint train.shape, test.shape, validate.shape\n# (8999, 20) (3000, 20) (3000, 20)\n\n# Separate target and predictors\ny_train = train['left']\nx_train = train.drop(['left'], axis=1)\ny_test = test['left']\nx_test = test.drop(['left'], axis=1)\ny_validate = validate['left']\nx_validate = validate.drop(['left'], axis=1)\n\n# Check the balance of the splits on y_\n&gt;&gt;&gt; y_test.mean()\n0.23933333333333334\n&gt;&gt;&gt; y_train.mean()\n0.24091565729525502\n</code></pre>\n<p>Feature importance can be ascertained from a random forest, alternatively <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">Principle Components Analysis (PCA)</a> can be used.</p>\n<pre><code class=\"language-python\"># Variable importance\nrf = RandomForestClassifier()\nrf.fit(x_train, y_train)\nprint &quot;Features sorted by their score:&quot;\nprint sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), x_train), reverse=True)\n#[(0.3523, 'satisfaction_level'), (0.1738, 'time_spend_company'), (0.1705, 'number_project'), (0.158, 'average_montly_hours'), (0.1043, 'last_evaluation'), (0.0077, '$_high'), (0.0065, 'Work_accident'), (0.0059, '$_medium'), (0.0042, 'sales_technical'), (0.0034, 'sales_sales'), (0.0028, 'sales_support'), (0.0024, 'sales_hr'), (0.0023, 'sales_accounting'), (0.0019, 'sales_management'), (0.0014, 'sales_RandD'), (0.001, 'sales_product_mng'), (0.001, 'sales_marketing'), (0.0006, 'promotion_last_5years')]\n</code></pre>\n<p>A very rough plot of <code>importances</code> reveals that past the first 5 columns, we really don't get that much more variance explained by the additional columns.<br>\n<img src=\"/content/images/2017/07/rf_variable_importance-1.png\" alt=\"\"></p>\n<p>From hereon in I'll only use the first 5 variables.</p>\n<pre><code class=\"language-python\"># Create variable lists and drop\nall_vars = x_train.columns.tolist()\ntop_5_vars = ['satisfaction_level', 'number_project', 'time_spend_company', 'average_montly_hours', 'last_evaluation']\nbottom_vars = [cols for cols in all_vars if cols not in top_5_vars]\n\n# Drop less important variables leaving the top_5\nx_train    = x_train.drop(bottom_vars, axis=1)\nx_test     = x_test.drop(bottom_vars, axis=1)\nx_validate = x_validate.drop(bottom_vars, axis=1)\n</code></pre>\n<h2 id=\"modelevaluationbasics\">Model Evaluation Basics</h2>\n<p>The formula for <strong>accuracy</strong> is:<br>\n<img src=\"/content/images/2017/07/2786000a854e3b49c4fb9d6f41a0d7c503469b77.svg\" alt=\"\"></p>\n<ul>\n<li>\n<p><strong>TP</strong> is the number of true positives</p>\n<p><em>Predicted as leaving and they do</em></p>\n</li>\n<li>\n<p><strong>TN</strong> is the number of true negatives</p>\n<p><em>Predicted as remaining and they do</em></p>\n</li>\n<li>\n<p><strong>FP</strong> is the number of false positives</p>\n<p><em>Predicted as leaving, but they aren't - whoops!</em></p>\n</li>\n<li>\n<p><strong>FN</strong> is the number of false negatives</p>\n<p><em>Predicted as remaining and they don't - eek!</em></p>\n</li>\n</ul>\n<p>As mentioned before, if we were aiming to predict those who remain (76%) then if we predicted 100%, we would have <code>(0 + 11428) / 14999</code> or 76% accurracy. As the <a href=\"https://en.wikipedia.org/wiki/Accuracy_paradox\">Accuracy Paradox wiki</a> states precision and recall are probably more appropriate evaluation metrics. Here's a really quick definition of sensitivity (AKA recall), specificity and precision:</p>\n<ul>\n<li><strong>Sensitivity/Recall</strong> - <code>TP / (TP + FN)</code> how well the model <em>recalls</em>/identifies those that will leave. AKA the true positive rate.</li>\n<li><strong>Specificity</strong> - <code>TN / (TN + FP)</code> how well the model identifies those that will stay.</li>\n<li><strong>Precision</strong> <code>TP / (TP + FP)</code> how believable is the model? A low precision model will alarm you to those who are leaving that are actually staying.</li>\n<li><strong>F1 score</strong> <code>2 * (precision * recall)/(precision + recall)</code>is the harmonic mean betwen precision and recall or the <em>balance</em>.</li>\n</ul>\n<p>For this problem, we are perhaps most interested in knowing who is going to leave next. That way an organisation can respond with workforce planning and recruitment activities. Therefore, precision and recall will be the metrics we focus on.</p>\n<blockquote>\n<p>Intuitively, precision is the ability of the classifier not to label as positive a sample that is negative, and recall is the ability of the classifier to find all the positive samples.</p>\n</blockquote>\n<p>Source: <a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-and-f-measures\">scikit-learn</a></p>\n<h2 id=\"modelling\">Modelling</h2>\n<h3 id=\"1logisticregression\">1. Logistic Regression</h3>\n<p>Starting off with a very simple model, logistic regression.</p>\n<pre><code class=\"language-python\"># Instantiate\nlogit_model = LogisticRegression()\n# Fit\nlogit_model = logit_model.fit(x_train, y_train)\n# How accurate?\nlogit_model.score(x_train, y_train)\n#0.7874\n\n# How does it perform on the test dataset?\n\n# Predictions on the test dataset\npredicted = pd.DataFrame(logit_model.predict(x_test))\n# Probabilities on the test dataset\nprobs = pd.DataFrame(logit_model.predict_proba(x_test))\nprint metrics.accuracy_score(y_test, predicted)\n0.785\n</code></pre>\n<p>Now let's look at the confusion matrix and the metrics we care about.</p>\n<pre><code class=\"language-python\">&gt;&gt;&gt; print metrics.confusion_matrix(y_test, predicted) \n[[2100  182]\n [ 463  255]]\n&gt;&gt;&gt; print metrics.classification_report(y_test, predicted)\n             precision    recall  f1-score   support\n\n          0       0.82      0.92      0.87      2282\n          1       0.58      0.36      0.44       718\n\navg / total       0.76      0.79      0.77      3000\n</code></pre>\n<p>Let's double check the <code>classification_report</code>. Focusing on those  who will leave.</p>\n<blockquote>\n<p>Precision = TP/(TP+FP) = 255/(255+182) = 0.58</p>\n</blockquote>\n<blockquote>\n<p>Recall = TP/(TP+FN) = 255/(255+463) = 0.36</p>\n</blockquote>\n<p>Perhaps we could optimise this model, however there are a range of models to test that may perform better out-of-the-box. I'll quickly run through a range of models...</p>\n<h3 id=\"2decisiontree\">2. Decision Tree</h3>\n<p>Decision trees are highly interpretable and tend to perform well on classification problems.</p>\n<pre><code class=\"language-python\"># Instantiate with a max depth of 3\ntree_model = tree.DecisionTreeClassifier(max_depth=3) \n# Fit a decision tree\ntree_model = tree_model.fit(x_train, y_train)\n# Training accuracy\ntree_model.score(x_train, y_train)\n\n# Predictions/probs on the test dataset\npredicted = pd.DataFrame(tree_model.predict(x_test))\nprobs = pd.DataFrame(tree_model.predict_proba(x_test))\n\n# Store metrics\ntree_accuracy = metrics.accuracy_score(y_test, predicted)     \ntree_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \ntree_confus_matrix = metrics.confusion_matrix(y_test, predicted) \ntree_classification_report = metrics.classification_report(y_test, predicted)\ntree_precision = metrics.precision_score(y_test, predicted, pos_label=1)\ntree_recall = metrics.recall_score(y_test, predicted, pos_label=1)\ntree_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n\n# evaluate the model using 10-fold cross-validation\ntree_cv_scores = cross_val_score(tree.DecisionTreeClassifier(max_depth=3), x_test, y_test, scoring='precision', cv=10)\n\n# output decision plot\ndot_data = tree.export_graphviz(tree_model, out_file=None, \n                     feature_names=x_test.columns.tolist(),\n                     class_names=['remain', 'left'],\n                     filled=True, rounded=True,  \n                     special_characters=True)  \ngraph = graph_from_dot_data(dot_data)\ngraph.write_png(&quot;images/decision_tree.png&quot;)\n</code></pre>\n<p><img src=\"/content/images/2017/07/decision_tree.png\" alt=\"\"></p>\n<h3 id=\"3randomforest\">3. Random Forest</h3>\n<pre><code class=\"language-python\"># Instantiate\nrf = RandomForestClassifier()\t   \n# Fit\nrf_model = rf.fit(x_train, y_train)\n# training accuracy 99.74%\nrf_model.score(x_train, y_train)\n\n# Predictions/probs on the test dataset\npredicted = pd.DataFrame(rf_model.predict(x_test))\nprobs = pd.DataFrame(rf_model.predict_proba(x_test))\n\n# Store metrics\nrf_accuracy = metrics.accuracy_score(y_test, predicted)     \nrf_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \nrf_confus_matrix = metrics.confusion_matrix(y_test, predicted) \nrf_classification_report = metrics.classification_report(y_test, predicted)\nrf_precision = metrics.precision_score(y_test, predicted, pos_label=1)\nrf_recall = metrics.recall_score(y_test, predicted, pos_label=1)\nrf_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n\n# Evaluate the model using 10-fold cross-validation\nrf_cv_scores = cross_val_score(RandomForestClassifier(), x_test, y_test, scoring='precision', cv=10)\nrf_cv_mean = np.mean(rf_cv_scores)\n</code></pre>\n<p>This has performed possibly a little too well!</p>\n<h3 id=\"4supportvectormachine\">4. SUPPORT VECTOR MACHINE</h3>\n<pre><code class=\"language-python\"># Instantiate\nsvm_model = SVC(probability=True)\n# Fit\nsvm_model = svm_model.fit(x_train, y_train)\n# Accuracy\nsvm_model.score(x_train, y_train)\n\n# Predictions/probs on the test dataset\npredicted = pd.DataFrame(svm_model.predict(x_test))\nprobs = pd.DataFrame(svm_model.predict_proba(x_test))\n\n# Store metrics\nsvm_accuracy = metrics.accuracy_score(y_test, predicted)     \nsvm_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \nsvm_confus_matrix = metrics.confusion_matrix(y_test, predicted) \nsvm_classification_report = metrics.classification_report(y_test, predicted)\nsvm_precision = metrics.precision_score(y_test, predicted, pos_label=1)\nsvm_recall = metrics.recall_score(y_test, predicted, pos_label=1)\nsvm_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n\n# Evaluate the model using 10-fold cross-validation\nsvm_cv_scores = cross_val_score(SVC(probability=True), x_test, y_test, scoring='precision', cv=10)\nsvm_cv_mean = np.mean(svm_cv_scores)\n</code></pre>\n<h3 id=\"5knn\">5. KNN</h3>\n<pre><code class=\"language-python\"># instantiate learning model (k = 3)\nknn_model = KNeighborsClassifier(n_neighbors=3)\n# fit the model\nknn_model.fit(x_train, y_train)\n# Accuracy\nknn_model.score(x_train, y_train)\n\n# Predictions/probs on the test dataset\npredicted = pd.DataFrame(knn_model.predict(x_test))\nprobs = pd.DataFrame(knn_model.predict_proba(x_test))\n\n# Store metrics\nknn_accuracy = metrics.accuracy_score(y_test, predicted)     \nknn_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \nknn_confus_matrix = metrics.confusion_matrix(y_test, predicted) \nknn_classification_report = metrics.classification_report(y_test, predicted)\nknn_precision = metrics.precision_score(y_test, predicted, pos_label=1)\nknn_recall = metrics.recall_score(y_test, predicted, pos_label=1)\nknn_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n\n# Evaluate the model using 10-fold cross-validation\nknn_cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=3), x_test, y_test, scoring='precision', cv=10)\nknn_cv_mean = np.mean(knn_cv_scores)\n</code></pre>\n<h3 id=\"6twoclassbayes\">6. TWO CLASS BAYES</h3>\n<pre><code class=\"language-python\"># Instantiate\nbayes_model = GaussianNB()\n# Fit the model\nbayes_model.fit(x_train, y_train)\n# Accuracy\nbayes_model.score(x_train, y_train)\n\n# Predictions/probs on the test dataset\npredicted = pd.DataFrame(bayes_model.predict(x_test))\nprobs = pd.DataFrame(bayes_model.predict_proba(x_test))\n\n# Store metrics\nbayes_accuracy = metrics.accuracy_score(y_test, predicted)     \nbayes_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \nbayes_confus_matrix = metrics.confusion_matrix(y_test, predicted) \nbayes_classification_report = metrics.classification_report(y_test, predicted)\nbayes_precision = metrics.precision_score(y_test, predicted, pos_label=1)\nbayes_recall = metrics.recall_score(y_test, predicted, pos_label=1)\nbayes_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n\n# Evaluate the model using 10-fold cross-validation\nbayes_cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=3), x_test, y_test, scoring='precision', cv=10)\nbayes_cv_mean = np.mean(bayes_cv_scores)\n</code></pre>\n<h2 id=\"results\">Results</h2>\n<pre><code class=\"language-python\"># Model comparison\nmodels = pd.DataFrame({\n  'Model': ['Logistic', 'd.Tree', 'r.f.', 'SVM', 'kNN',  'Bayes'],\n  'Accuracy' : [logit_accuracy, tree_accuracy, rf_accuracy, svm_accuracy, knn_accuracy, bayes_accuracy],\n  'Precision': [logit_precision, tree_precision, rf_precision, svm_precision, knn_precision, bayes_precision],\n  'recall' : [logit_recall, tree_recall, rf_recall, svm_recall, knn_recall, bayes_recall],\n  'F1' : [logit_f1, tree_f1, rf_f1, svm_f1, knn_f1, bayes_f1],\n  'cv_precision' : [logit_cv_mean, tree_cv_mean, rf_cv_mean, svm_cv_mean, knn_cv_mean, bayes_cv_mean]\n})\n# Print table and sort by test precision\nmodels.sort_values(by='Precision', ascending=False)\n\n   Model         F1 Accuracy Precision  cv_precision    recall\n    r.f.   0.976812 0.989333  0.994100      0.988294  0.960114\n  d.Tree   0.915088 0.959667  0.901798      0.899632  0.928775\n     SVM   0.902098 0.953333  0.885989      0.863300  0.918803\n     kNN   0.902959 0.953000  0.873502      0.831590  0.934473\n   Bayes   0.530179 0.808000  0.620229      0.831590  0.462963\nLogistic   0.327684 0.762000  0.483333      0.526624  0.247863\n</code></pre>\n<p>From the above results table, it is clear the Random Forest is the best model based on Accuracy, Precision and Recall metrics. The performance is so good, that I would be concerned about overfitting and a lack of generalisation to future periods of data. However, the classifier was evaluated with K-folds cross-validation (K=10) and also evaluated on the test dataset.</p>\n<p>To use the trained model later, it's first best practice to re-train on the entire dataset to get the best trained model. Then save the pickle i.e. serialised model.</p>\n<pre><code class=\"language-python\"># Create x and y from all data\ny = df['left']\nx = df.drop(['left'], axis=1)\n\n# Re-train model on all data\nrf_model = rf.fit(x, y)\n\n# Save model\nimport cPickle\nwith open('churn_classifier.pkl', 'wb') as fid:\n    cPickle.dump(rf_model, fid)\n</code></pre>\n<p>The unabridged version of the above code can be found <a href=\"https://github.com/ucg8j/kaggle_HR\">here on github</a>.</p>\n<p>If you've found this content helpful why not...</p>\n<a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a>\n<h2 id=\"goodresources\">Good Resources</h2>\n<ul>\n<li>\n<p><a href=\"http://www.statsmodels.org/dev/contrasts.html\">Contrasts/Encoding in Python</a></p>\n</li>\n<li>\n<p><a href=\"https://stats.idre.ucla.edu/sas/webbooks/reg/chapter5/regression-with-saschapter-5-additional-coding-systems-for-categorical-variables-in-regressionanalysis/\">UCLA's Contrasts/Encoding in SAS</a></p>\n</li>\n<li>\n<p><a href=\"http://rstudio-pubs-static.s3.amazonaws.com/65059_586f394d8eb84f84b1baaf56ffb6b47f.html\">Contrasts/Encoding in R</a></p>\n</li>\n<li>\n<p><a href=\"http://scikit-learn.org/stable/modules/preprocessing.html\">Sci-kit Learn's &quot;Preprocessing&quot; Doc</a></p>\n</li>\n<li>\n<p><a href=\"http://www.algosome.com/articles/dummy-variable-trap-regression.html\">Simple Explanation of the Dummy Variable Trap</a></p>\n</li>\n<li>\n<p><a href=\"https://stats.stackexchange.com/questions/199549/how-can-machine-learning-models-gbm-nn-etc-be-used-for-survival-analysis/263946#263946\">Cross Validated: How can machine learning models be used for survival analysis?</a></p>\n</li>\n<li>\n<p><a href=\"https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/\">Healthy criticism of the approach I took</a></p>\n</li>\n<li>\n<p><a href=\"https://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/\">A very well written (human speak) blog on accuracy vs. recall &amp; precision</a></p>\n</li>\n<li>\n<p><a href=\"http://www.damienfrancois.be/blog/files/modelperfcheatsheet.pdf\">Model Performance Cheat Sheet</a></p>\n</li>\n<li>\n<p><a href=\"https://classeval.wordpress.com/introduction/\">Very simple explanation of ML evaluation metrics with nice visuals</a></p>\n</li>\n<li>\n<p><a href=\"https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/\">A good blog on KNN using Python</a></p>\n</li>\n<li>\n<p><a href=\"http://www.datasciencecentral.com/profiles/blogs/customer-churn-logistic-regression-with-r\">Good process schema</a></p>\n</li>\n<li>\n<p><a href=\"https://www.capgemini.com/wp-content/uploads/2017/07/Predictive_Modeling_Using_Transactional_Data.pdf\">High level overview of using transactional data for churn modelling</a></p>\n</li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"18","plaintext":"This post goes through a binary classification problem with Python's machine\nlearning library scikit-learn [http://scikit-learn.org/].\n\nAim\n> Create a model that predicts who is going to leave the organisation next.\nCommonly known as churn modelling.\n\n\nTo follow along, I breakdown each piece of the coding journey in this post.\nAlternatively, you can find a complete copy of the code on github\n[https://github.com/ucg8j/kaggle_HR]. You'll need the following packages loaded:\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom random import sample\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn import metrics\nfrom IPython.display import Image  \nfrom pydotplus import graph_from_dot_data\n\n\nThe dataset for this exercise was found on kaggle\n[https://www.kaggle.com/ludobenistant/hr-analytics/downloads/human-resources-analytics.zip]\n. Once unzipped, I read in the data:\n\n# Set working directory\npath = os.path.expanduser('~/Projects/kaggle_HR/')\nos.chdir(path)\n# Read in the data\ndf = pd.read_csv('HR_comma_sep.csv')\n\n\nIt contains data of 14,999 employees who are either in the organisation or have\nleft, and 10 columns.\n\n>>> df.shape\n(14999, 10)\n>>> df.columns.tolist()\n['satisfaction_level',\n 'last_evaluation',\n 'number_project',\n 'average_montly_hours',\n 'time_spend_company',\n 'Work_accident',\n 'left',\n 'promotion_last_5years',\n 'sales',\n 'salary']\n\n\nWe need to get some sense of how balanced our dataset is...\n\n# Some basic stats on the target variable\nprint '# people left = {}'.format(len(df[df['left'] == 1]))\n# people left = 3571\n\nprint '# people stayed = {}'.format(len(df[df['left'] == 0]))\n# people stayed = 11428\n\nprint '% people left = {}%'.format(round(float(len(df[df['left'] == 1])) / len(df) * 100), 3)\n# % people left = 24.0%\n\n\nKnowing that 76% of the workforce will stay helps us not be excited by machine\nlearning diagnostics that aren't much better than 76%. If I predicted that all \nstaff would remain in the organisation, I would be 76% accurate!\n\nI next check for missing values.\n\n>>> df.apply(lambda x: sum(x.isnull()), axis=0)\nsatisfaction_level       0\nlast_evaluation          0\nnumber_project           0\naverage_montly_hours     0\ntime_spend_company       0\nWork_accident            0\nleft                     0\npromotion_last_5years    0\nsales                    0\nsalary                   0\n\n\nNo missing values, that's great. Next I look for correlations in the data.\n\n# Correlation heatmap\ncorrelation = df.corr()\nplt.figure(figsize=(10, 10))\nsns.heatmap(correlation, vmax=1, square=True, annot=True, cmap='cubehelix')\n\n\n\n\nUnsurprisingly, satisfaction_level has the largest correlation with the decision\nto stay or leave. Next up, pairwise plots provide a lot information for one\ndiagram.\n\n# take a 5% sample as this is computationally expensive\ndf_sample = df.sample(frac=0.05)\n# Pairwise plots\npplot = sns.pairplot(df_sample, hue=\"left\")\n\n\n\n\nOn the diagonal we can see the distribution of each variable, including the\ntarget variable of who has left (this visualises the balance). There are some\ninteresting groupings of those who leave (Green dots) when looking at the plots\nof satisfaction_level by last_evaluation and by average_montly_hours - they look\nto be the same sub-populations. E.g. Low satisfaction but high evaluation and\nmonthly hours, there's also a grouping of a high number of projects completed\nwith low satisfaction levels by those who leave. Perhaps these are high\nperformers that easily leave when they are not satisfied by their job. There are\nother interesting groupings so it's worth spending some looking at this output\nexamining these relationships. It's great to have these groupings, as our\nclassifier should be able to differentiate between those who leave and those\nthat remain. If there were no relationships revealed in this plot, we would face\na very difficult problem and perhaps have to go back to examine what data we\ncould collect to help predict the outcome.\n\nTwo variables were left out of the above plots, Salary and Sales. I need to deal\nwith these separately prior to modelling, as our machine learning classifier\nwill expect numbers to crunch rather than text.\n\n>>> df.salary.unique()\narray(['low', 'medium', 'high'], dtype=object)\n>>> df.sales.unique()\narray(['sales', 'accounting', 'hr', 'technical', 'support', 'management', 'IT', 'product_mng', 'marketing', 'RandD'], dtype=object)\n\n\nSalary has 3 values low, medium and high. These factor variables have an order.\nI'm going to go use dummy encoding as a simple and common method to handle\nfactor variables. The downsides to this is I am not handling the ordering and\nwill introduce extra dimensions to the data (see Curse of dimensionality\n[https://en.wikipedia.org/wiki/Curse_of_dimensionality] for why this is bad).\nHelmert, Forward/Backward differencing or Orthogonal encoding would probably be\nbetter.\n\n\n--------------------------------------------------------------------------------\n\nAs an aside...\nR makes factors really easy to handle. For example, let's say we have data with\nan unordered factor variable df$unordered.variable:\n\n# Order a factor variable in R\ndf$ordered.var <- factor(df$unordered.var\n                        , levels = c('low', 'medium', 'high')\n\n\n\n--------------------------------------------------------------------------------\n\nDummy Encoding\nDummy encoding, or one hot encoding, transforms categorical variables into a\nseries of binary columns. Before I one hot encode the sales and salary I prepend\nthe column names to the categories, that way I know later which column each new\ncolumn came from. This helps particularly in cases where the columns use the\nsame category names e.g. 'high' could apply to sales and salary.\n\n# Prepend string prior to encoding\ndf['salary'] = '$_' + df['salary'].astype(str)\ndf['sales'] = 'sales_' + df['sales'].astype(str)\n\n# Create 'salary' dummies and join\none_hot_salary = pd.get_dummies(df['salary'])\ndf = df.join(one_hot_salary)\n\n# Create 'sales' dummies and join\none_hot_sales = pd.get_dummies(df['sales'])\ndf = df.join(one_hot_sales)\n\n\nIf we proceeded to modelling without dropping columns, we would run into the \ndummy variable trap. To illustrate what the dummy variable trap is, let's say\nthat our data has a column for Gender in the form \n['male','female','male','female'...]. If we use pd.get_dummies()\n\n# Generate 'Gender' variable\nGender = np.random.choice(['male','female'], 4)\n# Create DataFrame\ndf1 = pd.DataFrame(Gender, columns=['Gender'])\n# One hot encode Gender\ndummies = pd.get_dummies(df1['Gender'])\n# Join dummies to 'df1'\ndf1 = df1.join(dummies)\nprint df1\n   Gender  female  male\n0    male       0     1\n1  female       1     0\n2    male       0     1\n3  female       1     0\n\n\nWe can see that the dummy columns of female and male reflect the Gender column,\na row with column Gender='male' has female=0 and male=1. In modelling, we are\nlooking for independent variables to predict a dependent outcome. One of our\ndummy variables is completely dependent on the other (see Multicollinearity\n[https://en.wikipedia.org/wiki/Multicollinearity]). We can naturally drop one of\nthe dummy variables to avoid this trap whilst not losing any valuable\ninformation.\n\nTo avoid the dummy variable trap in the Kaggle HR dataset, I'm dropping the\ncolumns, sales_IT and low salary $_low.\n\n# Drop unnecessary columns to avoid the dummy variable trap\ndf = df.drop(['salary', 'sales', '$_low', 'sales_IT'], axis=1)\n\n\nIf you're finding this content helpful why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]Split Into Test and\nTraining Sets\nNow to split the dataset up into our training, testing and optionally our\nvalidation sets. This is best done randomly to avoid having one subgroup of the\ndata overrepresented in either our training or testing datasets, hence \ndf.sample().\n\n# Randomly, split the data into test/training/validation sets\ntrain, test, validate = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\nprint train.shape, test.shape, validate.shape\n# (8999, 20) (3000, 20) (3000, 20)\n\n# Separate target and predictors\ny_train = train['left']\nx_train = train.drop(['left'], axis=1)\ny_test = test['left']\nx_test = test.drop(['left'], axis=1)\ny_validate = validate['left']\nx_validate = validate.drop(['left'], axis=1)\n\n# Check the balance of the splits on y_\n>>> y_test.mean()\n0.23933333333333334\n>>> y_train.mean()\n0.24091565729525502\n\n\nFeature importance can be ascertained from a random forest, alternatively \nPrinciple Components Analysis (PCA)\n[https://en.wikipedia.org/wiki/Principal_component_analysis] can be used.\n\n# Variable importance\nrf = RandomForestClassifier()\nrf.fit(x_train, y_train)\nprint \"Features sorted by their score:\"\nprint sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), x_train), reverse=True)\n#[(0.3523, 'satisfaction_level'), (0.1738, 'time_spend_company'), (0.1705, 'number_project'), (0.158, 'average_montly_hours'), (0.1043, 'last_evaluation'), (0.0077, '$_high'), (0.0065, 'Work_accident'), (0.0059, '$_medium'), (0.0042, 'sales_technical'), (0.0034, 'sales_sales'), (0.0028, 'sales_support'), (0.0024, 'sales_hr'), (0.0023, 'sales_accounting'), (0.0019, 'sales_management'), (0.0014, 'sales_RandD'), (0.001, 'sales_product_mng'), (0.001, 'sales_marketing'), (0.0006, 'promotion_last_5years')]\n\n\nA very rough plot of importances reveals that past the first 5 columns, we\nreally don't get that much more variance explained by the additional columns.\n\n\nFrom hereon in I'll only use the first 5 variables.\n\n# Create variable lists and drop\nall_vars = x_train.columns.tolist()\ntop_5_vars = ['satisfaction_level', 'number_project', 'time_spend_company', 'average_montly_hours', 'last_evaluation']\nbottom_vars = [cols for cols in all_vars if cols not in top_5_vars]\n\n# Drop less important variables leaving the top_5\nx_train    = x_train.drop(bottom_vars, axis=1)\nx_test     = x_test.drop(bottom_vars, axis=1)\nx_validate = x_validate.drop(bottom_vars, axis=1)\n\n\nModel Evaluation Basics\nThe formula for accuracy is:\n\n\n * TP is the number of true positives\n   \n   Predicted as leaving and they do\n   \n   \n * TN is the number of true negatives\n   \n   Predicted as remaining and they do\n   \n   \n * FP is the number of false positives\n   \n   Predicted as leaving, but they aren't - whoops!\n   \n   \n * FN is the number of false negatives\n   \n   Predicted as remaining and they don't - eek!\n   \n   \n\nAs mentioned before, if we were aiming to predict those who remain (76%) then if\nwe predicted 100%, we would have (0 + 11428) / 14999 or 76% accurracy. As the \nAccuracy Paradox wiki [https://en.wikipedia.org/wiki/Accuracy_paradox] states\nprecision and recall are probably more appropriate evaluation metrics. Here's a\nreally quick definition of sensitivity (AKA recall), specificity and precision:\n\n * Sensitivity/Recall - TP / (TP + FN) how well the model recalls/identifies\n   those that will leave. AKA the true positive rate.\n * Specificity - TN / (TN + FP) how well the model identifies those that will\n   stay.\n * Precision TP / (TP + FP) how believable is the model? A low precision model\n   will alarm you to those who are leaving that are actually staying.\n * F1 score 2 * (precision * recall)/(precision + recall)is the harmonic mean\n   betwen precision and recall or the balance.\n\nFor this problem, we are perhaps most interested in knowing who is going to\nleave next. That way an organisation can respond with workforce planning and\nrecruitment activities. Therefore, precision and recall will be the metrics we\nfocus on.\n\n> Intuitively, precision is the ability of the classifier not to label as positive\na sample that is negative, and recall is the ability of the classifier to find\nall the positive samples.\n\n\nSource: scikit-learn\n[http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-and-f-measures]\n\nModelling\n1. Logistic Regression\nStarting off with a very simple model, logistic regression.\n\n# Instantiate\nlogit_model = LogisticRegression()\n# Fit\nlogit_model = logit_model.fit(x_train, y_train)\n# How accurate?\nlogit_model.score(x_train, y_train)\n#0.7874\n\n# How does it perform on the test dataset?\n\n# Predictions on the test dataset\npredicted = pd.DataFrame(logit_model.predict(x_test))\n# Probabilities on the test dataset\nprobs = pd.DataFrame(logit_model.predict_proba(x_test))\nprint metrics.accuracy_score(y_test, predicted)\n0.785\n\n\nNow let's look at the confusion matrix and the metrics we care about.\n\n>>> print metrics.confusion_matrix(y_test, predicted) \n[[2100  182]\n [ 463  255]]\n>>> print metrics.classification_report(y_test, predicted)\n             precision    recall  f1-score   support\n\n          0       0.82      0.92      0.87      2282\n          1       0.58      0.36      0.44       718\n\navg / total       0.76      0.79      0.77      3000\n\n\nLet's double check the classification_report. Focusing on those who will leave.\n\n> Precision = TP/(TP+FP) = 255/(255+182) = 0.58\n\n\n> Recall = TP/(TP+FN) = 255/(255+463) = 0.36\n\n\nPerhaps we could optimise this model, however there are a range of models to\ntest that may perform better out-of-the-box. I'll quickly run through a range of\nmodels...\n\n2. Decision Tree\nDecision trees are highly interpretable and tend to perform well on\nclassification problems.\n\n# Instantiate with a max depth of 3\ntree_model = tree.DecisionTreeClassifier(max_depth=3) \n# Fit a decision tree\ntree_model = tree_model.fit(x_train, y_train)\n# Training accuracy\ntree_model.score(x_train, y_train)\n\n# Predictions/probs on the test dataset\npredicted = pd.DataFrame(tree_model.predict(x_test))\nprobs = pd.DataFrame(tree_model.predict_proba(x_test))\n\n# Store metrics\ntree_accuracy = metrics.accuracy_score(y_test, predicted)     \ntree_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \ntree_confus_matrix = metrics.confusion_matrix(y_test, predicted) \ntree_classification_report = metrics.classification_report(y_test, predicted)\ntree_precision = metrics.precision_score(y_test, predicted, pos_label=1)\ntree_recall = metrics.recall_score(y_test, predicted, pos_label=1)\ntree_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n\n# evaluate the model using 10-fold cross-validation\ntree_cv_scores = cross_val_score(tree.DecisionTreeClassifier(max_depth=3), x_test, y_test, scoring='precision', cv=10)\n\n# output decision plot\ndot_data = tree.export_graphviz(tree_model, out_file=None, \n                     feature_names=x_test.columns.tolist(),\n                     class_names=['remain', 'left'],\n                     filled=True, rounded=True,  \n                     special_characters=True)  \ngraph = graph_from_dot_data(dot_data)\ngraph.write_png(\"images/decision_tree.png\")\n\n\n\n\n3. Random Forest\n# Instantiate\nrf = RandomForestClassifier()\t   \n# Fit\nrf_model = rf.fit(x_train, y_train)\n# training accuracy 99.74%\nrf_model.score(x_train, y_train)\n\n# Predictions/probs on the test dataset\npredicted = pd.DataFrame(rf_model.predict(x_test))\nprobs = pd.DataFrame(rf_model.predict_proba(x_test))\n\n# Store metrics\nrf_accuracy = metrics.accuracy_score(y_test, predicted)     \nrf_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \nrf_confus_matrix = metrics.confusion_matrix(y_test, predicted) \nrf_classification_report = metrics.classification_report(y_test, predicted)\nrf_precision = metrics.precision_score(y_test, predicted, pos_label=1)\nrf_recall = metrics.recall_score(y_test, predicted, pos_label=1)\nrf_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n\n# Evaluate the model using 10-fold cross-validation\nrf_cv_scores = cross_val_score(RandomForestClassifier(), x_test, y_test, scoring='precision', cv=10)\nrf_cv_mean = np.mean(rf_cv_scores)\n\n\nThis has performed possibly a little too well!\n\n4. SUPPORT VECTOR MACHINE\n# Instantiate\nsvm_model = SVC(probability=True)\n# Fit\nsvm_model = svm_model.fit(x_train, y_train)\n# Accuracy\nsvm_model.score(x_train, y_train)\n\n# Predictions/probs on the test dataset\npredicted = pd.DataFrame(svm_model.predict(x_test))\nprobs = pd.DataFrame(svm_model.predict_proba(x_test))\n\n# Store metrics\nsvm_accuracy = metrics.accuracy_score(y_test, predicted)     \nsvm_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \nsvm_confus_matrix = metrics.confusion_matrix(y_test, predicted) \nsvm_classification_report = metrics.classification_report(y_test, predicted)\nsvm_precision = metrics.precision_score(y_test, predicted, pos_label=1)\nsvm_recall = metrics.recall_score(y_test, predicted, pos_label=1)\nsvm_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n\n# Evaluate the model using 10-fold cross-validation\nsvm_cv_scores = cross_val_score(SVC(probability=True), x_test, y_test, scoring='precision', cv=10)\nsvm_cv_mean = np.mean(svm_cv_scores)\n\n\n5. KNN\n# instantiate learning model (k = 3)\nknn_model = KNeighborsClassifier(n_neighbors=3)\n# fit the model\nknn_model.fit(x_train, y_train)\n# Accuracy\nknn_model.score(x_train, y_train)\n\n# Predictions/probs on the test dataset\npredicted = pd.DataFrame(knn_model.predict(x_test))\nprobs = pd.DataFrame(knn_model.predict_proba(x_test))\n\n# Store metrics\nknn_accuracy = metrics.accuracy_score(y_test, predicted)     \nknn_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \nknn_confus_matrix = metrics.confusion_matrix(y_test, predicted) \nknn_classification_report = metrics.classification_report(y_test, predicted)\nknn_precision = metrics.precision_score(y_test, predicted, pos_label=1)\nknn_recall = metrics.recall_score(y_test, predicted, pos_label=1)\nknn_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n\n# Evaluate the model using 10-fold cross-validation\nknn_cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=3), x_test, y_test, scoring='precision', cv=10)\nknn_cv_mean = np.mean(knn_cv_scores)\n\n\n6. TWO CLASS BAYES\n# Instantiate\nbayes_model = GaussianNB()\n# Fit the model\nbayes_model.fit(x_train, y_train)\n# Accuracy\nbayes_model.score(x_train, y_train)\n\n# Predictions/probs on the test dataset\npredicted = pd.DataFrame(bayes_model.predict(x_test))\nprobs = pd.DataFrame(bayes_model.predict_proba(x_test))\n\n# Store metrics\nbayes_accuracy = metrics.accuracy_score(y_test, predicted)     \nbayes_roc_auc = metrics.roc_auc_score(y_test, probs[1])       \nbayes_confus_matrix = metrics.confusion_matrix(y_test, predicted) \nbayes_classification_report = metrics.classification_report(y_test, predicted)\nbayes_precision = metrics.precision_score(y_test, predicted, pos_label=1)\nbayes_recall = metrics.recall_score(y_test, predicted, pos_label=1)\nbayes_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n\n# Evaluate the model using 10-fold cross-validation\nbayes_cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=3), x_test, y_test, scoring='precision', cv=10)\nbayes_cv_mean = np.mean(bayes_cv_scores)\n\n\nResults\n# Model comparison\nmodels = pd.DataFrame({\n  'Model': ['Logistic', 'd.Tree', 'r.f.', 'SVM', 'kNN',  'Bayes'],\n  'Accuracy' : [logit_accuracy, tree_accuracy, rf_accuracy, svm_accuracy, knn_accuracy, bayes_accuracy],\n  'Precision': [logit_precision, tree_precision, rf_precision, svm_precision, knn_precision, bayes_precision],\n  'recall' : [logit_recall, tree_recall, rf_recall, svm_recall, knn_recall, bayes_recall],\n  'F1' : [logit_f1, tree_f1, rf_f1, svm_f1, knn_f1, bayes_f1],\n  'cv_precision' : [logit_cv_mean, tree_cv_mean, rf_cv_mean, svm_cv_mean, knn_cv_mean, bayes_cv_mean]\n})\n# Print table and sort by test precision\nmodels.sort_values(by='Precision', ascending=False)\n\n   Model         F1 Accuracy Precision  cv_precision    recall\n    r.f.   0.976812 0.989333  0.994100      0.988294  0.960114\n  d.Tree   0.915088 0.959667  0.901798      0.899632  0.928775\n     SVM   0.902098 0.953333  0.885989      0.863300  0.918803\n     kNN   0.902959 0.953000  0.873502      0.831590  0.934473\n   Bayes   0.530179 0.808000  0.620229      0.831590  0.462963\nLogistic   0.327684 0.762000  0.483333      0.526624  0.247863\n\n\nFrom the above results table, it is clear the Random Forest is the best model\nbased on Accuracy, Precision and Recall metrics. The performance is so good,\nthat I would be concerned about overfitting and a lack of generalisation to\nfuture periods of data. However, the classifier was evaluated with K-folds\ncross-validation (K=10) and also evaluated on the test dataset.\n\nTo use the trained model later, it's first best practice to re-train on the\nentire dataset to get the best trained model. Then save the pickle i.e.\nserialised model.\n\n# Create x and y from all data\ny = df['left']\nx = df.drop(['left'], axis=1)\n\n# Re-train model on all data\nrf_model = rf.fit(x, y)\n\n# Save model\nimport cPickle\nwith open('churn_classifier.pkl', 'wb') as fid:\n    cPickle.dump(rf_model, fid)\n\n\nThe unabridged version of the above code can be found here on github\n[https://github.com/ucg8j/kaggle_HR].\n\nIf you've found this content helpful why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]Good Resources\n * Contrasts/Encoding in Python [http://www.statsmodels.org/dev/contrasts.html]\n   \n   \n * UCLA's Contrasts/Encoding in SAS\n   [https://stats.idre.ucla.edu/sas/webbooks/reg/chapter5/regression-with-saschapter-5-additional-coding-systems-for-categorical-variables-in-regressionanalysis/]\n   \n   \n * Contrasts/Encoding in R\n   [http://rstudio-pubs-static.s3.amazonaws.com/65059_586f394d8eb84f84b1baaf56ffb6b47f.html]\n   \n   \n * Sci-kit Learn's \"Preprocessing\" Doc\n   [http://scikit-learn.org/stable/modules/preprocessing.html]\n   \n   \n * Simple Explanation of the Dummy Variable Trap\n   [http://www.algosome.com/articles/dummy-variable-trap-regression.html]\n   \n   \n * Cross Validated: How can machine learning models be used for survival\n   analysis?\n   [https://stats.stackexchange.com/questions/199549/how-can-machine-learning-models-gbm-nn-etc-be-used-for-survival-analysis/263946#263946]\n   \n   \n * Healthy criticism of the approach I took\n   [https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/]\n   \n   \n * A very well written (human speak) blog on accuracy vs. recall & precision\n   [https://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/]\n   \n   \n * Model Performance Cheat Sheet\n   [http://www.damienfrancois.be/blog/files/modelperfcheatsheet.pdf]\n   \n   \n * Very simple explanation of ML evaluation metrics with nice visuals\n   [https://classeval.wordpress.com/introduction/]\n   \n   \n * A good blog on KNN using Python\n   [https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/]\n   \n   \n * Good process schema\n   [http://www.datasciencecentral.com/profiles/blogs/customer-churn-logistic-regression-with-r]\n   \n   \n * High level overview of using transactional data for churn modelling\n   [https://www.capgemini.com/wp-content/uploads/2017/07/Predictive_Modeling_Using_Transactional_Data.pdf]","feature_image":"/content/images/2017/11/bar-beach-sunset.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-06-30 08:19:25","created_by":"1","updated_at":"2019-08-04 13:12:11","updated_by":null,"published_at":"2017-07-12 11:41:53","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dceb","uuid":"7908224a-6be7-41b1-aa1f-8c1d451f8a37","title":"Rooting a Oneplus 3T Phone","slug":"rooting-a-oneplus-3t-phone","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I recently rooted my Oneplus 3T phone on Android 7.1.1. Finding the right information was not as easy as I thought it might be, so here's my summary. I used my macbook, you will need your USB cord to connect the phone. **Warning** - you will end up wiping your phone during this process, I'd recommend performing a backup prior. I didn't as I figured most app data are synced to the cloud, the only things I noticed missing was SMSs and things like numbers and Whatsapp history where a sync to the cloud occurred on a weekly basis.\\n\\n## 1. Unlock the [Bootloader](https://www.androidcentral.com/what-is-android-bootloader)\\n\\n a. If you haven't already done so, you will need to enable `Developer Options` on your phone by tapping 7x on the `Build Number` found in your phone under `Settings > About Phone > Build Number`. Once enabled, navigate to `Settings > Developer Options > USB debugging` and ensure this is toggled to ON.\\n\\nb. You will now need the to obtain the [Android Software Development Kit to run commands on your phone from your computer terminal](https://dl.google.com/android/android-sdk_r24.4.1-macosx.zip). This provides several tools you will use including [Android Development Bridge (ADB)](https://developer.android.com/studio/command-line/adb.html) and [Fastboot](https://www.androidcentral.com/android-z-what-fastboot). Once downloaded, unzip and place the files in a place where you can find them. Then navigate to the folder and launch the Android SDK manager by double clicking on `android-sdk-macosx/tools/android`. This will provide a GUI to install the tools needed as per the following screenshot.\\n\\n![](/content/images/2017/07/Screen-Shot-2017-07-05-at-14-16-57.png)\\n\\nNow you are ready to run some commands:\\n\\n```bash\\n# Navigate to the tools\\n$ cd /android-sdk-macosx/platform-tools\\n# Check that your phone is connected properly\\n$ adb devices \\nList of devices attached\\nfc031cd8\\tdevice\\n\\n# Reboot phone into bootloader\\n$ ./adb reboot bootloader\\n# Unlock the phone\\n$ ./fastboot oem unlock   \\n```\\n\\nThe last command will wipe your phone. Once complete you will then boot up the phone and need to repeat the process in step 1.a.\\n\\n## 2. Install TWRP Custom Recovery\\nI chose the open source [TWRP recovery software](https://en.wikipedia.org/wiki/TWRP). You can get a copy of TWRP [here](http://techerrata.com/browse/twrp2/bacon). Without the installation of TWRP, Step 3 will not be possible (I tried but the OnePlus recovery won't allow you to install SuperSu). \\n\\n```bash\\n# Check that your phone is connected properly\\n$ adb devices \\nList of devices attached\\nfc031cd8\\tdevice\\n\\n$ ./fastboot flash recovery /YOUR/PATH/TO/TWRP.img\\n$ ./fastboot reboot\\n```\\n\\n## 3. Root Via Flashing SuperSU \\nIf you haven't already [download Android File Transfer](https://www.android.com/filetransfer/). Transfer the [SuperSU zip](https://download.chainfire.eu/696/SuperSU/UPDATE-SuperSU-v2.46.zip) into the root of your phone. Ensure `Settings > Developer Options > Advanced Reboot` is toggled to on. Then press the power off button and select `Reboot > Recovery`. From here you will have the new TWRP menu to select `Install` and choose the SuperSu zip file. You will need to swipe right to confirm the Flash.\\n\\n### One Small Hiccup\\nI encountered one issue in this process. SuperSu was not intalling from the TWRP menu. I stopped the above process in between Step 2 and Step 3 as I had to go to bed for work the next day. On booting my phone for the day, I let my android install apps associated with my google account. Normally a great feature when signing into a new phone or one that has been formatted. However, the phone must be freshly wiped to be able to install SuperSu. So I had to separately wipe the phone again using the following commands found on this [SO answer](https://stackoverflow.com/a/39357553/3691003):\\n\\n```bash\\n$ ./adb reboot bootloader\\n# Wait a few seconds...\\n\\n# Check the phone is in bootloader\\n$ ./fastboot devices\\n# Wipe user data\\n$ ./fastboot -w\\n```\\n\\nIf you've found this content helpful why not...\\n\\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\\n  <link href=\\\"https://fonts.googleapis.com/css?family=Cookie\\\" rel=\\\"stylesheet\\\"><a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\\n\\n## Useful Resources\\n- [Android Explained](https://www.androidexplained.com/oneplus-3t-root/)\\n- [OnePlus Forums](https://forums.oneplus.net/threads/guide-for-mac-how-to-unlock-bootloader-install-custom-recovery-root-macbook-pro.279125/)\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>I recently rooted my Oneplus 3T phone on Android 7.1.1. Finding the right information was not as easy as I thought it might be, so here's my summary. I used my macbook, you will need your USB cord to connect the phone. <strong>Warning</strong> - you will end up wiping your phone during this process, I'd recommend performing a backup prior. I didn't as I figured most app data are synced to the cloud, the only things I noticed missing was SMSs and things like numbers and Whatsapp history where a sync to the cloud occurred on a weekly basis.</p>\n<h2 id=\"1unlockthebootloader\">1. Unlock the <a href=\"https://www.androidcentral.com/what-is-android-bootloader\">Bootloader</a></h2>\n<p>a. If you haven't already done so, you will need to enable <code>Developer Options</code> on your phone by tapping 7x on the <code>Build Number</code> found in your phone under <code>Settings &gt; About Phone &gt; Build Number</code>. Once enabled, navigate to <code>Settings &gt; Developer Options &gt; USB debugging</code> and ensure this is toggled to ON.</p>\n<p>b. You will now need the to obtain the <a href=\"https://dl.google.com/android/android-sdk_r24.4.1-macosx.zip\">Android Software Development Kit to run commands on your phone from your computer terminal</a>. This provides several tools you will use including <a href=\"https://developer.android.com/studio/command-line/adb.html\">Android Development Bridge (ADB)</a> and <a href=\"https://www.androidcentral.com/android-z-what-fastboot\">Fastboot</a>. Once downloaded, unzip and place the files in a place where you can find them. Then navigate to the folder and launch the Android SDK manager by double clicking on <code>android-sdk-macosx/tools/android</code>. This will provide a GUI to install the tools needed as per the following screenshot.</p>\n<p><img src=\"/content/images/2017/07/Screen-Shot-2017-07-05-at-14-16-57.png\" alt=\"\"></p>\n<p>Now you are ready to run some commands:</p>\n<pre><code class=\"language-bash\"># Navigate to the tools\n$ cd /android-sdk-macosx/platform-tools\n# Check that your phone is connected properly\n$ adb devices \nList of devices attached\nfc031cd8\tdevice\n\n# Reboot phone into bootloader\n$ ./adb reboot bootloader\n# Unlock the phone\n$ ./fastboot oem unlock   \n</code></pre>\n<p>The last command will wipe your phone. Once complete you will then boot up the phone and need to repeat the process in step 1.a.</p>\n<h2 id=\"2installtwrpcustomrecovery\">2. Install TWRP Custom Recovery</h2>\n<p>I chose the open source <a href=\"https://en.wikipedia.org/wiki/TWRP\">TWRP recovery software</a>. You can get a copy of TWRP <a href=\"http://techerrata.com/browse/twrp2/bacon\">here</a>. Without the installation of TWRP, Step 3 will not be possible (I tried but the OnePlus recovery won't allow you to install SuperSu).</p>\n<pre><code class=\"language-bash\"># Check that your phone is connected properly\n$ adb devices \nList of devices attached\nfc031cd8\tdevice\n\n$ ./fastboot flash recovery /YOUR/PATH/TO/TWRP.img\n$ ./fastboot reboot\n</code></pre>\n<h2 id=\"3rootviaflashingsupersu\">3. Root Via Flashing SuperSU</h2>\n<p>If you haven't already <a href=\"https://www.android.com/filetransfer/\">download Android File Transfer</a>. Transfer the <a href=\"https://download.chainfire.eu/696/SuperSU/UPDATE-SuperSU-v2.46.zip\">SuperSU zip</a> into the root of your phone. Ensure <code>Settings &gt; Developer Options &gt; Advanced Reboot</code> is toggled to on. Then press the power off button and select <code>Reboot &gt; Recovery</code>. From here you will have the new TWRP menu to select <code>Install</code> and choose the SuperSu zip file. You will need to swipe right to confirm the Flash.</p>\n<h3 id=\"onesmallhiccup\">One Small Hiccup</h3>\n<p>I encountered one issue in this process. SuperSu was not intalling from the TWRP menu. I stopped the above process in between Step 2 and Step 3 as I had to go to bed for work the next day. On booting my phone for the day, I let my android install apps associated with my google account. Normally a great feature when signing into a new phone or one that has been formatted. However, the phone must be freshly wiped to be able to install SuperSu. So I had to separately wipe the phone again using the following commands found on this <a href=\"https://stackoverflow.com/a/39357553/3691003\">SO answer</a>:</p>\n<pre><code class=\"language-bash\">$ ./adb reboot bootloader\n# Wait a few seconds...\n\n# Check the phone is in bootloader\n$ ./fastboot devices\n# Wipe user data\n$ ./fastboot -w\n</code></pre>\n<p>If you've found this content helpful why not...</p>\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\n  <link href=\"https://fonts.googleapis.com/css?family=Cookie\" rel=\"stylesheet\"><a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a>\n<h2 id=\"usefulresources\">Useful Resources</h2>\n<ul>\n<li><a href=\"https://www.androidexplained.com/oneplus-3t-root/\">Android Explained</a></li>\n<li><a href=\"https://forums.oneplus.net/threads/guide-for-mac-how-to-unlock-bootloader-install-custom-recovery-root-macbook-pro.279125/\">OnePlus Forums</a></li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"19","plaintext":"I recently rooted my Oneplus 3T phone on Android 7.1.1. Finding the right\ninformation was not as easy as I thought it might be, so here's my summary. I\nused my macbook, you will need your USB cord to connect the phone. Warning - you\nwill end up wiping your phone during this process, I'd recommend performing a\nbackup prior. I didn't as I figured most app data are synced to the cloud, the\nonly things I noticed missing was SMSs and things like numbers and Whatsapp\nhistory where a sync to the cloud occurred on a weekly basis.\n\n1. Unlock the Bootloader\n[https://www.androidcentral.com/what-is-android-bootloader]\na. If you haven't already done so, you will need to enable Developer Options on\nyour phone by tapping 7x on the Build Number found in your phone under Settings\n> About Phone > Build Number. Once enabled, navigate to Settings > Developer\nOptions > USB debugging and ensure this is toggled to ON.\n\nb. You will now need the to obtain the Android Software Development Kit to run\ncommands on your phone from your computer terminal\n[https://dl.google.com/android/android-sdk_r24.4.1-macosx.zip]. This provides\nseveral tools you will use including Android Development Bridge (ADB)\n[https://developer.android.com/studio/command-line/adb.html] and Fastboot\n[https://www.androidcentral.com/android-z-what-fastboot]. Once downloaded, unzip\nand place the files in a place where you can find them. Then navigate to the\nfolder and launch the Android SDK manager by double clicking on \nandroid-sdk-macosx/tools/android. This will provide a GUI to install the tools\nneeded as per the following screenshot.\n\n\n\nNow you are ready to run some commands:\n\n# Navigate to the tools\n$ cd /android-sdk-macosx/platform-tools\n# Check that your phone is connected properly\n$ adb devices \nList of devices attached\nfc031cd8\tdevice\n\n# Reboot phone into bootloader\n$ ./adb reboot bootloader\n# Unlock the phone\n$ ./fastboot oem unlock   \n\n\nThe last command will wipe your phone. Once complete you will then boot up the\nphone and need to repeat the process in step 1.a.\n\n2. Install TWRP Custom Recovery\nI chose the open source TWRP recovery software\n[https://en.wikipedia.org/wiki/TWRP]. You can get a copy of TWRP here\n[http://techerrata.com/browse/twrp2/bacon]. Without the installation of TWRP,\nStep 3 will not be possible (I tried but the OnePlus recovery won't allow you to\ninstall SuperSu).\n\n# Check that your phone is connected properly\n$ adb devices \nList of devices attached\nfc031cd8\tdevice\n\n$ ./fastboot flash recovery /YOUR/PATH/TO/TWRP.img\n$ ./fastboot reboot\n\n\n3. Root Via Flashing SuperSU\nIf you haven't already download Android File Transfer\n[https://www.android.com/filetransfer/]. Transfer the SuperSU zip\n[https://download.chainfire.eu/696/SuperSU/UPDATE-SuperSU-v2.46.zip] into the\nroot of your phone. Ensure Settings > Developer Options > Advanced Reboot is\ntoggled to on. Then press the power off button and select Reboot > Recovery.\nFrom here you will have the new TWRP menu to select Install and choose the\nSuperSu zip file. You will need to swipe right to confirm the Flash.\n\nOne Small Hiccup\nI encountered one issue in this process. SuperSu was not intalling from the TWRP\nmenu. I stopped the above process in between Step 2 and Step 3 as I had to go to\nbed for work the next day. On booting my phone for the day, I let my android\ninstall apps associated with my google account. Normally a great feature when\nsigning into a new phone or one that has been formatted. However, the phone must\nbe freshly wiped to be able to install SuperSu. So I had to separately wipe the\nphone again using the following commands found on this SO answer\n[https://stackoverflow.com/a/39357553/3691003]:\n\n$ ./adb reboot bootloader\n# Wait a few seconds...\n\n# Check the phone is in bootloader\n$ ./fastboot devices\n# Wipe user data\n$ ./fastboot -w\n\n\nIf you've found this content helpful why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]Useful Resources\n * Android Explained [https://www.androidexplained.com/oneplus-3t-root/]\n * OnePlus Forums\n   [https://forums.oneplus.net/threads/guide-for-mac-how-to-unlock-bootloader-install-custom-recovery-root-macbook-pro.279125/]","feature_image":null,"featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-07-05 03:42:16","created_by":"1","updated_at":"2019-08-04 13:06:46","updated_by":null,"published_at":"2017-07-05 13:46:57","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcec","uuid":"54e9c93e-d2f7-4ac0-8048-1b9a8c620017","title":"Highlight.js and Ghost","slug":"highlight-js-and-ghost","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"https://snarky.ca/adding-syntax-highlighting-to-ghosts-default-theme/\\n\\nPlus outline how to get the code specific highlighting through the cdn.\\n(see my own highlight.js r code injection in the 'GHOST>settings>code injection'\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><a href=\"https://snarky.ca/adding-syntax-highlighting-to-ghosts-default-theme/\">https://snarky.ca/adding-syntax-highlighting-to-ghosts-default-theme/</a></p>\n<p>Plus outline how to get the code specific highlighting through the cdn.<br>\n(see my own highlight.js r code injection in the 'GHOST&gt;settings&gt;code injection'</p>\n<!--kg-card-end: markdown-->","comment_id":"20","plaintext":"https://snarky.ca/adding-syntax-highlighting-to-ghosts-default-theme/\n\nPlus outline how to get the code specific highlighting through the cdn.\n(see my own highlight.js r code injection in the 'GHOST>settings>code injection'","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-07-08 06:51:29","created_by":"1","updated_at":"2017-07-08 06:52:28","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dced","uuid":"e213146b-5bb8-4663-8ad8-f14b98d827b9","title":"Feature Selection and Variable Importance Techniques","slug":"feature-selection-and-variable-importance-techniques","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Historical artefact that I use PCA and Random Forrests, I'll explain those and then other options available.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Historical artefact that I use PCA and Random Forrests, I'll explain those and then other options available.</p>\n<!--kg-card-end: markdown-->","comment_id":"21","plaintext":"Historical artefact that I use PCA and Random Forrests, I'll explain those and\nthen other options available.","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-07-15 07:32:22","created_by":"1","updated_at":"2017-07-15 07:34:00","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcee","uuid":"7ed0f30d-dcdd-43e1-9834-b62dbfa9e40a","title":"What are interaction terms?","slug":"what-are-interaction-terms","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}","html":null,"comment_id":"22","plaintext":null,"feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-07-15 07:35:47","created_by":"1","updated_at":"2017-07-15 07:35:47","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcef","uuid":"07af4bf6-e6e2-435d-91d3-9f56f7a16805","title":"Linear Algebra","slug":"linear-algebra","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"\\nhttps://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c\\n\\nbasics\\nhttp://www.datasciencecourse.org/matrices.pdf\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><a href=\"https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c\">https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c</a></p>\n<p>basics<br>\n<a href=\"http://www.datasciencecourse.org/matrices.pdf\">http://www.datasciencecourse.org/matrices.pdf</a></p>\n<!--kg-card-end: markdown-->","comment_id":"23","plaintext":"https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c\n\nbasics\nhttp://www.datasciencecourse.org/matrices.pdf","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-07-24 19:47:59","created_by":"1","updated_at":"2017-07-24 20:06:19","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcf0","uuid":"1b0d565b-2e0f-44b9-8321-bbd80b5da62f","title":"Dotfiles","slug":"dotfiles","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}","html":null,"comment_id":"24","plaintext":null,"feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-08-05 06:42:09","created_by":"1","updated_at":"2017-08-05 06:42:09","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcf1","uuid":"14c9c753-a4b9-4c2f-b1d5-88db519ed2f3","title":"Anonymous Functions in R and Python","slug":"anonymous-functions-in-r-python","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\">What's in a name? That which we call a rose by any other name would smell as sweet.\\n\\n\\n## Normal functions\\nBefore moving to anonymous functions, let's start with what normal functions look like.\\n\\n#### In R\\n```r\\n# Define a function\\nfunctionName <- function(variables) {\\n    # Function definition... do some stuff\\n    print(paste(variables, \\\"doing stuff\\\"))\\n}\\n\\n# Call the function\\n> functionName(\\\"boring strings\\\")\\n[1] \\\"boring strings doing stuff\\\"\\n```\\n\\n#### In Python\\n```python\\n# Define function\\ndef functionName(variables):\\n    # Do some stuff\\n    print variables, \\\"doing stuff\\\"\\n\\n# Call the function\\n>>> functionName(\\\"boring strings\\\")\\nboring strings doing stuff\\n```\\n\\n## Anonymous Functions\\nHave no identity, no name, but still do stuff! They will not live in the global environment. Like a person without a name, you would not be able to look the person up in the address book.\\n\\nThe anonymous function can be called like a normal function `functionName()`, except the functionName is switched for logic contained within parentheses `(fn logic goes here)()`.\\n\\n#### In R\\n```r\\n# Doing the same stuff anonymously\\n> (function(variables) print(paste(variables, \\\"doing stuff\\\")) )(\\\"code\\\")\\n[1] \\\"code doing stuff\\\"\\n```\\n\\n#### In Python\\nPython introduces the `lambda` keyword for anonymous functions, in contrast to R which sticks with the `function` keyword.\\n\\n```python\\n# Doing the same stuff anonymously\\n>>> (lambda variable: variable + \\\" doing stuff\\\")(\\\"code\\\")\\n'code doing stuff'\\n```\\n\\n### R Convention\\nThe most common convention is to use anonymous functions when using the `*apply` family of functions. For example, you might want to do an operation across a set of columns in a dataset. \\n\\n```r\\n# Create a dataset\\ndf <- data.frame(\\n  col1 = c(\\\"element1\\\", \\\"element2\\\"),\\n  col2 = c(\\\"element1\\\", \\\"element2\\\"), \\n  stringsAsFactors = FALSE\\n)\\n\\n# lapply an anonymous function to the columns of the dataset\\n> lapply(df, function(x) paste(x, \\\"doing stuff\\\"))\\n$col1\\n[1] \\\"element1 doing stuff\\\" \\\"element2 doing stuff\\\"\\n\\n$col2\\n[1] \\\"element1 doing stuff\\\" \\\"element2 doing stuff\\\"\\n```\\n\\n### Python Convention\\nDoing the exact operation as above in python:\\n\\n```python\\nimport pandas as pd\\n\\n# Create DataFrame\\ndf = pd.DataFrame(\\n    [[\\\"element1\\\", \\\"element1\\\"], [\\\"element2\\\", \\\"element2\\\"]],\\n    columns=['col1', 'col2']\\n)\\n\\n>>> df.apply(lambda x: x + \\\" doing stuff\\\", axis=0)\\n                   col1                  col2\\n0  element1 doing stuff  element1 doing stuff\\n1  element2 doing stuff  element2 doing stuff\\n```\\n\\nYou will generally see lambdas used with higher order functions like `map()`, `reduce()` and `filter()`. E.g.\\n\\n```python\\n# Map a anonymous function against all elements of df['col1']\\n>>> map(lambda x: x + \\\" doing stuff\\\", df['col1'])\\n['element1 doing stuff', 'element2 doing stuff']\\n```\\n\\nYou may also see a lambda used to define a function. e.g.\\n\\n```python\\nfunctionName = lambda x: x + \\\" doing stuff\\\"\\n```\\n\\nThis is poor practice when a standard function definition would have been sufficient. The Python community has a bit of a split over the use of anonymous functions (lambdas) vs. [list comprehensions](https://www.digitalocean.com/community/tutorials/understanding-list-comprehensions-in-python-3). The [benevolent dictator](https://en.wikipedia.org/wiki/Benevolent_dictator_for_life) of the Python community [Guido van Rossum](https://en.wikipedia.org/wiki/Guido_van_Rossum) has [argued for lambda's complete removal from python 3](https://www.artima.com/weblogs/viewpost.jsp?thread=98196).\\n\\n\\n## In Conclusion\\nAnonymous functions can make your code base harder to read. It can also make debugging harder. However, they can save you time from having to define yet another function in your code base. When should you use anonymous functions? According to [Hadley](http://hadley.nz/) *\\\"when it’s not worth the effort to give it a name\\\"*. A good example is when you won't use the function anywhere else in your code. Or when you want to `apply()` a couple of pre-defined functions in one call e.g. `lapply(df, function(x) secondFunction(firstFunction(x)))`.\\n\\n## Resources\\n* [Anonymous functions in Python](http://www.curiousefficiency.org/posts/bloggercom1999blog-9320223post-110439602874040740.html)\\n\\n* [Hadley on Anonymous Functions](http://adv-r.had.co.nz/Functional-programming.html#anonymous-functions)\\n\\n* [Lambda Functions in Python: What Are They Good For?](https://dbader.org/blog/python-lambda-functions)\"}]],\"markups\":[],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><blockquote>\n<p>What's in a name? That which we call a rose by any other name would smell as sweet.</p>\n</blockquote>\n<h2 id=\"normalfunctions\">Normal functions</h2>\n<p>Before moving to anonymous functions, let's start with what normal functions look like.</p>\n<h4 id=\"inr\">In R</h4>\n<pre><code class=\"language-r\"># Define a function\nfunctionName &lt;- function(variables) {\n    # Function definition... do some stuff\n    print(paste(variables, &quot;doing stuff&quot;))\n}\n\n# Call the function\n&gt; functionName(&quot;boring strings&quot;)\n[1] &quot;boring strings doing stuff&quot;\n</code></pre>\n<h4 id=\"inpython\">In Python</h4>\n<pre><code class=\"language-python\"># Define function\ndef functionName(variables):\n    # Do some stuff\n    print variables, &quot;doing stuff&quot;\n\n# Call the function\n&gt;&gt;&gt; functionName(&quot;boring strings&quot;)\nboring strings doing stuff\n</code></pre>\n<h2 id=\"anonymousfunctions\">Anonymous Functions</h2>\n<p>Have no identity, no name, but still do stuff! They will not live in the global environment. Like a person without a name, you would not be able to look the person up in the address book.</p>\n<p>The anonymous function can be called like a normal function <code>functionName()</code>, except the functionName is switched for logic contained within parentheses <code>(fn logic goes here)()</code>.</p>\n<h4 id=\"inr\">In R</h4>\n<pre><code class=\"language-r\"># Doing the same stuff anonymously\n&gt; (function(variables) print(paste(variables, &quot;doing stuff&quot;)) )(&quot;code&quot;)\n[1] &quot;code doing stuff&quot;\n</code></pre>\n<h4 id=\"inpython\">In Python</h4>\n<p>Python introduces the <code>lambda</code> keyword for anonymous functions, in contrast to R which sticks with the <code>function</code> keyword.</p>\n<pre><code class=\"language-python\"># Doing the same stuff anonymously\n&gt;&gt;&gt; (lambda variable: variable + &quot; doing stuff&quot;)(&quot;code&quot;)\n'code doing stuff'\n</code></pre>\n<h3 id=\"rconvention\">R Convention</h3>\n<p>The most common convention is to use anonymous functions when using the <code>*apply</code> family of functions. For example, you might want to do an operation across a set of columns in a dataset.</p>\n<pre><code class=\"language-r\"># Create a dataset\ndf &lt;- data.frame(\n  col1 = c(&quot;element1&quot;, &quot;element2&quot;),\n  col2 = c(&quot;element1&quot;, &quot;element2&quot;), \n  stringsAsFactors = FALSE\n)\n\n# lapply an anonymous function to the columns of the dataset\n&gt; lapply(df, function(x) paste(x, &quot;doing stuff&quot;))\n$col1\n[1] &quot;element1 doing stuff&quot; &quot;element2 doing stuff&quot;\n\n$col2\n[1] &quot;element1 doing stuff&quot; &quot;element2 doing stuff&quot;\n</code></pre>\n<h3 id=\"pythonconvention\">Python Convention</h3>\n<p>Doing the exact operation as above in python:</p>\n<pre><code class=\"language-python\">import pandas as pd\n\n# Create DataFrame\ndf = pd.DataFrame(\n    [[&quot;element1&quot;, &quot;element1&quot;], [&quot;element2&quot;, &quot;element2&quot;]],\n    columns=['col1', 'col2']\n)\n\n&gt;&gt;&gt; df.apply(lambda x: x + &quot; doing stuff&quot;, axis=0)\n                   col1                  col2\n0  element1 doing stuff  element1 doing stuff\n1  element2 doing stuff  element2 doing stuff\n</code></pre>\n<p>You will generally see lambdas used with higher order functions like <code>map()</code>, <code>reduce()</code> and <code>filter()</code>. E.g.</p>\n<pre><code class=\"language-python\"># Map a anonymous function against all elements of df['col1']\n&gt;&gt;&gt; map(lambda x: x + &quot; doing stuff&quot;, df['col1'])\n['element1 doing stuff', 'element2 doing stuff']\n</code></pre>\n<p>You may also see a lambda used to define a function. e.g.</p>\n<pre><code class=\"language-python\">functionName = lambda x: x + &quot; doing stuff&quot;\n</code></pre>\n<p>This is poor practice when a standard function definition would have been sufficient. The Python community has a bit of a split over the use of anonymous functions (lambdas) vs. <a href=\"https://www.digitalocean.com/community/tutorials/understanding-list-comprehensions-in-python-3\">list comprehensions</a>. The <a href=\"https://en.wikipedia.org/wiki/Benevolent_dictator_for_life\">benevolent dictator</a> of the Python community <a href=\"https://en.wikipedia.org/wiki/Guido_van_Rossum\">Guido van Rossum</a> has <a href=\"https://www.artima.com/weblogs/viewpost.jsp?thread=98196\">argued for lambda's complete removal from python 3</a>.</p>\n<h2 id=\"inconclusion\">In Conclusion</h2>\n<p>Anonymous functions can make your code base harder to read. It can also make debugging harder. However, they can save you time from having to define yet another function in your code base. When should you use anonymous functions? According to <a href=\"http://hadley.nz/\">Hadley</a> <em>&quot;when it’s not worth the effort to give it a name&quot;</em>. A good example is when you won't use the function anywhere else in your code. Or when you want to <code>apply()</code> a couple of pre-defined functions in one call e.g. <code>lapply(df, function(x) secondFunction(firstFunction(x)))</code>.</p>\n<h2 id=\"resources\">Resources</h2>\n<ul>\n<li>\n<p><a href=\"http://www.curiousefficiency.org/posts/bloggercom1999blog-9320223post-110439602874040740.html\">Anonymous functions in Python</a></p>\n</li>\n<li>\n<p><a href=\"http://adv-r.had.co.nz/Functional-programming.html#anonymous-functions\">Hadley on Anonymous Functions</a></p>\n</li>\n<li>\n<p><a href=\"https://dbader.org/blog/python-lambda-functions\">Lambda Functions in Python: What Are They Good For?</a></p>\n</li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"25","plaintext":"> What's in a name? That which we call a rose by any other name would smell as\nsweet.\n\n\nNormal functions\nBefore moving to anonymous functions, let's start with what normal functions\nlook like.\n\nIn R\n# Define a function\nfunctionName <- function(variables) {\n    # Function definition... do some stuff\n    print(paste(variables, \"doing stuff\"))\n}\n\n# Call the function\n> functionName(\"boring strings\")\n[1] \"boring strings doing stuff\"\n\n\nIn Python\n# Define function\ndef functionName(variables):\n    # Do some stuff\n    print variables, \"doing stuff\"\n\n# Call the function\n>>> functionName(\"boring strings\")\nboring strings doing stuff\n\n\nAnonymous Functions\nHave no identity, no name, but still do stuff! They will not live in the global\nenvironment. Like a person without a name, you would not be able to look the\nperson up in the address book.\n\nThe anonymous function can be called like a normal function functionName(),\nexcept the functionName is switched for logic contained within parentheses (fn\nlogic goes here)().\n\nIn R\n# Doing the same stuff anonymously\n> (function(variables) print(paste(variables, \"doing stuff\")) )(\"code\")\n[1] \"code doing stuff\"\n\n\nIn Python\nPython introduces the lambda keyword for anonymous functions, in contrast to R\nwhich sticks with the function keyword.\n\n# Doing the same stuff anonymously\n>>> (lambda variable: variable + \" doing stuff\")(\"code\")\n'code doing stuff'\n\n\nR Convention\nThe most common convention is to use anonymous functions when using the *apply \nfamily of functions. For example, you might want to do an operation across a set\nof columns in a dataset.\n\n# Create a dataset\ndf <- data.frame(\n  col1 = c(\"element1\", \"element2\"),\n  col2 = c(\"element1\", \"element2\"), \n  stringsAsFactors = FALSE\n)\n\n# lapply an anonymous function to the columns of the dataset\n> lapply(df, function(x) paste(x, \"doing stuff\"))\n$col1\n[1] \"element1 doing stuff\" \"element2 doing stuff\"\n\n$col2\n[1] \"element1 doing stuff\" \"element2 doing stuff\"\n\n\nPython Convention\nDoing the exact operation as above in python:\n\nimport pandas as pd\n\n# Create DataFrame\ndf = pd.DataFrame(\n    [[\"element1\", \"element1\"], [\"element2\", \"element2\"]],\n    columns=['col1', 'col2']\n)\n\n>>> df.apply(lambda x: x + \" doing stuff\", axis=0)\n                   col1                  col2\n0  element1 doing stuff  element1 doing stuff\n1  element2 doing stuff  element2 doing stuff\n\n\nYou will generally see lambdas used with higher order functions like map(), \nreduce() and filter(). E.g.\n\n# Map a anonymous function against all elements of df['col1']\n>>> map(lambda x: x + \" doing stuff\", df['col1'])\n['element1 doing stuff', 'element2 doing stuff']\n\n\nYou may also see a lambda used to define a function. e.g.\n\nfunctionName = lambda x: x + \" doing stuff\"\n\n\nThis is poor practice when a standard function definition would have been\nsufficient. The Python community has a bit of a split over the use of anonymous\nfunctions (lambdas) vs. list comprehensions\n[https://www.digitalocean.com/community/tutorials/understanding-list-comprehensions-in-python-3]\n. The benevolent dictator\n[https://en.wikipedia.org/wiki/Benevolent_dictator_for_life] of the Python\ncommunity Guido van Rossum [https://en.wikipedia.org/wiki/Guido_van_Rossum] has \nargued for lambda's complete removal from python 3\n[https://www.artima.com/weblogs/viewpost.jsp?thread=98196].\n\nIn Conclusion\nAnonymous functions can make your code base harder to read. It can also make\ndebugging harder. However, they can save you time from having to define yet\nanother function in your code base. When should you use anonymous functions?\nAccording to Hadley [http://hadley.nz/] \"when it’s not worth the effort to give\nit a name\". A good example is when you won't use the function anywhere else in\nyour code. Or when you want to apply() a couple of pre-defined functions in one\ncall e.g. lapply(df, function(x) secondFunction(firstFunction(x))).\n\nResources\n * Anonymous functions in Python\n   [http://www.curiousefficiency.org/posts/bloggercom1999blog-9320223post-110439602874040740.html]\n   \n   \n * Hadley on Anonymous Functions\n   [http://adv-r.had.co.nz/Functional-programming.html#anonymous-functions]\n   \n   \n * Lambda Functions in Python: What Are They Good For?\n   [https://dbader.org/blog/python-lambda-functions]","feature_image":null,"featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-08-05 13:44:50","created_by":"1","updated_at":"2020-01-10 08:27:06","updated_by":null,"published_at":"2017-09-13 21:30:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcf2","uuid":"966c6715-3057-44c5-b1ff-bba007bbdbfa","title":"Make it Fast - Speeding up R Code","slug":"make-it-fast-speeding-up-r-code-2","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\">... a lot of R code is slow simply because it’s poorly written. Few R users have any formal training in programming or software development.\\n\\nIn a previous post I covered [Microbenchmarking](/benchmarking-the-performance-of-r-code/) this provides the tool to measure how much we have sped up our code.\\n\\n>the trade-offs that are key to language design: ... speed, flexibility, and ease of implementation.\\n\\n> dynamism is that you need minimal upfront planning. You can change your mind at any time, iterating your way to a solution without having to start afresh. The disadvantage of dynamism is that it’s difficult to predict exactly what will happen with a given function call.\\n\\n>..[R] contains nearly 800,000 lines of code (about 45% C, 19% R, and 17% Fortran). Changes to base R can only be made by members of the R Core Team (or R-core for short). \\n\\n## Parallel\\nhttps://stackoverflow.com/questions/38318139/run-a-for-loop-in-parallel-in-r\\n\\nhttp://bgc.yale.edu/sites/default/files/ParallelR.html#Parallel_foreach___loop\\n\\n\\n\\n## Resources\\n- [Hadley Wickham's book on advanced R](http://adv-r.had.co.nz/Performance.html)\\n\\n- [Inferno R](http://www.burns-stat.com/pages/Tutor/R_inferno.pdf)\\n\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><blockquote>\n<p>... a lot of R code is slow simply because it’s poorly written. Few R users have any formal training in programming or software development.</p>\n</blockquote>\n<p>In a previous post I covered <a href=\"/benchmarking-the-performance-of-r-code/\">Microbenchmarking</a> this provides the tool to measure how much we have sped up our code.</p>\n<blockquote>\n<p>the trade-offs that are key to language design: ... speed, flexibility, and ease of implementation.</p>\n</blockquote>\n<blockquote>\n<p>dynamism is that you need minimal upfront planning. You can change your mind at any time, iterating your way to a solution without having to start afresh. The disadvantage of dynamism is that it’s difficult to predict exactly what will happen with a given function call.</p>\n</blockquote>\n<blockquote>\n<p>..[R] contains nearly 800,000 lines of code (about 45% C, 19% R, and 17% Fortran). Changes to base R can only be made by members of the R Core Team (or R-core for short).</p>\n</blockquote>\n<h2 id=\"parallel\">Parallel</h2>\n<p><a href=\"https://stackoverflow.com/questions/38318139/run-a-for-loop-in-parallel-in-r\">https://stackoverflow.com/questions/38318139/run-a-for-loop-in-parallel-in-r</a></p>\n<p><a href=\"http://bgc.yale.edu/sites/default/files/ParallelR.html#Parallel_foreach___loop\">http://bgc.yale.edu/sites/default/files/ParallelR.html#Parallel_foreach___loop</a></p>\n<h2 id=\"resources\">Resources</h2>\n<ul>\n<li>\n<p><a href=\"http://adv-r.had.co.nz/Performance.html\">Hadley Wickham's book on advanced R</a></p>\n</li>\n<li>\n<p><a href=\"http://www.burns-stat.com/pages/Tutor/R_inferno.pdf\">Inferno R</a></p>\n</li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"26","plaintext":"> ... a lot of R code is slow simply because it’s poorly written. Few R users have\nany formal training in programming or software development.\n\n\nIn a previous post I covered Microbenchmarking\n[/benchmarking-the-performance-of-r-code/] this provides the tool to measure how\nmuch we have sped up our code.\n\n> the trade-offs that are key to language design: ... speed, flexibility, and ease\nof implementation.\n\n\n> dynamism is that you need minimal upfront planning. You can change your mind at\nany time, iterating your way to a solution without having to start afresh. The\ndisadvantage of dynamism is that it’s difficult to predict exactly what will\nhappen with a given function call.\n\n\n> ..[R] contains nearly 800,000 lines of code (about 45% C, 19% R, and 17%\nFortran). Changes to base R can only be made by members of the R Core Team (or\nR-core for short).\n\n\nParallel\nhttps://stackoverflow.com/questions/38318139/run-a-for-loop-in-parallel-in-r\n\nhttp://bgc.yale.edu/sites/default/files/ParallelR.html#Parallel_foreach___loop\n\nResources\n * Hadley Wickham's book on advanced R [http://adv-r.had.co.nz/Performance.html]\n   \n   \n * Inferno R [http://www.burns-stat.com/pages/Tutor/R_inferno.pdf]","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-08-06 01:17:06","created_by":"1","updated_at":"2017-08-09 15:52:50","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcf3","uuid":"c7ed72e7-eda1-4881-9640-100d9e1e2e87","title":"Shiny (R) Web App Performance","slug":"shiny-r-web-app-performance","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"## Background\\n>a single R process can usually serve 5 to 30 requests/second. \\n\\nhttps://shiny.rstudio.com/articles/scaling-and-tuning.html\\n\\n[good diagrams/overview of what is shared/excuted per process/connection](https://support.rstudio.com/hc/en-us/articles/220546267-Scaling-and-Performance-Tuning-Applications-in-Shiny-Server-Pro)\\n\\n## Load Testing\\n> Tests must be recorded against a local copy of the deployed application.\\n\\n```\\nlibrary(shinyloadtest)  \\nloadTest(testFile = 'myloadTest.R',\\n         url = 'https://beta.rstudioconnect.com/content/2551',\\n         numConcurrent = 8,\\n         numTotal = 16,\\n         loadTimeout = 5,\\n         stagger = 4, \\n         phantomTimeout = 20)\\n```\\n\\nhttps://github.com/rstudio/shinyloadtest\\n\\n[RMD report of load test](https://github.com/rstudio/shinyloadtest/blob/master/inst/loadTestReport/load_test_template.Rmd)\\n[Output report of load test](https://beta.rstudioconnect.com/content/2612/addinTest.html)\\n\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><h2 id=\"background\">Background</h2>\n<blockquote>\n<p>a single R process can usually serve 5 to 30 requests/second.</p>\n</blockquote>\n<p><a href=\"https://shiny.rstudio.com/articles/scaling-and-tuning.html\">https://shiny.rstudio.com/articles/scaling-and-tuning.html</a></p>\n<p><a href=\"https://support.rstudio.com/hc/en-us/articles/220546267-Scaling-and-Performance-Tuning-Applications-in-Shiny-Server-Pro\">good diagrams/overview of what is shared/excuted per process/connection</a></p>\n<h2 id=\"loadtesting\">Load Testing</h2>\n<blockquote>\n<p>Tests must be recorded against a local copy of the deployed application.</p>\n</blockquote>\n<pre><code>library(shinyloadtest)  \nloadTest(testFile = 'myloadTest.R',\n         url = 'https://beta.rstudioconnect.com/content/2551',\n         numConcurrent = 8,\n         numTotal = 16,\n         loadTimeout = 5,\n         stagger = 4, \n         phantomTimeout = 20)\n</code></pre>\n<p><a href=\"https://github.com/rstudio/shinyloadtest\">https://github.com/rstudio/shinyloadtest</a></p>\n<p><a href=\"https://github.com/rstudio/shinyloadtest/blob/master/inst/loadTestReport/load_test_template.Rmd\">RMD report of load test</a><br>\n<a href=\"https://beta.rstudioconnect.com/content/2612/addinTest.html\">Output report of load test</a></p>\n<!--kg-card-end: markdown-->","comment_id":"27","plaintext":"Background\n> a single R process can usually serve 5 to 30 requests/second.\n\n\nhttps://shiny.rstudio.com/articles/scaling-and-tuning.html\n\ngood diagrams/overview of what is shared/excuted per process/connection\n[https://support.rstudio.com/hc/en-us/articles/220546267-Scaling-and-Performance-Tuning-Applications-in-Shiny-Server-Pro]\n\nLoad Testing\n> Tests must be recorded against a local copy of the deployed application.\n\n\nlibrary(shinyloadtest)  \nloadTest(testFile = 'myloadTest.R',\n         url = 'https://beta.rstudioconnect.com/content/2551',\n         numConcurrent = 8,\n         numTotal = 16,\n         loadTimeout = 5,\n         stagger = 4, \n         phantomTimeout = 20)\n\n\nhttps://github.com/rstudio/shinyloadtest\n\nRMD report of load test\n[https://github.com/rstudio/shinyloadtest/blob/master/inst/loadTestReport/load_test_template.Rmd]\nOutput report of load test\n[https://beta.rstudioconnect.com/content/2612/addinTest.html]","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-08-06 02:17:29","created_by":"1","updated_at":"2017-08-06 11:22:48","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcf4","uuid":"a26da590-479e-499f-b718-ddd598861d80","title":"Tests for Shiny Apps","slug":"tests-for-shiny-apps","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"[Selenium](https://cran.r-project.org/web/packages/RSelenium/vignettes/shinytesting.html)\\n\\n[ShinyTest](https://rstudio.github.io/shinytest/articles/shinytest.html)\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><a href=\"https://cran.r-project.org/web/packages/RSelenium/vignettes/shinytesting.html\">Selenium</a></p>\n<p><a href=\"https://rstudio.github.io/shinytest/articles/shinytest.html\">ShinyTest</a></p>\n<!--kg-card-end: markdown-->","comment_id":"28","plaintext":"Selenium\n[https://cran.r-project.org/web/packages/RSelenium/vignettes/shinytesting.html]\n\nShinyTest [https://rstudio.github.io/shinytest/articles/shinytest.html]","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-08-06 02:38:41","created_by":"1","updated_at":"2017-08-06 02:46:21","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcf5","uuid":"c7995e38-f645-4878-a568-3f7324e37a45","title":"Shiny (R) Web App Performance - Profiling","slug":"shiny-r-performance-profiling","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Introduced at the 2016 R conference, the `profvis` package offers a visual way of inspecting the call stack and highlights the most memory and computationally intensive parts of your code.\\n\\n## Run Profvis\\n\\n```r\\n# Load library\\nlibrary(profvis)\\n\\n# Run profiler on shiny app with optional arg to save output\\nprofvis({ runApp('Projects/path_of_app') }\\n        , prof_output = '/path_to_save_output')\\n```\\n\\nAt this point your app will launch, to get the shiny app code to execute, interact with the parts of the application that you are interested in. Once you have completed the interactions, close the page and press the 'stop' button at the top of the console in RStudio. Profvis will recognise you have stopped running the shiny app, display the profvis interface and *optionally* save the file. The file name is randomly generated and should look something like this `file108f93bff877b.Rprof`.\\n\\n>Each block in the flame graph represents a call to a function, or possibly multiple calls to the same function. The width of the block is proportional to the amount of time spent in that function. When a function calls another function, another block is added on top of it in the flame graph.\\n\\nSource: [Profvis Overview](https://rstudio.github.io/profvis/) \\n\\nTo reload the saved profvis profile:\\n\\n```r\\n# Load saved profvis\\nprofvis(prof_input = '/path_to_save_output/file108f93bff877b.Rprof')\\n```\\n\\n## Reload a Saved Profvis\\nYou can also save as a webpage using the following code:\\n\\n```r\\n# Assign to variable\\np <- profvis(prof_input = '/path_to_save_output/file108f93bff877b.Rprof')\\n\\n# Save as a webpage\\nhtmlwidgets::saveWidget(p, \\\"/path_to_save_output/profile.html\\\")\\n```\\n\\nIf you've found this content helpful why not...\\n\\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\\n  <link href=\\\"https://fonts.googleapis.com/css?family=Cookie\\\" rel=\\\"stylesheet\\\"><a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\\n\\n## Resources\\n* [Video of Winstan Chang presenting ProfViz](https://www.rstudio.com/resources/videos/profiling-and-performance/)\\n\\n* [Profiling code integrated into RStudio IDE, great for non-shiny code](https://blog.rstudio.com/2016/05/23/profiling-with-rstudio-and-profvis/)\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Introduced at the 2016 R conference, the <code>profvis</code> package offers a visual way of inspecting the call stack and highlights the most memory and computationally intensive parts of your code.</p>\n<h2 id=\"runprofvis\">Run Profvis</h2>\n<pre><code class=\"language-r\"># Load library\nlibrary(profvis)\n\n# Run profiler on shiny app with optional arg to save output\nprofvis({ runApp('Projects/path_of_app') }\n        , prof_output = '/path_to_save_output')\n</code></pre>\n<p>At this point your app will launch, to get the shiny app code to execute, interact with the parts of the application that you are interested in. Once you have completed the interactions, close the page and press the 'stop' button at the top of the console in RStudio. Profvis will recognise you have stopped running the shiny app, display the profvis interface and <em>optionally</em> save the file. The file name is randomly generated and should look something like this <code>file108f93bff877b.Rprof</code>.</p>\n<blockquote>\n<p>Each block in the flame graph represents a call to a function, or possibly multiple calls to the same function. The width of the block is proportional to the amount of time spent in that function. When a function calls another function, another block is added on top of it in the flame graph.</p>\n</blockquote>\n<p>Source: <a href=\"https://rstudio.github.io/profvis/\">Profvis Overview</a></p>\n<p>To reload the saved profvis profile:</p>\n<pre><code class=\"language-r\"># Load saved profvis\nprofvis(prof_input = '/path_to_save_output/file108f93bff877b.Rprof')\n</code></pre>\n<h2 id=\"reloadasavedprofvis\">Reload a Saved Profvis</h2>\n<p>You can also save as a webpage using the following code:</p>\n<pre><code class=\"language-r\"># Assign to variable\np &lt;- profvis(prof_input = '/path_to_save_output/file108f93bff877b.Rprof')\n\n# Save as a webpage\nhtmlwidgets::saveWidget(p, &quot;/path_to_save_output/profile.html&quot;)\n</code></pre>\n<p>If you've found this content helpful why not...</p>\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\n  <link href=\"https://fonts.googleapis.com/css?family=Cookie\" rel=\"stylesheet\"><a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a>\n<h2 id=\"resources\">Resources</h2>\n<ul>\n<li>\n<p><a href=\"https://www.rstudio.com/resources/videos/profiling-and-performance/\">Video of Winstan Chang presenting ProfViz</a></p>\n</li>\n<li>\n<p><a href=\"https://blog.rstudio.com/2016/05/23/profiling-with-rstudio-and-profvis/\">Profiling code integrated into RStudio IDE, great for non-shiny code</a></p>\n</li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"29","plaintext":"Introduced at the 2016 R conference, the profvis package offers a visual way of\ninspecting the call stack and highlights the most memory and computationally\nintensive parts of your code.\n\nRun Profvis\n# Load library\nlibrary(profvis)\n\n# Run profiler on shiny app with optional arg to save output\nprofvis({ runApp('Projects/path_of_app') }\n        , prof_output = '/path_to_save_output')\n\n\nAt this point your app will launch, to get the shiny app code to execute,\ninteract with the parts of the application that you are interested in. Once you\nhave completed the interactions, close the page and press the 'stop' button at\nthe top of the console in RStudio. Profvis will recognise you have stopped\nrunning the shiny app, display the profvis interface and optionally save the\nfile. The file name is randomly generated and should look something like this \nfile108f93bff877b.Rprof.\n\n> Each block in the flame graph represents a call to a function, or possibly\nmultiple calls to the same function. The width of the block is proportional to\nthe amount of time spent in that function. When a function calls another\nfunction, another block is added on top of it in the flame graph.\n\n\nSource: Profvis Overview [https://rstudio.github.io/profvis/]\n\nTo reload the saved profvis profile:\n\n# Load saved profvis\nprofvis(prof_input = '/path_to_save_output/file108f93bff877b.Rprof')\n\n\nReload a Saved Profvis\nYou can also save as a webpage using the following code:\n\n# Assign to variable\np <- profvis(prof_input = '/path_to_save_output/file108f93bff877b.Rprof')\n\n# Save as a webpage\nhtmlwidgets::saveWidget(p, \"/path_to_save_output/profile.html\")\n\n\nIf you've found this content helpful why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]Resources\n * Video of Winstan Chang presenting ProfViz\n   [https://www.rstudio.com/resources/videos/profiling-and-performance/]\n   \n   \n * Profiling code integrated into RStudio IDE, great for non-shiny code\n   [https://blog.rstudio.com/2016/05/23/profiling-with-rstudio-and-profvis/]","feature_image":null,"featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-08-06 11:23:06","created_by":"1","updated_at":"2019-08-04 13:13:13","updated_by":null,"published_at":"2017-08-06 12:27:18","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcf6","uuid":"9af75c8e-044d-4988-9062-ea30427ed294","title":"Shiny (R) Web App Tests","slug":"shiny-r-web-app-tests","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"https://rstudio.github.io/shinytest/articles/in-depth.html\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><a href=\"https://rstudio.github.io/shinytest/articles/in-depth.html\">https://rstudio.github.io/shinytest/articles/in-depth.html</a></p>\n<!--kg-card-end: markdown-->","comment_id":"30","plaintext":"https://rstudio.github.io/shinytest/articles/in-depth.html","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-08-08 13:16:57","created_by":"1","updated_at":"2017-08-08 13:17:50","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcf7","uuid":"6c4c170d-1343-4293-a75d-5830959998e2","title":"Data Science Interview Questions 2017","slug":"data-science-interview-questions-2017","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"## What are the main benefits of a logistic regression compared to a decision tree?\\ncoefficients positive and negative\\np-values\\n\\n## What would you have done differently?\\n\\n## What is the dot product rule?\\n\\n## what's CLT?\\n\\n## what's a p-value?\\n\\n## what's the difference between systematic vs clustering sampling? And when would you use one over the other?\\n\\n## how would you find the optimal K in a clustering algorithm?\\n\\n## how would work out how many ping-pong balls fit into a 747?\\n\\n##Vaguely data sci...\\n\\n- What's not on your CV? Failure?\\n\\n- What project are you most proud of?\\n\\n- What are you're weaknesses coming into this role?\\n\\nGreat point, if you are changing industrys then industry specific knowledge.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><h2 id=\"whatarethemainbenefitsofalogisticregressioncomparedtoadecisiontree\">What are the main benefits of a logistic regression compared to a decision tree?</h2>\n<p>coefficients positive and negative<br>\np-values</p>\n<h2 id=\"whatwouldyouhavedonedifferently\">What would you have done differently?</h2>\n<h2 id=\"whatisthedotproductrule\">What is the dot product rule?</h2>\n<h2 id=\"whatsclt\">what's CLT?</h2>\n<h2 id=\"whatsapvalue\">what's a p-value?</h2>\n<h2 id=\"whatsthedifferencebetweensystematicvsclusteringsamplingandwhenwouldyouuseoneovertheother\">what's the difference between systematic vs clustering sampling? And when would you use one over the other?</h2>\n<h2 id=\"howwouldyoufindtheoptimalkinaclusteringalgorithm\">how would you find the optimal K in a clustering algorithm?</h2>\n<h2 id=\"howwouldworkouthowmanypingpongballsfitintoa747\">how would work out how many ping-pong balls fit into a 747?</h2>\n<h2 id=\"vaguelydatasci\">Vaguely data sci...</h2>\n<ul>\n<li>\n<p>What's not on your CV? Failure?</p>\n</li>\n<li>\n<p>What project are you most proud of?</p>\n</li>\n<li>\n<p>What are you're weaknesses coming into this role?</p>\n</li>\n</ul>\n<p>Great point, if you are changing industrys then industry specific knowledge.</p>\n<!--kg-card-end: markdown-->","comment_id":"31","plaintext":"What are the main benefits of a logistic regression compared to a decision tree?\ncoefficients positive and negative\np-values\n\nWhat would you have done differently?\nWhat is the dot product rule?\nwhat's CLT?\nwhat's a p-value?\nwhat's the difference between systematic vs clustering sampling? And when would\nyou use one over the other?\nhow would you find the optimal K in a clustering algorithm?\nhow would work out how many ping-pong balls fit into a 747?\nVaguely data sci...\n * What's not on your CV? Failure?\n   \n   \n * What project are you most proud of?\n   \n   \n * What are you're weaknesses coming into this role?\n   \n   \n\nGreat point, if you are changing industrys then industry specific knowledge.","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-08-20 00:26:18","created_by":"1","updated_at":"2017-08-22 04:24:55","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcf8","uuid":"e9e2c100-5fb0-49fc-b746-0e39cd6eb3c6","title":"Work on Important Things","slug":"work-on-important-things","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"[Great advice here - summarise](http://www.mccurley.org/advice/hamming_advice.html)\\n\\nThe internet's own boy\\n\\n80000hrs\\n\\nthe life you can save\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><a href=\"http://www.mccurley.org/advice/hamming_advice.html\">Great advice here - summarise</a></p>\n<p>The internet's own boy</p>\n<p>80000hrs</p>\n<p>the life you can save</p>\n<!--kg-card-end: markdown-->","comment_id":"32","plaintext":"Great advice here - summarise\n[http://www.mccurley.org/advice/hamming_advice.html]\n\nThe internet's own boy\n\n80000hrs\n\nthe life you can save","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-08-22 04:25:23","created_by":"1","updated_at":"2017-08-22 04:26:09","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcf9","uuid":"c9454eb5-a8f5-4c91-8569-69cae08750f9","title":"Supporting a Union 2017","slug":"supporting-a-union-2017","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\">the most important factor is that employers have gained bargaining power over wages while workers have lost it. \\n- Larry summers?\\n\\n\\nhttps://www.bloomberg.com/news/articles/2017-09-21/why-wages-aren-t-growing\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><blockquote>\n<p>the most important factor is that employers have gained bargaining power over wages while workers have lost it.</p>\n</blockquote>\n<ul>\n<li>Larry summers?</li>\n</ul>\n<p><a href=\"https://www.bloomberg.com/news/articles/2017-09-21/why-wages-aren-t-growing\">https://www.bloomberg.com/news/articles/2017-09-21/why-wages-aren-t-growing</a></p>\n<!--kg-card-end: markdown-->","comment_id":"33","plaintext":"> the most important factor is that employers have gained bargaining power over\nwages while workers have lost it.\n\n\n * Larry summers?\n\nhttps://www.bloomberg.com/news/articles/2017-09-21/why-wages-aren-t-growing","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-09-04 03:03:01","created_by":"1","updated_at":"2017-09-22 13:54:19","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcfa","uuid":"3add9280-6370-4971-a645-e2133c165356","title":"Dual Booting Ubuntu and Windows 10 on a Dell XPS 13","slug":"dual-booting-ubuntu-and-windows-10-on-a-dell-xps","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"**My Setup**: [Dell XPS 13-9360](http://www.dell.com/en-uk/shop/laptop-and-2-in-1-pcs/xps-13/spd/xps-13-9360-laptop) + Windows 10\\n\\n**Aim** - Get Ubuntu on the machine.\\n\\nEssentially, follow [these instructions](https://askubuntu.com/a/868294/424657) with two key modifications.\\n\\n1. You should be able to get away with switching from [RAID](https://en.wikipedia.org/wiki/RAID) to [AHCI](https://en.wikipedia.org/wiki/Advanced_Host_Controller_Interface) without re-imaging windows using [these instuctions](http://triplescomputers.com/blog/uncategorized/solution-switch-windows-10-from-raidide-to-ahci-operation/).\\n2. Disable [Bitlocker](https://en.wikipedia.org/wiki/RAID) - this is crucial! The first time I ran this... I didn't complete this step which led me to screwing up my Windows 10 installation.\\n\\nI neglected Step 2. If the boot priority is Ubuntu first (i.e. Grub) then Ubuntu loads fine. However, windows doesn't load as the Bitlocker detects a change to the BIOS and will require a code to unlock it which, if you're like me, you probably don't have. \\n\\nI did try running through the [boot-repair tool](https://help.ubuntu.com/community/Boot-Repair) from Ubuntu but kept facing the `GPT detected... ` error.\\n\\nIf you've found this content helpful why not...\\n\\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\\n  <link href=\\\"https://fonts.googleapis.com/css?family=Cookie\\\" rel=\\\"stylesheet\\\"><a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\\n\\n**Solution:** I use the BIOS boot priority to change the operating system. Now both Win10 and Ubuntu are running on my machine.\\n\\n\\n## Resources\\n* [Main instuctions](https://askubuntu.com/a/868294/424657)\\n* [Switching from RAID to AHCI without re-imaging Win10](http://triplescomputers.com/blog/uncategorized/solution-switch-windows-10-from-raidide-to-ahci-operation/)\\n* [Turn off Bitlocker](http://triplescomputers.com/blog/uncategorized/solution-switch-windows-10-from-raidide-to-ahci-operation/)\\n* [Reddit thread](https://www.reddit.com/r/Dell/comments/5xw27t/dell_xps_15_9560_dualboot_windows_10_ubuntu_lts/)\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><strong>My Setup</strong>: <a href=\"http://www.dell.com/en-uk/shop/laptop-and-2-in-1-pcs/xps-13/spd/xps-13-9360-laptop\">Dell XPS 13-9360</a> + Windows 10</p>\n<p><strong>Aim</strong> - Get Ubuntu on the machine.</p>\n<p>Essentially, follow <a href=\"https://askubuntu.com/a/868294/424657\">these instructions</a> with two key modifications.</p>\n<ol>\n<li>You should be able to get away with switching from <a href=\"https://en.wikipedia.org/wiki/RAID\">RAID</a> to <a href=\"https://en.wikipedia.org/wiki/Advanced_Host_Controller_Interface\">AHCI</a> without re-imaging windows using <a href=\"http://triplescomputers.com/blog/uncategorized/solution-switch-windows-10-from-raidide-to-ahci-operation/\">these instuctions</a>.</li>\n<li>Disable <a href=\"https://en.wikipedia.org/wiki/RAID\">Bitlocker</a> - this is crucial! The first time I ran this... I didn't complete this step which led me to screwing up my Windows 10 installation.</li>\n</ol>\n<p>I neglected Step 2. If the boot priority is Ubuntu first (i.e. Grub) then Ubuntu loads fine. However, windows doesn't load as the Bitlocker detects a change to the BIOS and will require a code to unlock it which, if you're like me, you probably don't have.</p>\n<p>I did try running through the <a href=\"https://help.ubuntu.com/community/Boot-Repair\">boot-repair tool</a> from Ubuntu but kept facing the <code>GPT detected... </code> error.</p>\n<p>If you've found this content helpful why not...</p>\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\n  <link href=\"https://fonts.googleapis.com/css?family=Cookie\" rel=\"stylesheet\"><a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a>\n<p><strong>Solution:</strong> I use the BIOS boot priority to change the operating system. Now both Win10 and Ubuntu are running on my machine.</p>\n<h2 id=\"resources\">Resources</h2>\n<ul>\n<li><a href=\"https://askubuntu.com/a/868294/424657\">Main instuctions</a></li>\n<li><a href=\"http://triplescomputers.com/blog/uncategorized/solution-switch-windows-10-from-raidide-to-ahci-operation/\">Switching from RAID to AHCI without re-imaging Win10</a></li>\n<li><a href=\"http://triplescomputers.com/blog/uncategorized/solution-switch-windows-10-from-raidide-to-ahci-operation/\">Turn off Bitlocker</a></li>\n<li><a href=\"https://www.reddit.com/r/Dell/comments/5xw27t/dell_xps_15_9560_dualboot_windows_10_ubuntu_lts/\">Reddit thread</a></li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"34","plaintext":"My Setup: Dell XPS 13-9360\n[http://www.dell.com/en-uk/shop/laptop-and-2-in-1-pcs/xps-13/spd/xps-13-9360-laptop] \n+ Windows 10\n\nAim - Get Ubuntu on the machine.\n\nEssentially, follow these instructions [https://askubuntu.com/a/868294/424657] \nwith two key modifications.\n\n 1. You should be able to get away with switching from RAID\n    [https://en.wikipedia.org/wiki/RAID] to AHCI\n    [https://en.wikipedia.org/wiki/Advanced_Host_Controller_Interface] without\n    re-imaging windows using these instuctions\n    [http://triplescomputers.com/blog/uncategorized/solution-switch-windows-10-from-raidide-to-ahci-operation/]\n    .\n 2. Disable Bitlocker [https://en.wikipedia.org/wiki/RAID] - this is crucial!\n    The first time I ran this... I didn't complete this step which led me to\n    screwing up my Windows 10 installation.\n\nI neglected Step 2. If the boot priority is Ubuntu first (i.e. Grub) then Ubuntu\nloads fine. However, windows doesn't load as the Bitlocker detects a change to\nthe BIOS and will require a code to unlock it which, if you're like me, you\nprobably don't have.\n\nI did try running through the boot-repair tool\n[https://help.ubuntu.com/community/Boot-Repair] from Ubuntu but kept facing the \nGPT detected... error.\n\nIf you've found this content helpful why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]Solution: I use the BIOS boot priority to change the operating system. Now both\nWin10 and Ubuntu are running on my machine.\n\nResources\n * Main instuctions [https://askubuntu.com/a/868294/424657]\n * Switching from RAID to AHCI without re-imaging Win10\n   [http://triplescomputers.com/blog/uncategorized/solution-switch-windows-10-from-raidide-to-ahci-operation/]\n * Turn off Bitlocker\n   [http://triplescomputers.com/blog/uncategorized/solution-switch-windows-10-from-raidide-to-ahci-operation/]\n * Reddit thread\n   [https://www.reddit.com/r/Dell/comments/5xw27t/dell_xps_15_9560_dualboot_windows_10_ubuntu_lts/]","feature_image":null,"featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-09-19 11:52:34","created_by":"1","updated_at":"2019-08-04 13:14:04","updated_by":null,"published_at":"2017-09-20 16:13:08","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcfb","uuid":"b40b6eb3-593a-42a6-9b05-39e1fd680154","title":"How to Use Shiny Containers with Shinyproxy","slug":"shiny-containers-with-shinyproxy","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"**Aim**: Setup [shinyproxy](https://www.shinyproxy.io) as a production level way of deploying multiple containerised shiny apps with authentication. Additionally I'll demonstrate how to incorporate containerised python web apps and deploy this behind a reverse proxy using Nginx.\\n\\n**Conceptual Overview**\\n![](/content/images/2017/10/overview.png)\\n\\nThis is a pretty significant piece of software. It renders [Shiny Server Pro](https://www.rstudio.com/products/shiny-server-pro/) redundant. As this solution, provides authentication, scalability, decreases maintenance (<i class=\\\"em em-heart\\\"></i> containers) and enables any web app to be running behind the scenes. So you could have a [shiny](https://shiny.rstudio.com/gallery/) app, a [Django](https://www.djangoproject.com/) app, a [flask](http://flask.pocoo.org/) app, [Vue.js](https://vuejs.org/)... and the end-user will simply see a website, whilst the backend has a whole range of different web applications spinning up on demand.\\n\\nI ran into some hurdles with [these instructions](https://www.shinyproxy.io/getting-started/#prerequisites). So I'm documenting what worked for me. **My setup:** I set this up once on a Dell XPS 13 with a fresh install of Ubuntu 16.04.3 LTS and a Ubuntu 16.04.03 Azure VM.\\n\\n## Pre-requisites / Assumptions\\n\\n### Docker\\nIf you don't have [Docker](https://www.docker.com/) installed yet, I highly recommend running through [Part 1: Orientation and setup](https://docs.docker.com/get-started/) to [Part 3: Services](https://docs.docker.com/get-started/part3/) of [Docker's Getting Started Documentation](https://docs.docker.com/get-started/).\\n\\nIf running the following commands return similar console prints you should be fine to continue on.\\n\\n```bash\\n$ docker -v\\nDocker version 17.06.2-ce, build cec0b72\\n$ docker run hello-world \\nHello from Docker!\\nThis message shows that your installation appears to be working correctly.\\n```\\n\\n##### Docker Problems Encountered for Ubuntu 16 \\nThe most time consuming part of the setup were [the instructions for Ubuntu 16](https://support.openanalytics.eu/t/shinyproxy-on-ubuntu-16-04/38/4) on the [shinyproxy site](https://www.shinyproxy.io) guiding modification of `/lib/systemd/system/docker.service`. Per the documentation on Shinyproxy:\\n\\n![shinyproxy-docker-instructions](/content/images/2017/09/shinyproxy-ubuntu16-docker-config.png)\\n\\nRunning this configuration produced the following error:\\n\\n```bash\\n$ sudo systemctl restart docker\\nJob for docker.service failed because the control process exited with error code. See \\\"systemctl status docker.service\\\" and \\\"journalctl -xe\\\" for details.\\n```\\n\\n##### Solution\\nInstead, edit `/lib/systemd/system/docker.service` and set `ExecStart` to:\\n\\n```\\nExecStart=/usr/bin/dockerd -H fd:// -D -H tcp://127.0.0.1:2375\\n```\\nWhy? Shinyproxy will look to talk to the docker daemon on `tcp://127.0.0.1:2375`. If it can't, then the application won't work.\\n\\nEnsure that you restart docker after editing the config with the following commands:\\n\\n```bash\\n$ sudo systemctl daemon-reload\\n$ sudo systemctl restart docker\\n```\\n\\n### Java8\\nShinyproxy requires a minimum of [Java 8](http://www.oracle.com/technetwork/java/javase/overview/java8-2100321.html). I followed the advice per [shinyproxy.io](https://www.shinyproxy.io/getting-started/) and installed the OpenJDK [Zulu](http://zulu.org/). This Quora answer from the product manager of Zulu gives some context behind this JDK:\\n\\n<span class='quora-content-embed' data-name='What-is-it-like-to-use-Azul-Systems-Zulu-JVM/answer/Matt-Schuetze'>Read <a class='quora-content-link' data-width='559' data-height='250' href='https://www.quora.com/What-is-it-like-to-use-Azul-Systems-Zulu-JVM/answer/Matt-Schuetze' data-type='answer' data-id='8731568' data-key='6ab4b9b9ff8a05fcb861058252be404c' load-full-answer='False' data-embed='hdayukz'><a href='https://www.quora.com/Matt-Schuetze'>Matt Schuetze</a>&#039;s <a href='/What-is-it-like-to-use-Azul-Systems-Zulu-JVM#ans8731568'>answer</a> to <a href='/What-is-it-like-to-use-Azul-Systems-Zulu-JVM' ref='canonical'><span class=\\\"rendered_qtext\\\">What is it like to use Azul Systems&#039; Zulu JVM?</span></a></a> on <a href='https://www.__nousername__.main.quora.com'>Quora</a><script type=\\\"text/javascript\\\" src=\\\"https://www.quora.com/widgets/content\\\"></script></span>  \\n\\nRun the following at the command line:\\n\\n```bash\\n$ java -version\\nopenjdk version \\\"1.8.0_144\\\"\\nOpenJDK Runtime Environment (Zulu 8.23.0.3-linux64) (build 1.8.0_144-b01)\\nOpenJDK 64-Bit Server VM (Zulu 8.23.0.3-linux64) (build 25.144-b01, mixed mode)\\n```\\n\\nIf you have Java 8 then continue. N.B. Zulu 8.xx.x.x is Java 8. Otherwise, run the following commands to install:\\n\\n```bash\\n$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 0x219BD9C9\\n$ apt_source='deb http://repos.azulsystems.com/debian stable main'\\n$ apt_list='/etc/apt/sources.list.d/zulu.list'\\n$ echo \\\"$apt_source\\\" | sudo tee \\\"$apt_list\\\" > /dev/null\\n$ sudo apt-get update\\n$ sudo apt-get install zulu-8\\n```\\n\\n**N.B.** I did experience a maven build fail when running `mvn -U clean install` on a Mac with a Java9 runtime. Uninstalling Java 9 and running `brew cask install java8` fixed the problem.\\n\\n## Download and Compile Shiny Proxy\\nThere are two options for downloading the shinyproxy software.\\n\\n**Option 1** - I cloned the [shinyproxy repo](https://github.com/openanalytics/shinyproxy/). I did not have [apache maven](https://maven.apache.org/) installed so I needed to [run through the installation of maven first](https://maven.apache.org/install.html).\\n\\n```bash\\n# Clone the repo\\n$ git clone https://github.com/openanalytics/shinyproxy.git  \\n# Change directory\\n$ cd shinyproxy/\\n# Compile the program\\n$ mvn -U clean install\\n```\\n\\n**Option 2** - You can also go to the [downloads page](https://www.shinyproxy.io/downloads/) and select the latest file for your particular operating system.\\n\\n## Pull the shiny app containers\\nWe now have shinyproxy ready to run containerised applications. However, we don't have the images that shinyproxy is expecting. We need to pull those first:\\n\\n```bash\\n# Pull the shiny app images\\n$ sudo docker pull openanalytics/shinyproxy-demo\\n# Check they are now in the list of docker images\\n$ docker images | grep shinyproxy\\n```\\n\\n## Run Shiny Proxy\\nRun the following commands:\\n\\n```bash\\n# Ensure you are in the right directory\\n$ cd ~/shinyproxy/target/\\n# Run shinyproxy\\n$ java -jar shinyproxy-1.0.0.jar\\n```\\n\\nAt this point you should now be able to navigate to [http://localhost:8080](http://localhost:8080) and see the sign in page and login with 'tesla' and 'password' as username and password respectively. If you are doing this on a remote server, test the application is up and running by running:\\n\\n```bash\\n# Print the html of the site to the terminal\\ncurl http://localhost:8080\\n```\\n\\n![shinyproxy-login](/content/images/2017/09/shinyproxy-login.png)\\n\\nOnce logged in launch an application.\\n\\n![shinyproxy-select-app](/content/images/2017/09/shinyproxy-select-app.png)\\n\\nAnd bingo we have a containerised and authenticated shiny environment.\\n\\n![](/content/images/2017/09/shinyproxy-shiny-app.png)\\n\\n## Adding Additional Shiny Apps\\n[Shinyproxy docs](https://www.shinyproxy.io/deploying-apps/) provide a ready to go shiny application with a [dockerfile](https://docs.docker.com/engine/reference/builder/) to build the image. Of course, to use shinyproxy for your own custom shiny applications the dockerfile in the [shinyproxy-template repo](https://github.com/openanalytics/shinyproxy-template) acts as a template to follow. I'm going to go through the process of adding shinyproxy's pre-built shiny app.\\n\\n```bash\\n# Clone the shiny app repo\\n$ git clone https://github.com/openanalytics/shinyproxy-template.git\\n# Change into the repo\\n$ cd shinyproxy-template/\\n# Build the image\\n$ docker build -t openanalytics/shinyproxy-template .\\n# Check the image is now available in docker\\n$ docker images | grep \\\"shinyproxy-template\\\\|REPOSITORY\\\"\\nREPOSITORY                          TAG                 IMAGE ID            CREATED             SIZE\\nopenanalytics/shinyproxy-template   latest              16e8c49e2261        25 minutes ago      851MB\\n# Check the shiny app runs normally outside shinyproxy\\n$ docker run -it -p 3838:3838  openanalytics/shinyproxy-template\\n```\\n\\nThe new shiny app should now be running on [http://0.0.0.0:3838/](http://0.0.0.0:3838/).\\n\\n![additional-shiny-app](/content/images/2017/09/additional-shiny-app.png)\\n\\nNow we need to tell shinyproxy to include this additional shiny app by editing creating and editing a `application.yml` file. \\n\\n```bash\\n# Ensure you are in the right directory\\n$ cd ~/shinyproxy/target/\\n# Create application.yml from repo template\\n$ curl https://raw.githubusercontent.com/openanalytics/shinyproxy/master/src/main/resources/application-demo.yml > application.yml\\n```\\n\\nWe now need to add our new shiny app to the `application.yml` file.\\n\\n```yaml\\napps:\\n  - name: euler\\n    display-name: Euler's number\\n    description: Adding another app to shinyproxy\\n    docker-cmd: [\\\"R\\\", \\\"-e shiny::runApp('/root/euler')\\\"]\\n    docker-image: openanalytics/shinyproxy-template\\n    groups: scientists\\n```\\n\\nNow I'm going to run shinyproxy again to confirm the change has been successful.\\n\\n![add-another-shiny-app](/content/images/2017/09/add-another-shiny-app.png)\\n\\nAnd does the application launch\\n\\n![](/content/images/2017/09/euler-app-running.png)\\n\\nSuccess!\\n\\nIf you're finding this content helpful why not...\\n\\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\\n  <link href=\\\"https://fonts.googleapis.com/css?family=Cookie\\\" rel=\\\"stylesheet\\\"><a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\\n\\n## Adding Additional non-Shiny Apps\\nI'm going with a very basic 'hello world' flask app. The main issue here is configuring the ports for shinyproxy to pick up the app. Since shiny by default runs on port 3838, I mapped the containerised flask app to run on port 3838 too. I pushed [a flask repo](https://github.com/ucg8j/flask-port3838-container) to github where I adjusted the 'hello world' [flask app](https://github.com/ucg8j/flask-port3838-container/blob/master/app.py) on line 28 to run on port 3838 (inside the container) and the [dockerfile](https://github.com/ucg8j/flask-port3838-container/blob/master/Dockerfile) to expose the container's port 3838 (line 14).\\n\\n```bash\\n# Clone repo\\n$ git clone https://github.com/ucg8j/flask-port3838-container.git\\n# Change directory\\n$ cd flask-port3838-container/\\n# Build image\\n$ docker build -t flaskapp .\\n# Test it runs as a docker container\\n$ docker run -p 4000:3838 flaskapp \\n```\\n\\nYou'll note that I am passing the publish argument `-p 4000:3838`. This allows you to map a container's port to the port of your host in the format of `hostport:containerport`. Since I've configured the flask docker container to port 3838 to mimic a shiny app I'll see whether I can manually map it to my computer's localhost of port 4000. That way I'll know that shinyproxy *should* be able to do the same.\\n\\n![flask-hello-world](/content/images/2017/09/flask-hello-world.png)\\n\\nNow to add the flask hello world image to shinyproxy by appending it to the app similar to the way we did with the additional shiny app:\\n\\n```yaml\\napps:\\n  - name: flaskapp\\n    display-name: Demo Flask app\\n    description: Adding another non shiny app to shinyproxy\\n    docker-cmd: [\\\"python\\\", \\\"app.py\\\"]\\n    docker-image: flaskapp\\n    groups: scientists\\n```\\n\\nNow (re)start shiny proxy `java -jar shinyproxy-1.0.0.jar`.\\n\\n![add-flask-app](/content/images/2017/10/add-flask-app.gif)\\n\\n\\n### Auth\\nBy default the authentication is setup to use a demo ldap server `ldap://ldap.forumsys.com:389/dc=example,dc=com` for more details visit [this page](http://www.forumsys.com/tutorials/integration-how-to/ldap/online-ldap-test-server/). I'm going to switch this to use the `simple` authentication. I'm also going to change one of the passwords to be confident these changes are taking effect. The top half of my `application.yml` file now looks like this:\\n\\n![application-switch-auth-to-simple](/content/images/2017/09/application-switch-auth-to-simple.png)\\n\\n## Deploy on a cloud service (nginx config)\\nAt this point it would be nice to have this on the cloud. Using [nginx](https://nginx.org/en/) the following config worked for me which was an adaption of the config listed on the [shinyproxy site](https://www.shinyproxy.io/security/). The box I was using was an [Azure VM](https://azure.microsoft.com/en-gb/services/virtual-machines/). \\n\\n```nginx\\n# Navigate into the nginx config\\n$ cd /etc/nginx/sites-available\\n# print the config that works for me\\n$ cat default\\n# Default server configuration\\nserver {\\n  listen 80 default_server;\\n  listen [::]:80 default_server;\\n\\n  root /var/www/html;\\n\\n  # Add index.php to the list if you are using PHP\\n  index index.html index.htm index.nginx-debian.html;\\n\\n  server_name **put your dns or domain here**;\\n\\n  location / {\\n\\n    proxy_pass                            http://127.0.0.1:8090/;\\n    proxy_http_version                    1.1;\\n    proxy_set_header Upgrade              $http_upgrade;\\n    proxy_set_header Connection           \\\"upgrade\\\";\\n    proxy_read_timeout                    600s;\\n    proxy_redirect                        off;\\n    proxy_set_header Host                 $http_host;\\n    proxy_set_header X-Real-IP            $remote_addr;\\n    proxy_set_header X-Forwarded-For      $proxy_add_x_forwarded_for;\\n    proxy_set_header X-Forwarded-Protocol $scheme;\\n    \\n  }\\n}\\n```\\n\\nOnce you have shinyproxy running on a cloud server, you may want to leave the application running without having a terminal open. To run leave shinyproxy running in the background run the following:\\n\\n```bash\\n$ nohup java -jar shinyproxy-1.0.0.jar &\\n```\\n\\n`nohup` is not a permanent solution. For instance, if your server goes down, shinyproxy will not automatically reboot. `nohup` did allow me to quickly get the app running in the background. Check the app, discover issues I needed to address then find the process by running `ps aux | grep java` kill that process... rinse and repeat until complete. To leave this permanently on a Ubuntu 16 server the [Spring documentation](https://spring.io/docs) has [a configuration guide for setting this as a system service](https://docs.spring.io/spring-boot/docs/current/reference/html/deployment-install.html#deployment-systemd-service). Additionally, [this SO post](https://stackoverflow.com/a/22121547/3691003) is also very helpful.\\n\\n## Conclusion\\nShinyproxy is a great contribution to the R/Shiny community by the team at [openanalytics](https://www.openanalytics.eu/). So a big thank you to them! *Productionising* shiny apps can be a bit of a pain considering the open source shiny server limitations (no authentication, no concurrency and no resource monitoring). Even accepting those limitations, [dependency management of R applications](/using-package-management-in-r/) is not widely practiced. Even when R applications use [packrat](https://rstudio.github.io/packrat/), language level dependency management doesn't prevent your software breaking due to different system level requirements. Hence, the generalisation of running a website that proxies to containerised applications *should* be an incredibly reliable architecture.\\n\\nIf you've found this content helpful why not...\\n\\n<a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\\n\\n## Resources\\n* [How to deploy Dash apps on Shinyproxy](/how-to-deploy-plotlys-dash-using-shinyproxy/)\\n* [Helpful SO post](https://stackoverflow.com/a/40187306/3691003)\\n* [Configuring nginx](https://www.linode.com/docs/web-servers/nginx/how-to-configure-nginx)\\n* [Spring configuration options i.e. `application.yml`](https://docs.spring.io/spring-boot/docs/current/reference/html/common-application-properties.html)\"}]],\"markups\":[],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><strong>Aim</strong>: Setup <a href=\"https://www.shinyproxy.io\">shinyproxy</a> as a production level way of deploying multiple containerised shiny apps with authentication. Additionally I'll demonstrate how to incorporate containerised python web apps and deploy this behind a reverse proxy using Nginx.</p>\n<p><strong>Conceptual Overview</strong><br>\n<img src=\"/content/images/2017/10/overview.png\" alt=\"\"></p>\n<p>This is a pretty significant piece of software. It renders <a href=\"https://www.rstudio.com/products/shiny-server-pro/\">Shiny Server Pro</a> redundant. As this solution, provides authentication, scalability, decreases maintenance (<i class=\"em em-heart\"></i> containers) and enables any web app to be running behind the scenes. So you could have a <a href=\"https://shiny.rstudio.com/gallery/\">shiny</a> app, a <a href=\"https://www.djangoproject.com/\">Django</a> app, a <a href=\"http://flask.pocoo.org/\">flask</a> app, <a href=\"https://vuejs.org/\">Vue.js</a>... and the end-user will simply see a website, whilst the backend has a whole range of different web applications spinning up on demand.</p>\n<p>I ran into some hurdles with <a href=\"https://www.shinyproxy.io/getting-started/#prerequisites\">these instructions</a>. So I'm documenting what worked for me. <strong>My setup:</strong> I set this up once on a Dell XPS 13 with a fresh install of Ubuntu 16.04.3 LTS and a Ubuntu 16.04.03 Azure VM.</p>\n<h2 id=\"prerequisitesassumptions\">Pre-requisites / Assumptions</h2>\n<h3 id=\"docker\">Docker</h3>\n<p>If you don't have <a href=\"https://www.docker.com/\">Docker</a> installed yet, I highly recommend running through <a href=\"https://docs.docker.com/get-started/\">Part 1: Orientation and setup</a> to <a href=\"https://docs.docker.com/get-started/part3/\">Part 3: Services</a> of <a href=\"https://docs.docker.com/get-started/\">Docker's Getting Started Documentation</a>.</p>\n<p>If running the following commands return similar console prints you should be fine to continue on.</p>\n<pre><code class=\"language-bash\">$ docker -v\nDocker version 17.06.2-ce, build cec0b72\n$ docker run hello-world \nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n</code></pre>\n<h5 id=\"dockerproblemsencounteredforubuntu16\">Docker Problems Encountered for Ubuntu 16</h5>\n<p>The most time consuming part of the setup were <a href=\"https://support.openanalytics.eu/t/shinyproxy-on-ubuntu-16-04/38/4\">the instructions for Ubuntu 16</a> on the <a href=\"https://www.shinyproxy.io\">shinyproxy site</a> guiding modification of <code>/lib/systemd/system/docker.service</code>. Per the documentation on Shinyproxy:</p>\n<p><img src=\"/content/images/2017/09/shinyproxy-ubuntu16-docker-config.png\" alt=\"shinyproxy-docker-instructions\"></p>\n<p>Running this configuration produced the following error:</p>\n<pre><code class=\"language-bash\">$ sudo systemctl restart docker\nJob for docker.service failed because the control process exited with error code. See &quot;systemctl status docker.service&quot; and &quot;journalctl -xe&quot; for details.\n</code></pre>\n<h5 id=\"solution\">Solution</h5>\n<p>Instead, edit <code>/lib/systemd/system/docker.service</code> and set <code>ExecStart</code> to:</p>\n<pre><code>ExecStart=/usr/bin/dockerd -H fd:// -D -H tcp://127.0.0.1:2375\n</code></pre>\n<p>Why? Shinyproxy will look to talk to the docker daemon on <code>tcp://127.0.0.1:2375</code>. If it can't, then the application won't work.</p>\n<p>Ensure that you restart docker after editing the config with the following commands:</p>\n<pre><code class=\"language-bash\">$ sudo systemctl daemon-reload\n$ sudo systemctl restart docker\n</code></pre>\n<h3 id=\"java8\">Java8</h3>\n<p>Shinyproxy requires a minimum of <a href=\"http://www.oracle.com/technetwork/java/javase/overview/java8-2100321.html\">Java 8</a>. I followed the advice per <a href=\"https://www.shinyproxy.io/getting-started/\">shinyproxy.io</a> and installed the OpenJDK <a href=\"http://zulu.org/\">Zulu</a>. This Quora answer from the product manager of Zulu gives some context behind this JDK:</p>\n<p><span class='quora-content-embed' data-name='What-is-it-like-to-use-Azul-Systems-Zulu-JVM/answer/Matt-Schuetze'>Read <a class='quora-content-link' data-width='559' data-height='250' href='https://www.quora.com/What-is-it-like-to-use-Azul-Systems-Zulu-JVM/answer/Matt-Schuetze' data-type='answer' data-id='8731568' data-key='6ab4b9b9ff8a05fcb861058252be404c' load-full-answer='False' data-embed='hdayukz'><a href='https://www.quora.com/Matt-Schuetze'>Matt Schuetze</a>'s <a href='/What-is-it-like-to-use-Azul-Systems-Zulu-JVM#ans8731568'>answer</a> to <a href='/What-is-it-like-to-use-Azul-Systems-Zulu-JVM' ref='canonical'><span class=\"rendered_qtext\">What is it like to use Azul Systems' Zulu JVM?</span></a></a> on <a href='https://www.__nousername__.main.quora.com'>Quora</a><script type=\"text/javascript\" src=\"https://www.quora.com/widgets/content\"></script></span></p>\n<p>Run the following at the command line:</p>\n<pre><code class=\"language-bash\">$ java -version\nopenjdk version &quot;1.8.0_144&quot;\nOpenJDK Runtime Environment (Zulu 8.23.0.3-linux64) (build 1.8.0_144-b01)\nOpenJDK 64-Bit Server VM (Zulu 8.23.0.3-linux64) (build 25.144-b01, mixed mode)\n</code></pre>\n<p>If you have Java 8 then continue. N.B. Zulu 8.xx.x.x is Java 8. Otherwise, run the following commands to install:</p>\n<pre><code class=\"language-bash\">$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 0x219BD9C9\n$ apt_source='deb http://repos.azulsystems.com/debian stable main'\n$ apt_list='/etc/apt/sources.list.d/zulu.list'\n$ echo &quot;$apt_source&quot; | sudo tee &quot;$apt_list&quot; &gt; /dev/null\n$ sudo apt-get update\n$ sudo apt-get install zulu-8\n</code></pre>\n<p><strong>N.B.</strong> I did experience a maven build fail when running <code>mvn -U clean install</code> on a Mac with a Java9 runtime. Uninstalling Java 9 and running <code>brew cask install java8</code> fixed the problem.</p>\n<h2 id=\"downloadandcompileshinyproxy\">Download and Compile Shiny Proxy</h2>\n<p>There are two options for downloading the shinyproxy software.</p>\n<p><strong>Option 1</strong> - I cloned the <a href=\"https://github.com/openanalytics/shinyproxy/\">shinyproxy repo</a>. I did not have <a href=\"https://maven.apache.org/\">apache maven</a> installed so I needed to <a href=\"https://maven.apache.org/install.html\">run through the installation of maven first</a>.</p>\n<pre><code class=\"language-bash\"># Clone the repo\n$ git clone https://github.com/openanalytics/shinyproxy.git  \n# Change directory\n$ cd shinyproxy/\n# Compile the program\n$ mvn -U clean install\n</code></pre>\n<p><strong>Option 2</strong> - You can also go to the <a href=\"https://www.shinyproxy.io/downloads/\">downloads page</a> and select the latest file for your particular operating system.</p>\n<h2 id=\"pulltheshinyappcontainers\">Pull the shiny app containers</h2>\n<p>We now have shinyproxy ready to run containerised applications. However, we don't have the images that shinyproxy is expecting. We need to pull those first:</p>\n<pre><code class=\"language-bash\"># Pull the shiny app images\n$ sudo docker pull openanalytics/shinyproxy-demo\n# Check they are now in the list of docker images\n$ docker images | grep shinyproxy\n</code></pre>\n<h2 id=\"runshinyproxy\">Run Shiny Proxy</h2>\n<p>Run the following commands:</p>\n<pre><code class=\"language-bash\"># Ensure you are in the right directory\n$ cd ~/shinyproxy/target/\n# Run shinyproxy\n$ java -jar shinyproxy-1.0.0.jar\n</code></pre>\n<p>At this point you should now be able to navigate to <a href=\"http://localhost:8080\">http://localhost:8080</a> and see the sign in page and login with 'tesla' and 'password' as username and password respectively. If you are doing this on a remote server, test the application is up and running by running:</p>\n<pre><code class=\"language-bash\"># Print the html of the site to the terminal\ncurl http://localhost:8080\n</code></pre>\n<p><img src=\"/content/images/2017/09/shinyproxy-login.png\" alt=\"shinyproxy-login\"></p>\n<p>Once logged in launch an application.</p>\n<p><img src=\"/content/images/2017/09/shinyproxy-select-app.png\" alt=\"shinyproxy-select-app\"></p>\n<p>And bingo we have a containerised and authenticated shiny environment.</p>\n<p><img src=\"/content/images/2017/09/shinyproxy-shiny-app.png\" alt=\"\"></p>\n<h2 id=\"addingadditionalshinyapps\">Adding Additional Shiny Apps</h2>\n<p><a href=\"https://www.shinyproxy.io/deploying-apps/\">Shinyproxy docs</a> provide a ready to go shiny application with a <a href=\"https://docs.docker.com/engine/reference/builder/\">dockerfile</a> to build the image. Of course, to use shinyproxy for your own custom shiny applications the dockerfile in the <a href=\"https://github.com/openanalytics/shinyproxy-template\">shinyproxy-template repo</a> acts as a template to follow. I'm going to go through the process of adding shinyproxy's pre-built shiny app.</p>\n<pre><code class=\"language-bash\"># Clone the shiny app repo\n$ git clone https://github.com/openanalytics/shinyproxy-template.git\n# Change into the repo\n$ cd shinyproxy-template/\n# Build the image\n$ docker build -t openanalytics/shinyproxy-template .\n# Check the image is now available in docker\n$ docker images | grep &quot;shinyproxy-template\\|REPOSITORY&quot;\nREPOSITORY                          TAG                 IMAGE ID            CREATED             SIZE\nopenanalytics/shinyproxy-template   latest              16e8c49e2261        25 minutes ago      851MB\n# Check the shiny app runs normally outside shinyproxy\n$ docker run -it -p 3838:3838  openanalytics/shinyproxy-template\n</code></pre>\n<p>The new shiny app should now be running on <a href=\"http://0.0.0.0:3838/\">http://0.0.0.0:3838/</a>.</p>\n<p><img src=\"/content/images/2017/09/additional-shiny-app.png\" alt=\"additional-shiny-app\"></p>\n<p>Now we need to tell shinyproxy to include this additional shiny app by editing creating and editing a <code>application.yml</code> file.</p>\n<pre><code class=\"language-bash\"># Ensure you are in the right directory\n$ cd ~/shinyproxy/target/\n# Create application.yml from repo template\n$ curl https://raw.githubusercontent.com/openanalytics/shinyproxy/master/src/main/resources/application-demo.yml &gt; application.yml\n</code></pre>\n<p>We now need to add our new shiny app to the <code>application.yml</code> file.</p>\n<pre><code class=\"language-yaml\">apps:\n  - name: euler\n    display-name: Euler's number\n    description: Adding another app to shinyproxy\n    docker-cmd: [&quot;R&quot;, &quot;-e shiny::runApp('/root/euler')&quot;]\n    docker-image: openanalytics/shinyproxy-template\n    groups: scientists\n</code></pre>\n<p>Now I'm going to run shinyproxy again to confirm the change has been successful.</p>\n<p><img src=\"/content/images/2017/09/add-another-shiny-app.png\" alt=\"add-another-shiny-app\"></p>\n<p>And does the application launch</p>\n<p><img src=\"/content/images/2017/09/euler-app-running.png\" alt=\"\"></p>\n<p>Success!</p>\n<p>If you're finding this content helpful why not...</p>\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\n  <link href=\"https://fonts.googleapis.com/css?family=Cookie\" rel=\"stylesheet\"><a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a>\n<h2 id=\"addingadditionalnonshinyapps\">Adding Additional non-Shiny Apps</h2>\n<p>I'm going with a very basic 'hello world' flask app. The main issue here is configuring the ports for shinyproxy to pick up the app. Since shiny by default runs on port 3838, I mapped the containerised flask app to run on port 3838 too. I pushed <a href=\"https://github.com/ucg8j/flask-port3838-container\">a flask repo</a> to github where I adjusted the 'hello world' <a href=\"https://github.com/ucg8j/flask-port3838-container/blob/master/app.py\">flask app</a> on line 28 to run on port 3838 (inside the container) and the <a href=\"https://github.com/ucg8j/flask-port3838-container/blob/master/Dockerfile\">dockerfile</a> to expose the container's port 3838 (line 14).</p>\n<pre><code class=\"language-bash\"># Clone repo\n$ git clone https://github.com/ucg8j/flask-port3838-container.git\n# Change directory\n$ cd flask-port3838-container/\n# Build image\n$ docker build -t flaskapp .\n# Test it runs as a docker container\n$ docker run -p 4000:3838 flaskapp \n</code></pre>\n<p>You'll note that I am passing the publish argument <code>-p 4000:3838</code>. This allows you to map a container's port to the port of your host in the format of <code>hostport:containerport</code>. Since I've configured the flask docker container to port 3838 to mimic a shiny app I'll see whether I can manually map it to my computer's localhost of port 4000. That way I'll know that shinyproxy <em>should</em> be able to do the same.</p>\n<p><img src=\"/content/images/2017/09/flask-hello-world.png\" alt=\"flask-hello-world\"></p>\n<p>Now to add the flask hello world image to shinyproxy by appending it to the app similar to the way we did with the additional shiny app:</p>\n<pre><code class=\"language-yaml\">apps:\n  - name: flaskapp\n    display-name: Demo Flask app\n    description: Adding another non shiny app to shinyproxy\n    docker-cmd: [&quot;python&quot;, &quot;app.py&quot;]\n    docker-image: flaskapp\n    groups: scientists\n</code></pre>\n<p>Now (re)start shiny proxy <code>java -jar shinyproxy-1.0.0.jar</code>.</p>\n<p><img src=\"/content/images/2017/10/add-flask-app.gif\" alt=\"add-flask-app\"></p>\n<h3 id=\"auth\">Auth</h3>\n<p>By default the authentication is setup to use a demo ldap server <code>ldap://ldap.forumsys.com:389/dc=example,dc=com</code> for more details visit <a href=\"http://www.forumsys.com/tutorials/integration-how-to/ldap/online-ldap-test-server/\">this page</a>. I'm going to switch this to use the <code>simple</code> authentication. I'm also going to change one of the passwords to be confident these changes are taking effect. The top half of my <code>application.yml</code> file now looks like this:</p>\n<p><img src=\"/content/images/2017/09/application-switch-auth-to-simple.png\" alt=\"application-switch-auth-to-simple\"></p>\n<h2 id=\"deployonacloudservicenginxconfig\">Deploy on a cloud service (nginx config)</h2>\n<p>At this point it would be nice to have this on the cloud. Using <a href=\"https://nginx.org/en/\">nginx</a> the following config worked for me which was an adaption of the config listed on the <a href=\"https://www.shinyproxy.io/security/\">shinyproxy site</a>. The box I was using was an <a href=\"https://azure.microsoft.com/en-gb/services/virtual-machines/\">Azure VM</a>.</p>\n<pre><code class=\"language-nginx\"># Navigate into the nginx config\n$ cd /etc/nginx/sites-available\n# print the config that works for me\n$ cat default\n# Default server configuration\nserver {\n  listen 80 default_server;\n  listen [::]:80 default_server;\n\n  root /var/www/html;\n\n  # Add index.php to the list if you are using PHP\n  index index.html index.htm index.nginx-debian.html;\n\n  server_name **put your dns or domain here**;\n\n  location / {\n\n    proxy_pass                            http://127.0.0.1:8090/;\n    proxy_http_version                    1.1;\n    proxy_set_header Upgrade              $http_upgrade;\n    proxy_set_header Connection           &quot;upgrade&quot;;\n    proxy_read_timeout                    600s;\n    proxy_redirect                        off;\n    proxy_set_header Host                 $http_host;\n    proxy_set_header X-Real-IP            $remote_addr;\n    proxy_set_header X-Forwarded-For      $proxy_add_x_forwarded_for;\n    proxy_set_header X-Forwarded-Protocol $scheme;\n    \n  }\n}\n</code></pre>\n<p>Once you have shinyproxy running on a cloud server, you may want to leave the application running without having a terminal open. To run leave shinyproxy running in the background run the following:</p>\n<pre><code class=\"language-bash\">$ nohup java -jar shinyproxy-1.0.0.jar &amp;\n</code></pre>\n<p><code>nohup</code> is not a permanent solution. For instance, if your server goes down, shinyproxy will not automatically reboot. <code>nohup</code> did allow me to quickly get the app running in the background. Check the app, discover issues I needed to address then find the process by running <code>ps aux | grep java</code> kill that process... rinse and repeat until complete. To leave this permanently on a Ubuntu 16 server the <a href=\"https://spring.io/docs\">Spring documentation</a> has <a href=\"https://docs.spring.io/spring-boot/docs/current/reference/html/deployment-install.html#deployment-systemd-service\">a configuration guide for setting this as a system service</a>. Additionally, <a href=\"https://stackoverflow.com/a/22121547/3691003\">this SO post</a> is also very helpful.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Shinyproxy is a great contribution to the R/Shiny community by the team at <a href=\"https://www.openanalytics.eu/\">openanalytics</a>. So a big thank you to them! <em>Productionising</em> shiny apps can be a bit of a pain considering the open source shiny server limitations (no authentication, no concurrency and no resource monitoring). Even accepting those limitations, <a href=\"/using-package-management-in-r/\">dependency management of R applications</a> is not widely practiced. Even when R applications use <a href=\"https://rstudio.github.io/packrat/\">packrat</a>, language level dependency management doesn't prevent your software breaking due to different system level requirements. Hence, the generalisation of running a website that proxies to containerised applications <em>should</em> be an incredibly reliable architecture.</p>\n<p>If you've found this content helpful why not...</p>\n<a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a>\n<h2 id=\"resources\">Resources</h2>\n<ul>\n<li><a href=\"/how-to-deploy-plotlys-dash-using-shinyproxy/\">How to deploy Dash apps on Shinyproxy</a></li>\n<li><a href=\"https://stackoverflow.com/a/40187306/3691003\">Helpful SO post</a></li>\n<li><a href=\"https://www.linode.com/docs/web-servers/nginx/how-to-configure-nginx\">Configuring nginx</a></li>\n<li><a href=\"https://docs.spring.io/spring-boot/docs/current/reference/html/common-application-properties.html\">Spring configuration options i.e. <code>application.yml</code></a></li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"35","plaintext":"Aim: Setup shinyproxy [https://www.shinyproxy.io] as a production level way of\ndeploying multiple containerised shiny apps with authentication. Additionally\nI'll demonstrate how to incorporate containerised python web apps and deploy\nthis behind a reverse proxy using Nginx.\n\nConceptual Overview\n\n\nThis is a pretty significant piece of software. It renders Shiny Server Pro\n[https://www.rstudio.com/products/shiny-server-pro/] redundant. As this\nsolution, provides authentication, scalability, decreases maintenance ( \ncontainers) and enables any web app to be running behind the scenes. So you\ncould have a shiny [https://shiny.rstudio.com/gallery/] app, a Django\n[https://www.djangoproject.com/] app, a flask [http://flask.pocoo.org/] app, \nVue.js [https://vuejs.org/]... and the end-user will simply see a website,\nwhilst the backend has a whole range of different web applications spinning up\non demand.\n\nI ran into some hurdles with these instructions\n[https://www.shinyproxy.io/getting-started/#prerequisites]. So I'm documenting\nwhat worked for me. My setup: I set this up once on a Dell XPS 13 with a fresh\ninstall of Ubuntu 16.04.3 LTS and a Ubuntu 16.04.03 Azure VM.\n\nPre-requisites / Assumptions\nDocker\nIf you don't have Docker [https://www.docker.com/] installed yet, I highly\nrecommend running through Part 1: Orientation and setup\n[https://docs.docker.com/get-started/] to Part 3: Services\n[https://docs.docker.com/get-started/part3/] of Docker's Getting Started\nDocumentation [https://docs.docker.com/get-started/].\n\nIf running the following commands return similar console prints you should be\nfine to continue on.\n\n$ docker -v\nDocker version 17.06.2-ce, build cec0b72\n$ docker run hello-world \nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\n\nDocker Problems Encountered for Ubuntu 16\nThe most time consuming part of the setup were the instructions for Ubuntu 16\n[https://support.openanalytics.eu/t/shinyproxy-on-ubuntu-16-04/38/4] on the \nshinyproxy site [https://www.shinyproxy.io] guiding modification of \n/lib/systemd/system/docker.service. Per the documentation on Shinyproxy:\n\n\n\nRunning this configuration produced the following error:\n\n$ sudo systemctl restart docker\nJob for docker.service failed because the control process exited with error code. See \"systemctl status docker.service\" and \"journalctl -xe\" for details.\n\n\nSolution\nInstead, edit /lib/systemd/system/docker.service and set ExecStart to:\n\nExecStart=/usr/bin/dockerd -H fd:// -D -H tcp://127.0.0.1:2375\n\n\nWhy? Shinyproxy will look to talk to the docker daemon on tcp://127.0.0.1:2375.\nIf it can't, then the application won't work.\n\nEnsure that you restart docker after editing the config with the following\ncommands:\n\n$ sudo systemctl daemon-reload\n$ sudo systemctl restart docker\n\n\nJava8\nShinyproxy requires a minimum of Java 8\n[http://www.oracle.com/technetwork/java/javase/overview/java8-2100321.html]. I\nfollowed the advice per shinyproxy.io\n[https://www.shinyproxy.io/getting-started/] and installed the OpenJDK Zulu\n[http://zulu.org/]. This Quora answer from the product manager of Zulu gives\nsome context behind this JDK:\n\nRead Matt Schuetze [https://www.quora.com/Matt-Schuetze]'s answer\n[/What-is-it-like-to-use-Azul-Systems-Zulu-JVM#ans8731568] to What is it like\nto\nuse Azul Systems' Zulu JVM? [/What-is-it-like-to-use-Azul-Systems-Zulu-JVM]\n[https://www.quora.com/What-is-it-like-to-use-Azul-Systems-Zulu-JVM/answer/Matt-Schuetze] \non Quora [https://www.__nousername__.main.quora.com]\n\nRun the following at the command line:\n\n$ java -version\nopenjdk version \"1.8.0_144\"\nOpenJDK Runtime Environment (Zulu 8.23.0.3-linux64) (build 1.8.0_144-b01)\nOpenJDK 64-Bit Server VM (Zulu 8.23.0.3-linux64) (build 25.144-b01, mixed mode)\n\n\nIf you have Java 8 then continue. N.B. Zulu 8.xx.x.x is Java 8. Otherwise, run\nthe following commands to install:\n\n$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 0x219BD9C9\n$ apt_source='deb http://repos.azulsystems.com/debian stable main'\n$ apt_list='/etc/apt/sources.list.d/zulu.list'\n$ echo \"$apt_source\" | sudo tee \"$apt_list\" > /dev/null\n$ sudo apt-get update\n$ sudo apt-get install zulu-8\n\n\nN.B. I did experience a maven build fail when running mvn -U clean install on a\nMac with a Java9 runtime. Uninstalling Java 9 and running brew cask install\njava8 fixed the problem.\n\nDownload and Compile Shiny Proxy\nThere are two options for downloading the shinyproxy software.\n\nOption 1 - I cloned the shinyproxy repo\n[https://github.com/openanalytics/shinyproxy/]. I did not have apache maven\n[https://maven.apache.org/] installed so I needed to run through the\ninstallation of maven first [https://maven.apache.org/install.html].\n\n# Clone the repo\n$ git clone https://github.com/openanalytics/shinyproxy.git  \n# Change directory\n$ cd shinyproxy/\n# Compile the program\n$ mvn -U clean install\n\n\nOption 2 - You can also go to the downloads page\n[https://www.shinyproxy.io/downloads/] and select the latest file for your\nparticular operating system.\n\nPull the shiny app containers\nWe now have shinyproxy ready to run containerised applications. However, we\ndon't have the images that shinyproxy is expecting. We need to pull those first:\n\n# Pull the shiny app images\n$ sudo docker pull openanalytics/shinyproxy-demo\n# Check they are now in the list of docker images\n$ docker images | grep shinyproxy\n\n\nRun Shiny Proxy\nRun the following commands:\n\n# Ensure you are in the right directory\n$ cd ~/shinyproxy/target/\n# Run shinyproxy\n$ java -jar shinyproxy-1.0.0.jar\n\n\nAt this point you should now be able to navigate to http://localhost:8080 and\nsee the sign in page and login with 'tesla' and 'password' as username and\npassword respectively. If you are doing this on a remote server, test the\napplication is up and running by running:\n\n# Print the html of the site to the terminal\ncurl http://localhost:8080\n\n\n\n\nOnce logged in launch an application.\n\n\n\nAnd bingo we have a containerised and authenticated shiny environment.\n\n\n\nAdding Additional Shiny Apps\nShinyproxy docs [https://www.shinyproxy.io/deploying-apps/] provide a ready to\ngo shiny application with a dockerfile\n[https://docs.docker.com/engine/reference/builder/] to build the image. Of\ncourse, to use shinyproxy for your own custom shiny applications the dockerfile\nin the shinyproxy-template repo\n[https://github.com/openanalytics/shinyproxy-template] acts as a template to\nfollow. I'm going to go through the process of adding shinyproxy's pre-built\nshiny app.\n\n# Clone the shiny app repo\n$ git clone https://github.com/openanalytics/shinyproxy-template.git\n# Change into the repo\n$ cd shinyproxy-template/\n# Build the image\n$ docker build -t openanalytics/shinyproxy-template .\n# Check the image is now available in docker\n$ docker images | grep \"shinyproxy-template\\|REPOSITORY\"\nREPOSITORY                          TAG                 IMAGE ID            CREATED             SIZE\nopenanalytics/shinyproxy-template   latest              16e8c49e2261        25 minutes ago      851MB\n# Check the shiny app runs normally outside shinyproxy\n$ docker run -it -p 3838:3838  openanalytics/shinyproxy-template\n\n\nThe new shiny app should now be running on http://0.0.0.0:3838/.\n\n\n\nNow we need to tell shinyproxy to include this additional shiny app by editing\ncreating and editing a application.yml file.\n\n# Ensure you are in the right directory\n$ cd ~/shinyproxy/target/\n# Create application.yml from repo template\n$ curl https://raw.githubusercontent.com/openanalytics/shinyproxy/master/src/main/resources/application-demo.yml > application.yml\n\n\nWe now need to add our new shiny app to the application.yml file.\n\napps:\n  - name: euler\n    display-name: Euler's number\n    description: Adding another app to shinyproxy\n    docker-cmd: [\"R\", \"-e shiny::runApp('/root/euler')\"]\n    docker-image: openanalytics/shinyproxy-template\n    groups: scientists\n\n\nNow I'm going to run shinyproxy again to confirm the change has been successful.\n\n\n\nAnd does the application launch\n\n\n\nSuccess!\n\nIf you're finding this content helpful why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]Adding Additional\nnon-Shiny Apps\nI'm going with a very basic 'hello world' flask app. The main issue here is\nconfiguring the ports for shinyproxy to pick up the app. Since shiny by default\nruns on port 3838, I mapped the containerised flask app to run on port 3838 too.\nI pushed a flask repo [https://github.com/ucg8j/flask-port3838-container] to\ngithub where I adjusted the 'hello world' flask app\n[https://github.com/ucg8j/flask-port3838-container/blob/master/app.py] on line\n28 to run on port 3838 (inside the container) and the dockerfile\n[https://github.com/ucg8j/flask-port3838-container/blob/master/Dockerfile] to\nexpose the container's port 3838 (line 14).\n\n# Clone repo\n$ git clone https://github.com/ucg8j/flask-port3838-container.git\n# Change directory\n$ cd flask-port3838-container/\n# Build image\n$ docker build -t flaskapp .\n# Test it runs as a docker container\n$ docker run -p 4000:3838 flaskapp \n\n\nYou'll note that I am passing the publish argument -p 4000:3838. This allows you\nto map a container's port to the port of your host in the format of \nhostport:containerport. Since I've configured the flask docker container to port\n3838 to mimic a shiny app I'll see whether I can manually map it to my\ncomputer's localhost of port 4000. That way I'll know that shinyproxy should be\nable to do the same.\n\n\n\nNow to add the flask hello world image to shinyproxy by appending it to the app\nsimilar to the way we did with the additional shiny app:\n\napps:\n  - name: flaskapp\n    display-name: Demo Flask app\n    description: Adding another non shiny app to shinyproxy\n    docker-cmd: [\"python\", \"app.py\"]\n    docker-image: flaskapp\n    groups: scientists\n\n\nNow (re)start shiny proxy java -jar shinyproxy-1.0.0.jar.\n\n\n\nAuth\nBy default the authentication is setup to use a demo ldap server \nldap://ldap.forumsys.com:389/dc=example,dc=com for more details visit this page\n[http://www.forumsys.com/tutorials/integration-how-to/ldap/online-ldap-test-server/]\n. I'm going to switch this to use the simple authentication. I'm also going to\nchange one of the passwords to be confident these changes are taking effect. The\ntop half of my application.yml file now looks like this:\n\n\n\nDeploy on a cloud service (nginx config)\nAt this point it would be nice to have this on the cloud. Using nginx\n[https://nginx.org/en/] the following config worked for me which was an adaption\nof the config listed on the shinyproxy site\n[https://www.shinyproxy.io/security/]. The box I was using was an Azure VM\n[https://azure.microsoft.com/en-gb/services/virtual-machines/].\n\n# Navigate into the nginx config\n$ cd /etc/nginx/sites-available\n# print the config that works for me\n$ cat default\n# Default server configuration\nserver {\n  listen 80 default_server;\n  listen [::]:80 default_server;\n\n  root /var/www/html;\n\n  # Add index.php to the list if you are using PHP\n  index index.html index.htm index.nginx-debian.html;\n\n  server_name **put your dns or domain here**;\n\n  location / {\n\n    proxy_pass                            http://127.0.0.1:8090/;\n    proxy_http_version                    1.1;\n    proxy_set_header Upgrade              $http_upgrade;\n    proxy_set_header Connection           \"upgrade\";\n    proxy_read_timeout                    600s;\n    proxy_redirect                        off;\n    proxy_set_header Host                 $http_host;\n    proxy_set_header X-Real-IP            $remote_addr;\n    proxy_set_header X-Forwarded-For      $proxy_add_x_forwarded_for;\n    proxy_set_header X-Forwarded-Protocol $scheme;\n    \n  }\n}\n\n\nOnce you have shinyproxy running on a cloud server, you may want to leave the\napplication running without having a terminal open. To run leave shinyproxy\nrunning in the background run the following:\n\n$ nohup java -jar shinyproxy-1.0.0.jar &\n\n\nnohup is not a permanent solution. For instance, if your server goes down,\nshinyproxy will not automatically reboot. nohup did allow me to quickly get the\napp running in the background. Check the app, discover issues I needed to\naddress then find the process by running ps aux | grep java kill that process...\nrinse and repeat until complete. To leave this permanently on a Ubuntu 16 server\nthe Spring documentation [https://spring.io/docs] has a configuration guide for\nsetting this as a system service\n[https://docs.spring.io/spring-boot/docs/current/reference/html/deployment-install.html#deployment-systemd-service]\n. Additionally, this SO post [https://stackoverflow.com/a/22121547/3691003] is\nalso very helpful.\n\nConclusion\nShinyproxy is a great contribution to the R/Shiny community by the team at \nopenanalytics [https://www.openanalytics.eu/]. So a big thank you to them! \nProductionising shiny apps can be a bit of a pain considering the open source\nshiny server limitations (no authentication, no concurrency and no resource\nmonitoring). Even accepting those limitations, dependency management of R\napplications [/using-package-management-in-r/] is not widely practiced. Even\nwhen R applications use packrat [https://rstudio.github.io/packrat/], language\nlevel dependency management doesn't prevent your software breaking due to\ndifferent system level requirements. Hence, the generalisation of running a\nwebsite that proxies to containerised applications should be an incredibly\nreliable architecture.\n\nIf you've found this content helpful why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]Resources\n * How to deploy Dash apps on Shinyproxy\n   [/how-to-deploy-plotlys-dash-using-shinyproxy/]\n * Helpful SO post [https://stackoverflow.com/a/40187306/3691003]\n * Configuring nginx\n   [https://www.linode.com/docs/web-servers/nginx/how-to-configure-nginx]\n * Spring configuration options i.e. application.yml\n   [https://docs.spring.io/spring-boot/docs/current/reference/html/common-application-properties.html]","feature_image":"/content/images/2017/10/background-1.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-09-21 10:55:53","created_by":"1","updated_at":"2020-02-29 22:46:57","updated_by":null,"published_at":"2017-10-11 13:07:06","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcfc","uuid":"6294a8f5-500a-4cc5-848e-6a06f5b9360b","title":"Predicting Churn in R","slug":"predicting-churn-in-r","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"https://www.linkedin.com/pulse/churn-modelling-you-ready-face-your-demons-maciej-wasiak\\nhttps://medium.com/@b.khaleghi/what-makes-predicting-customer-churn-a-challenge-be195f35366e\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><a href=\"https://www.linkedin.com/pulse/churn-modelling-you-ready-face-your-demons-maciej-wasiak\">https://www.linkedin.com/pulse/churn-modelling-you-ready-face-your-demons-maciej-wasiak</a><br>\n<a href=\"https://medium.com/@b.khaleghi/what-makes-predicting-customer-churn-a-challenge-be195f35366e\">https://medium.com/@b.khaleghi/what-makes-predicting-customer-churn-a-challenge-be195f35366e</a></p>\n<!--kg-card-end: markdown-->","comment_id":"36","plaintext":"https://www.linkedin.com/pulse/churn-modelling-you-ready-face-your-demons-maciej-wasiak\nhttps://medium.com/@b.khaleghi/what-makes-predicting-customer-churn-a-challenge-be195f35366e","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-09-22 08:29:47","created_by":"1","updated_at":"2017-09-22 08:32:35","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcfd","uuid":"16ab998a-61f7-48f9-8b67-dc1c4f19d938","title":"Docker on macOS Sierra","slug":"docker-on-macos-sierra","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"http://www.cameronmaske.com/docker-on-osx/\\n\\nhttps://www.viget.com/articles/how-to-use-docker-on-os-x-the-missing-guide\\n\\nhttps://pilsniak.com/how-to-install-docker-on-mac-os-using-brew/\\n\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><a href=\"http://www.cameronmaske.com/docker-on-osx/\">http://www.cameronmaske.com/docker-on-osx/</a></p>\n<p><a href=\"https://www.viget.com/articles/how-to-use-docker-on-os-x-the-missing-guide\">https://www.viget.com/articles/how-to-use-docker-on-os-x-the-missing-guide</a></p>\n<p><a href=\"https://pilsniak.com/how-to-install-docker-on-mac-os-using-brew/\">https://pilsniak.com/how-to-install-docker-on-mac-os-using-brew/</a></p>\n<!--kg-card-end: markdown-->","comment_id":"37","plaintext":"http://www.cameronmaske.com/docker-on-osx/\n\nhttps://www.viget.com/articles/how-to-use-docker-on-os-x-the-missing-guide\n\nhttps://pilsniak.com/how-to-install-docker-on-mac-os-using-brew/","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-09-29 10:26:54","created_by":"1","updated_at":"2017-09-29 13:22:18","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcfe","uuid":"18a00980-f372-4042-ba2f-6338628e1be5","title":"Python on a mac and virtualenv - the right way to do things","slug":"python-on-a-mac-and-virtualenv-the-right-way-to-do-things","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"brew install python versions\\n\\n\\n\\nmanaging projects with different python versions\\n\\n\\n\\n## Resources\\n[Activate/deactivate virtualenv](http://sourabhbajaj.com/mac-setup/Python/virtualenv.html)\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>brew install python versions</p>\n<p>managing projects with different python versions</p>\n<h2 id=\"resources\">Resources</h2>\n<p><a href=\"http://sourabhbajaj.com/mac-setup/Python/virtualenv.html\">Activate/deactivate virtualenv</a></p>\n<!--kg-card-end: markdown-->","comment_id":"38","plaintext":"brew install python versions\n\nmanaging projects with different python versions\n\nResources\nActivate/deactivate virtualenv\n[http://sourabhbajaj.com/mac-setup/Python/virtualenv.html]","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-09-29 13:22:54","created_by":"1","updated_at":"2017-09-29 13:25:42","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dcff","uuid":"0b29f471-ab25-470a-8aba-fd03991dd1b0","title":"Deployment Architectures for Shiny","slug":"deployment-architectures-for-shiny","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}","html":null,"comment_id":"39","plaintext":null,"feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-10-11 10:57:25","created_by":"1","updated_at":"2017-10-11 10:57:25","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd00","uuid":"d79c4df7-e154-4940-82b8-cd417317a7db","title":"An Introduction to Recommender Systems","slug":"an-introduction-to-recommender-systems","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"\\\"NBO has its origins in Amazon.com’s early use of so-called recommendation software (or collaborative filtering, in engineering-speak) to spur shoppers toward a “next best offer” based on site page visits and, of course, actual purchases.\\\"\\nhttps://www.cooladata.com/blog/next-best-offer-customer-based-predictive-datas-new-frontier/\\n\\nhttps://hackernoon.com/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe\\n\\n## Dithering\\nIs a technique to improve the learning of a recommender system by taking items with lower relevancy scores and bringing them up the ranking. As users generally don't frequently go beyond the first few results we won't have feedback data on these recommendations. E.g. click through rates on search results almost never go past the first page of results.\\n\\n>As a result, the recommendation engine mostly gets feedback on re‐ sults that it already knows about and gets very little feedback on results at the edge of current knowledge. This limitation causes the recom‐ mendation engine to stagnate at or near the initial performance level. It does not continue to learn.\\n\\nSource: p39 Practical Machine Learning: Innovations in Recommendation\\n\\n\\n## Anti-Flood\\n>It is much better to avoid monotony in the user experience by pro‐ viding diversity in recommendations with no more than a few of each kind of results. \\n\\n>...our experience has been that it is much easier to ruin an otherwise good recommendation engine than it is to get diverse results out of the engine while maintaining overall quality... As a precaution, it is much easier to simply reorder the recommendations\\n\\nSource: p.40 Practical Machine Learning: Innovations in Recommendation\\n\\n## Evaluate with A/B testing\\n\\n\\n# Further Reading\\n- [](https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3)\\n- [](https://www.cs.uic.edu/~elena/pubs/kotsogiannis-wsdm17.pdf)\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>&quot;NBO has its origins in Amazon.com’s early use of so-called recommendation software (or collaborative filtering, in engineering-speak) to spur shoppers toward a “next best offer” based on site page visits and, of course, actual purchases.&quot;<br>\n<a href=\"https://www.cooladata.com/blog/next-best-offer-customer-based-predictive-datas-new-frontier/\">https://www.cooladata.com/blog/next-best-offer-customer-based-predictive-datas-new-frontier/</a></p>\n<p><a href=\"https://hackernoon.com/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe\">https://hackernoon.com/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe</a></p>\n<h2 id=\"dithering\">Dithering</h2>\n<p>Is a technique to improve the learning of a recommender system by taking items with lower relevancy scores and bringing them up the ranking. As users generally don't frequently go beyond the first few results we won't have feedback data on these recommendations. E.g. click through rates on search results almost never go past the first page of results.</p>\n<blockquote>\n<p>As a result, the recommendation engine mostly gets feedback on re‐ sults that it already knows about and gets very little feedback on results at the edge of current knowledge. This limitation causes the recom‐ mendation engine to stagnate at or near the initial performance level. It does not continue to learn.</p>\n</blockquote>\n<p>Source: p39 Practical Machine Learning: Innovations in Recommendation</p>\n<h2 id=\"antiflood\">Anti-Flood</h2>\n<blockquote>\n<p>It is much better to avoid monotony in the user experience by pro‐ viding diversity in recommendations with no more than a few of each kind of results.</p>\n</blockquote>\n<blockquote>\n<p>...our experience has been that it is much easier to ruin an otherwise good recommendation engine than it is to get diverse results out of the engine while maintaining overall quality... As a precaution, it is much easier to simply reorder the recommendations</p>\n</blockquote>\n<p>Source: p.40 Practical Machine Learning: Innovations in Recommendation</p>\n<h2 id=\"evaluatewithabtesting\">Evaluate with A/B testing</h2>\n<h1 id=\"furtherreading\">Further Reading</h1>\n<ul>\n<li><a href=\"https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3\"></a></li>\n<li><a href=\"https://www.cs.uic.edu/~elena/pubs/kotsogiannis-wsdm17.pdf\"></a></li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"40","plaintext":"\"NBO has its origins in Amazon.com’s early use of so-called recommendation\nsoftware (or collaborative filtering, in engineering-speak) to spur shoppers\ntoward a “next best offer” based on site page visits and, of course, actual\npurchases.\"\nhttps://www.cooladata.com/blog/next-best-offer-customer-based-predictive-datas-new-frontier/\n\nhttps://hackernoon.com/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe\n\nDithering\nIs a technique to improve the learning of a recommender system by taking items\nwith lower relevancy scores and bringing them up the ranking. As users generally\ndon't frequently go beyond the first few results we won't have feedback data on\nthese recommendations. E.g. click through rates on search results almost never\ngo past the first page of results.\n\n> As a result, the recommendation engine mostly gets feedback on re‐ sults that it\nalready knows about and gets very little feedback on results at the edge of\ncurrent knowledge. This limitation causes the recom‐ mendation engine to\nstagnate at or near the initial performance level. It does not continue to\nlearn.\n\n\nSource: p39 Practical Machine Learning: Innovations in Recommendation\n\nAnti-Flood\n> It is much better to avoid monotony in the user experience by pro‐ viding\ndiversity in recommendations with no more than a few of each kind of results.\n\n\n> ...our experience has been that it is much easier to ruin an otherwise good\nrecommendation engine than it is to get diverse results out of the engine while\nmaintaining overall quality... As a precaution, it is much easier to simply\nreorder the recommendations\n\n\nSource: p.40 Practical Machine Learning: Innovations in Recommendation\n\nEvaluate with A/B testing\nFurther Reading\n *  [https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3]\n *  [https://www.cs.uic.edu/~elena/pubs/kotsogiannis-wsdm17.pdf]","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-10-11 10:57:47","created_by":"1","updated_at":"2018-01-20 13:44:10","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd01","uuid":"97c69136-d57a-4007-a3a1-778ee5261cfe","title":"Conceptual Overview of Programming Paradigms","slug":"conceptual-overview-of-programming-paradigms","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"\\\"The functional programming paradigm was explicitly created to support a pure functional approach to problem solving. Functional programming is a form of declarative programming. \\\"\\n\\nhttps://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/functional-programming-vs-imperative-programming\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>&quot;The functional programming paradigm was explicitly created to support a pure functional approach to problem solving. Functional programming is a form of declarative programming. &quot;</p>\n<p><a href=\"https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/functional-programming-vs-imperative-programming\">https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/functional-programming-vs-imperative-programming</a></p>\n<!--kg-card-end: markdown-->","comment_id":"41","plaintext":"\"The functional programming paradigm was explicitly created to support a pure\nfunctional approach to problem solving. Functional programming is a form of\ndeclarative programming. \"\n\nhttps://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/functional-programming-vs-imperative-programming","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-10-17 10:34:58","created_by":"1","updated_at":"2017-10-18 09:05:13","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd02","uuid":"dab27782-b747-44b1-b3be-c4c71f2add3e","title":"How to Deploy Plotly's Dash using Shinyproxy","slug":"how-to-deploy-plotlys-dash-using-shinyproxy","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In a [previous post](/shiny-containers-with-shinyproxy/#addingadditionalnonshinyapps) I established that I could easily deploy a 'Hello World' [flask.py](http://flask.pocoo.org/) web application using [Shinyproxy](https://www.shinyproxy.io/). Therefore, I thought it would be straightforward to deploy a [Dash](https://plot.ly/dash/) app which is built on top of [flask.py](http://flask.pocoo.org/). However, it proved to be a little more difficult than that. This blog post runs through the errors and eventual solution to deploying a Dash app on Shinyproxy. \\n\\n##### The Issue #####\\nDash apps, like most web apps, load additional static resources, but upon deploying on Shinyproxy these aren't being delivered to the client (browser) as the URL path is wrong. This results in the all too familiar **404 Not Found**. *Disclaimer*: I haven't programmed in Java or SpringBoot (yet!). However, as far as I can see from looking at [Chrome dev tools](https://developer.chrome.com/devtools) there are two ways Shinyproxy delivers static files to the client. I'm taking a look first at how it works with a [Shiny](https://shiny.rstudio.com/) app.\\n\\n1.  **Webjars** - I can see that the [main template of shinyproxy](https://github.com/openanalytics/shinyproxy/blob/master/src/main/resources/templates/app.html) loads the bootstrap CSS which also gets utilised by each Shiny app.\\n\\n2. **Load from the container** - Additional static content for shiny apps is requested by using the container name, in this case `peaceful_jepsen` e.g.:\\n\\n```\\nRequest URL:http://<my-ip-address>/peaceful_jepsen/dt-core-1.10.12/css/jquery.dataTables.min.css\\n```\\n\\nNow let's move on to it not working with Dash...\\n\\n## Deploying a Dash App on Shinyproxy\\nUsing `react.min.js` as an example, I'm going to contrast how the app fetches static content on my local machine in a container vs Shinyproxy on a remote server. I'll do this by inspecting the path for the `GET` requests using [Chrome dev tools](https://developer.chrome.com/devtools). Firstly, what is the default behaviour of Dash.\\n\\n>By default, dash serves the JavaScript and CSS resources from the online CDNs.\\nSource: [Dash Docs](https://plot.ly/dash/external-resources)\\n\\n**Dash Container on my Local Machine**\\n![local-dash-cdn-get](/content/images/2017/11/local-dash-cdn-get.png)\\n\\nAll resources loaded and the application displayed correctly.\\n\\n**Dash Container on Shinyproxy**\\n![shinyproxy-dash-cdn-get](/content/images/2017/11/shinyproxy-dash-cdn-get.png)\\n\\nOn Shinyproxy `react.min.js` gets loaded however `_dash-layout` and `_dash-dependencies` fail to GET the necessary resources from the `Request URL: http://<my-ip-address>/_dash-layout`. As we know from the Shiny app, the container name should be in there e.g. `Request URL: http://<my-ip-address>/<container_name>/_dash-layout`. This is how Dash is supposed to behave, looking at [the source code](https://github.com/plotly/dash/blob/4ee769d3593d5297602c2bb6faca0eb63884d480/dash/dash.py#L74) we can see a configurable `routes_pathname_prefix` (I'll come back to this later in the post). So a possible solution is to get the Dash app to obtain the container name from which it is running.\\n\\nThe [Dash Docs](https://plot.ly/dash/external-resources) have the following lines of code to serve content locally.\\n\\n```python\\napp.css.config.serve_locally = True\\napp.scripts.config.serve_locally = True\\n```\\n\\nLet's see how that changes the behaviour.\\n\\n**Dash Container on my Local Machine**\\n![local-dash-serve-local-get](/content/images/2017/11/local-dash-serve-local-get.png)\\nSince all static content is now requested from Dash this is probably going to prevent all content from being loaded on Shinyproxy. \\n\\n**Dash Container on Shinyproxy**\\n![shinyproxy-dash-serve-local-get](/content/images/2017/11/shinyproxy-dash-serve-local-get.png)\\nYup! A whole lot of red 404s.\\n\\nEssentially it doesn't matter if you configure Dash to serve static files locally, the underlying issue is the relative URL route prefix. We know from the [Dash source code](https://github.com/plotly/dash/blob/master/dash/dash.py#L50) how we could prefix the URL route. However, Shinyproxy is already doing this for Shiny apps, what's different for the requests from Dash? \\n\\nI started scouring the Shinyproxy Java code base. I found the construction of the `containerPath` in [Shinyproxy's AppController.java](https://github.com/openanalytics/shinyproxy/blob/f934f108573f1ed1d24a719d9e0815012240e11f/src/main/java/eu/openanalytics/controllers/AppController.java#L74). But since I am not a Java developer I decided that this would probably not be the best allocation of my efforts. However, what the `containerPath` does show is the construction of the path begins with `'/'`.\\n\\nAnother approach I thought of was obtaining the container name from within the Dash application and appending that to the GET URL requests initiated Dash. Alas, Docker doesn't allow that. You can access the ContainerID but not the ContainerName. There is a [3 year old open issue regarding this](https://github.com/moby/moby/issues/8427). I then started looking for an easy way to pass into the container the containerName but I couldn't find a non-hacky way to do that either.\\n\\n**Sidenote** - I was wondering what the logic was on the container name generation e.g. `adorin_raman`. If you look at the docker source code you will find [some go-lang code called `names-generator.go`.](https://github.com/moby/moby/blob/master/pkg/namesgenerator/names-generator.go) This contains two lists, a list of adjectives and a list of \\\"notable scientists and hackers\\\". The function that randomly combines these into a container name formatted as \\\"adjective_surname\\\" has one funny exception:\\n\\n```\\nif name == \\\"boring_wozniak\\\" /* Steve Wozniak is not boring */ {\\n\\tgoto begin\\n}\\n```\\n\\nTwo other approaches that weren't very appealing would have been managing the resources as [webjars](https://www.webjars.org/) or an [Nginx](https://nginx.org/en/) redirect.\\n\\n## The Final Solution\\nI posted [a question to the shinyproxy forum](https://support.openanalytics.eu/t/what-is-the-best-way-of-delivering-static-assets-to-the-client-for-custom-apps/363/5), [Frederick](http://www.fcm-consulting.be/resume.html) one of the main authors responded with:\\n\\n>Then that means the href being used was `/_dash-layout` and not `_dash-layout`.\\nDo you control the hrefs? If so, removing the slash may solve your issue. If not, it becomes trickier… you could use javascript and extract the correct URL from window.location.\\n\\nIf we dig into the [dash.py code](https://github.com/plotly/dash/blob/master/dash/dash.py) We can see the construction of the URL paths for `_dash-layout` `_dash-dependencies` occurs on lines 73-80:\\n\\n```python\\nadd_url(\\n    '{}_dash-layout'.format(self.config['routes_pathname_prefix']),\\n    self.serve_layout)\\n\\nadd_url(\\n\\t'{}_dash-dependencies'.format(self.config['routes_pathname_prefix']), self.dependencies)\\n```\\n\\nThe next question is, where is `routes_pathname_prefix` defined? On lines 46-51 `routes_pathname_prefix` is mapped to `url_base_pathname`:\\n\\n```python\\nself.url_base_pathname = url_base_pathname\\nself.config = _AttributeDict({\\n    'suppress_callback_exceptions': False,\\n    'routes_pathname_prefix': url_base_pathname,\\n    'requests_pathname_prefix': url_base_pathname\\n})\\n```\\n\\nWhich begs the question, where is url `url_base_pathname` defined... on lines 20-28:\\n\\n```python\\nclass Dash(object):\\n    def __init__(\\n        self,\\n        name=None,\\n        server=None,\\n        static_folder=None,\\n        url_base_pathname='/',\\n        **kwargs\\n    ):\\n```\\n\\nAs Frederick said, the request should not have the `'/'` prefix in the GET request. Unsurprisingly, having `'//'` in your request paths is not good practice (see this [SO answer](https://stackoverflow.com/a/20524044/3691003) and [this one](https://stackoverflow.com/a/10161264/3691003)) as it can cause problems depending on how requests are handled. The [software layers of Shinyproxy](/shiny-containers-with-shinyproxy/#post-content) are reasonably complex with requests being handled my multiple technologies. The best solution after this trouble-shooting exercise is to use the following code in your Dash app.\\n\\n```python\\n# In order to work on shinyproxy (and perhaps other middleware)\\napp.config.supress_callback_exceptions = True\\napp.config.update({\\n    # remove the default of '/'\\n    'routes_pathname_prefix': '',\\n\\n    # remove the default of '/'\\n    'requests_pathname_prefix': ''\\n})\\n```\\nIf you've found this content helpful why not...\\n\\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\\n  <link href=\\\"https://fonts.googleapis.com/css?family=Cookie\\\" rel=\\\"stylesheet\\\"><a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>In a <a href=\"/shiny-containers-with-shinyproxy/#addingadditionalnonshinyapps\">previous post</a> I established that I could easily deploy a 'Hello World' <a href=\"http://flask.pocoo.org/\">flask.py</a> web application using <a href=\"https://www.shinyproxy.io/\">Shinyproxy</a>. Therefore, I thought it would be straightforward to deploy a <a href=\"https://plot.ly/dash/\">Dash</a> app which is built on top of <a href=\"http://flask.pocoo.org/\">flask.py</a>. However, it proved to be a little more difficult than that. This blog post runs through the errors and eventual solution to deploying a Dash app on Shinyproxy.</p>\n<h5 id=\"theissue\">The Issue</h5>\n<p>Dash apps, like most web apps, load additional static resources, but upon deploying on Shinyproxy these aren't being delivered to the client (browser) as the URL path is wrong. This results in the all too familiar <strong>404 Not Found</strong>. <em>Disclaimer</em>: I haven't programmed in Java or SpringBoot (yet!). However, as far as I can see from looking at <a href=\"https://developer.chrome.com/devtools\">Chrome dev tools</a> there are two ways Shinyproxy delivers static files to the client. I'm taking a look first at how it works with a <a href=\"https://shiny.rstudio.com/\">Shiny</a> app.</p>\n<ol>\n<li>\n<p><strong>Webjars</strong> - I can see that the <a href=\"https://github.com/openanalytics/shinyproxy/blob/master/src/main/resources/templates/app.html\">main template of shinyproxy</a> loads the bootstrap CSS which also gets utilised by each Shiny app.</p>\n</li>\n<li>\n<p><strong>Load from the container</strong> - Additional static content for shiny apps is requested by using the container name, in this case <code>peaceful_jepsen</code> e.g.:</p>\n</li>\n</ol>\n<pre><code>Request URL:http://&lt;my-ip-address&gt;/peaceful_jepsen/dt-core-1.10.12/css/jquery.dataTables.min.css\n</code></pre>\n<p>Now let's move on to it not working with Dash...</p>\n<h2 id=\"deployingadashapponshinyproxy\">Deploying a Dash App on Shinyproxy</h2>\n<p>Using <code>react.min.js</code> as an example, I'm going to contrast how the app fetches static content on my local machine in a container vs Shinyproxy on a remote server. I'll do this by inspecting the path for the <code>GET</code> requests using <a href=\"https://developer.chrome.com/devtools\">Chrome dev tools</a>. Firstly, what is the default behaviour of Dash.</p>\n<blockquote>\n<p>By default, dash serves the JavaScript and CSS resources from the online CDNs.<br>\nSource: <a href=\"https://plot.ly/dash/external-resources\">Dash Docs</a></p>\n</blockquote>\n<p><strong>Dash Container on my Local Machine</strong><br>\n<img src=\"/content/images/2017/11/local-dash-cdn-get.png\" alt=\"local-dash-cdn-get\"></p>\n<p>All resources loaded and the application displayed correctly.</p>\n<p><strong>Dash Container on Shinyproxy</strong><br>\n<img src=\"/content/images/2017/11/shinyproxy-dash-cdn-get.png\" alt=\"shinyproxy-dash-cdn-get\"></p>\n<p>On Shinyproxy <code>react.min.js</code> gets loaded however <code>_dash-layout</code> and <code>_dash-dependencies</code> fail to GET the necessary resources from the <code>Request URL: http://&lt;my-ip-address&gt;/_dash-layout</code>. As we know from the Shiny app, the container name should be in there e.g. <code>Request URL: http://&lt;my-ip-address&gt;/&lt;container_name&gt;/_dash-layout</code>. This is how Dash is supposed to behave, looking at <a href=\"https://github.com/plotly/dash/blob/4ee769d3593d5297602c2bb6faca0eb63884d480/dash/dash.py#L74\">the source code</a> we can see a configurable <code>routes_pathname_prefix</code> (I'll come back to this later in the post). So a possible solution is to get the Dash app to obtain the container name from which it is running.</p>\n<p>The <a href=\"https://plot.ly/dash/external-resources\">Dash Docs</a> have the following lines of code to serve content locally.</p>\n<pre><code class=\"language-python\">app.css.config.serve_locally = True\napp.scripts.config.serve_locally = True\n</code></pre>\n<p>Let's see how that changes the behaviour.</p>\n<p><strong>Dash Container on my Local Machine</strong><br>\n<img src=\"/content/images/2017/11/local-dash-serve-local-get.png\" alt=\"local-dash-serve-local-get\"><br>\nSince all static content is now requested from Dash this is probably going to prevent all content from being loaded on Shinyproxy.</p>\n<p><strong>Dash Container on Shinyproxy</strong><br>\n<img src=\"/content/images/2017/11/shinyproxy-dash-serve-local-get.png\" alt=\"shinyproxy-dash-serve-local-get\"><br>\nYup! A whole lot of red 404s.</p>\n<p>Essentially it doesn't matter if you configure Dash to serve static files locally, the underlying issue is the relative URL route prefix. We know from the <a href=\"https://github.com/plotly/dash/blob/master/dash/dash.py#L50\">Dash source code</a> how we could prefix the URL route. However, Shinyproxy is already doing this for Shiny apps, what's different for the requests from Dash?</p>\n<p>I started scouring the Shinyproxy Java code base. I found the construction of the <code>containerPath</code> in <a href=\"https://github.com/openanalytics/shinyproxy/blob/f934f108573f1ed1d24a719d9e0815012240e11f/src/main/java/eu/openanalytics/controllers/AppController.java#L74\">Shinyproxy's AppController.java</a>. But since I am not a Java developer I decided that this would probably not be the best allocation of my efforts. However, what the <code>containerPath</code> does show is the construction of the path begins with <code>'/'</code>.</p>\n<p>Another approach I thought of was obtaining the container name from within the Dash application and appending that to the GET URL requests initiated Dash. Alas, Docker doesn't allow that. You can access the ContainerID but not the ContainerName. There is a <a href=\"https://github.com/moby/moby/issues/8427\">3 year old open issue regarding this</a>. I then started looking for an easy way to pass into the container the containerName but I couldn't find a non-hacky way to do that either.</p>\n<p><strong>Sidenote</strong> - I was wondering what the logic was on the container name generation e.g. <code>adorin_raman</code>. If you look at the docker source code you will find <a href=\"https://github.com/moby/moby/blob/master/pkg/namesgenerator/names-generator.go\">some go-lang code called <code>names-generator.go</code>.</a> This contains two lists, a list of adjectives and a list of &quot;notable scientists and hackers&quot;. The function that randomly combines these into a container name formatted as &quot;adjective_surname&quot; has one funny exception:</p>\n<pre><code>if name == &quot;boring_wozniak&quot; /* Steve Wozniak is not boring */ {\n\tgoto begin\n}\n</code></pre>\n<p>Two other approaches that weren't very appealing would have been managing the resources as <a href=\"https://www.webjars.org/\">webjars</a> or an <a href=\"https://nginx.org/en/\">Nginx</a> redirect.</p>\n<h2 id=\"thefinalsolution\">The Final Solution</h2>\n<p>I posted <a href=\"https://support.openanalytics.eu/t/what-is-the-best-way-of-delivering-static-assets-to-the-client-for-custom-apps/363/5\">a question to the shinyproxy forum</a>, <a href=\"http://www.fcm-consulting.be/resume.html\">Frederick</a> one of the main authors responded with:</p>\n<blockquote>\n<p>Then that means the href being used was <code>/_dash-layout</code> and not <code>_dash-layout</code>.<br>\nDo you control the hrefs? If so, removing the slash may solve your issue. If not, it becomes trickier… you could use javascript and extract the correct URL from window.location.</p>\n</blockquote>\n<p>If we dig into the <a href=\"https://github.com/plotly/dash/blob/master/dash/dash.py\">dash.py code</a> We can see the construction of the URL paths for <code>_dash-layout</code> <code>_dash-dependencies</code> occurs on lines 73-80:</p>\n<pre><code class=\"language-python\">add_url(\n    '{}_dash-layout'.format(self.config['routes_pathname_prefix']),\n    self.serve_layout)\n\nadd_url(\n\t'{}_dash-dependencies'.format(self.config['routes_pathname_prefix']), self.dependencies)\n</code></pre>\n<p>The next question is, where is <code>routes_pathname_prefix</code> defined? On lines 46-51 <code>routes_pathname_prefix</code> is mapped to <code>url_base_pathname</code>:</p>\n<pre><code class=\"language-python\">self.url_base_pathname = url_base_pathname\nself.config = _AttributeDict({\n    'suppress_callback_exceptions': False,\n    'routes_pathname_prefix': url_base_pathname,\n    'requests_pathname_prefix': url_base_pathname\n})\n</code></pre>\n<p>Which begs the question, where is url <code>url_base_pathname</code> defined... on lines 20-28:</p>\n<pre><code class=\"language-python\">class Dash(object):\n    def __init__(\n        self,\n        name=None,\n        server=None,\n        static_folder=None,\n        url_base_pathname='/',\n        **kwargs\n    ):\n</code></pre>\n<p>As Frederick said, the request should not have the <code>'/'</code> prefix in the GET request. Unsurprisingly, having <code>'//'</code> in your request paths is not good practice (see this <a href=\"https://stackoverflow.com/a/20524044/3691003\">SO answer</a> and <a href=\"https://stackoverflow.com/a/10161264/3691003\">this one</a>) as it can cause problems depending on how requests are handled. The <a href=\"/shiny-containers-with-shinyproxy/#post-content\">software layers of Shinyproxy</a> are reasonably complex with requests being handled my multiple technologies. The best solution after this trouble-shooting exercise is to use the following code in your Dash app.</p>\n<pre><code class=\"language-python\"># In order to work on shinyproxy (and perhaps other middleware)\napp.config.supress_callback_exceptions = True\napp.config.update({\n    # remove the default of '/'\n    'routes_pathname_prefix': '',\n\n    # remove the default of '/'\n    'requests_pathname_prefix': ''\n})\n</code></pre>\n<p>If you've found this content helpful why not...</p>\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\n  <link href=\"https://fonts.googleapis.com/css?family=Cookie\" rel=\"stylesheet\"><a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a><!--kg-card-end: markdown-->","comment_id":"42","plaintext":"In a previous post\n[/shiny-containers-with-shinyproxy/#addingadditionalnonshinyapps] I established\nthat I could easily deploy a 'Hello World' flask.py [http://flask.pocoo.org/] \nweb application using Shinyproxy [https://www.shinyproxy.io/]. Therefore, I\nthought it would be straightforward to deploy a Dash [https://plot.ly/dash/] app\nwhich is built on top of flask.py [http://flask.pocoo.org/]. However, it proved\nto be a little more difficult than that. This blog post runs through the errors\nand eventual solution to deploying a Dash app on Shinyproxy.\n\nThe Issue\nDash apps, like most web apps, load additional static resources, but upon\ndeploying on Shinyproxy these aren't being delivered to the client (browser) as\nthe URL path is wrong. This results in the all too familiar 404 Not Found. \nDisclaimer: I haven't programmed in Java or SpringBoot (yet!). However, as far\nas I can see from looking at Chrome dev tools\n[https://developer.chrome.com/devtools] there are two ways Shinyproxy delivers\nstatic files to the client. I'm taking a look first at how it works with a Shiny\n[https://shiny.rstudio.com/] app.\n\n 1. Webjars - I can see that the main template of shinyproxy\n    [https://github.com/openanalytics/shinyproxy/blob/master/src/main/resources/templates/app.html] \n    loads the bootstrap CSS which also gets utilised by each Shiny app.\n    \n    \n 2. Load from the container - Additional static content for shiny apps is\n    requested by using the container name, in this case peaceful_jepsen e.g.:\n    \n    \n\nRequest URL:http://<my-ip-address>/peaceful_jepsen/dt-core-1.10.12/css/jquery.dataTables.min.css\n\n\nNow let's move on to it not working with Dash...\n\nDeploying a Dash App on Shinyproxy\nUsing react.min.js as an example, I'm going to contrast how the app fetches\nstatic content on my local machine in a container vs Shinyproxy on a remote\nserver. I'll do this by inspecting the path for the GET requests using Chrome\ndev tools [https://developer.chrome.com/devtools]. Firstly, what is the default\nbehaviour of Dash.\n\n> By default, dash serves the JavaScript and CSS resources from the online CDNs.\nSource: Dash Docs [https://plot.ly/dash/external-resources]\n\n\nDash Container on my Local Machine\n\n\nAll resources loaded and the application displayed correctly.\n\nDash Container on Shinyproxy\n\n\nOn Shinyproxy react.min.js gets loaded however _dash-layout and \n_dash-dependencies fail to GET the necessary resources from the Request URL:\nhttp://<my-ip-address>/_dash-layout. As we know from the Shiny app, the\ncontainer name should be in there e.g. Request URL:\nhttp://<my-ip-address>/<container_name>/_dash-layout. This is how Dash is\nsupposed to behave, looking at the source code\n[https://github.com/plotly/dash/blob/4ee769d3593d5297602c2bb6faca0eb63884d480/dash/dash.py#L74] \nwe can see a configurable routes_pathname_prefix (I'll come back to this later\nin the post). So a possible solution is to get the Dash app to obtain the\ncontainer name from which it is running.\n\nThe Dash Docs [https://plot.ly/dash/external-resources] have the following lines\nof code to serve content locally.\n\napp.css.config.serve_locally = True\napp.scripts.config.serve_locally = True\n\n\nLet's see how that changes the behaviour.\n\nDash Container on my Local Machine\n\nSince all static content is now requested from Dash this is probably going to\nprevent all content from being loaded on Shinyproxy.\n\nDash Container on Shinyproxy\n\nYup! A whole lot of red 404s.\n\nEssentially it doesn't matter if you configure Dash to serve static files\nlocally, the underlying issue is the relative URL route prefix. We know from the \nDash source code [https://github.com/plotly/dash/blob/master/dash/dash.py#L50] \nhow we could prefix the URL route. However, Shinyproxy is already doing this for\nShiny apps, what's different for the requests from Dash?\n\nI started scouring the Shinyproxy Java code base. I found the construction of\nthe containerPath in Shinyproxy's AppController.java\n[https://github.com/openanalytics/shinyproxy/blob/f934f108573f1ed1d24a719d9e0815012240e11f/src/main/java/eu/openanalytics/controllers/AppController.java#L74]\n. But since I am not a Java developer I decided that this would probably not be\nthe best allocation of my efforts. However, what the containerPath does show is\nthe construction of the path begins with '/'.\n\nAnother approach I thought of was obtaining the container name from within the\nDash application and appending that to the GET URL requests initiated Dash.\nAlas, Docker doesn't allow that. You can access the ContainerID but not the\nContainerName. There is a 3 year old open issue regarding this\n[https://github.com/moby/moby/issues/8427]. I then started looking for an easy\nway to pass into the container the containerName but I couldn't find a non-hacky\nway to do that either.\n\nSidenote - I was wondering what the logic was on the container name generation\ne.g. adorin_raman. If you look at the docker source code you will find some\ngo-lang code called names-generator.go.\n[https://github.com/moby/moby/blob/master/pkg/namesgenerator/names-generator.go] \nThis contains two lists, a list of adjectives and a list of \"notable scientists\nand hackers\". The function that randomly combines these into a container name\nformatted as \"adjective_surname\" has one funny exception:\n\nif name == \"boring_wozniak\" /* Steve Wozniak is not boring */ {\n\tgoto begin\n}\n\n\nTwo other approaches that weren't very appealing would have been managing the\nresources as webjars [https://www.webjars.org/] or an Nginx\n[https://nginx.org/en/] redirect.\n\nThe Final Solution\nI posted a question to the shinyproxy forum\n[https://support.openanalytics.eu/t/what-is-the-best-way-of-delivering-static-assets-to-the-client-for-custom-apps/363/5]\n, Frederick [http://www.fcm-consulting.be/resume.html] one of the main authors\nresponded with:\n\n> Then that means the href being used was /_dash-layout and not _dash-layout.\nDo you control the hrefs? If so, removing the slash may solve your issue. If\nnot, it becomes trickier… you could use javascript and extract the correct URL\nfrom window.location.\n\n\nIf we dig into the dash.py code\n[https://github.com/plotly/dash/blob/master/dash/dash.py] We can see the\nconstruction of the URL paths for _dash-layout _dash-dependencies occurs on\nlines 73-80:\n\nadd_url(\n    '{}_dash-layout'.format(self.config['routes_pathname_prefix']),\n    self.serve_layout)\n\nadd_url(\n\t'{}_dash-dependencies'.format(self.config['routes_pathname_prefix']), self.dependencies)\n\n\nThe next question is, where is routes_pathname_prefix defined? On lines 46-51 \nroutes_pathname_prefix is mapped to url_base_pathname:\n\nself.url_base_pathname = url_base_pathname\nself.config = _AttributeDict({\n    'suppress_callback_exceptions': False,\n    'routes_pathname_prefix': url_base_pathname,\n    'requests_pathname_prefix': url_base_pathname\n})\n\n\nWhich begs the question, where is url url_base_pathname defined... on lines\n20-28:\n\nclass Dash(object):\n    def __init__(\n        self,\n        name=None,\n        server=None,\n        static_folder=None,\n        url_base_pathname='/',\n        **kwargs\n    ):\n\n\nAs Frederick said, the request should not have the '/' prefix in the GET\nrequest. Unsurprisingly, having '//' in your request paths is not good practice\n(see this SO answer [https://stackoverflow.com/a/20524044/3691003] and this one\n[https://stackoverflow.com/a/10161264/3691003]) as it can cause problems\ndepending on how requests are handled. The software layers of Shinyproxy\n[/shiny-containers-with-shinyproxy/#post-content] are reasonably complex with\nrequests being handled my multiple technologies. The best solution after this\ntrouble-shooting exercise is to use the following code in your Dash app.\n\n# In order to work on shinyproxy (and perhaps other middleware)\napp.config.supress_callback_exceptions = True\napp.config.update({\n    # remove the default of '/'\n    'routes_pathname_prefix': '',\n\n    # remove the default of '/'\n    'requests_pathname_prefix': ''\n})\n\n\nIf you've found this content helpful why not...\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]","feature_image":"/content/images/2017/11/background-horses.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-10-31 12:50:34","created_by":"1","updated_at":"2019-08-04 13:16:05","updated_by":null,"published_at":"2017-11-14 13:44:50","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd03","uuid":"5c71ad97-db58-4490-85e1-bec2ec9b68a4","title":"Plato: The Apology of Socrates","slug":"plato-the-apology-of-socrates","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"http://www.sparknotes.com/philosophy/apology/section3.rhtml\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><a href=\"http://www.sparknotes.com/philosophy/apology/section3.rhtml\">http://www.sparknotes.com/philosophy/apology/section3.rhtml</a></p>\n<!--kg-card-end: markdown-->","comment_id":"43","plaintext":"http://www.sparknotes.com/philosophy/apology/section3.rhtml","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-11-04 15:32:27","created_by":"1","updated_at":"2017-11-06 09:19:48","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd04","uuid":"3752f172-8a11-458a-bbff-81f5d73784f3","title":"How to Make a Churn Model in R","slug":"how-to-make-a-churn-model-in-r","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The following post details how to make a churn model in R. It was part of an interview process for which a take home assignment was one of the stages. The company stated this should take 2hrs, which is entirely unrealistic. To minimise the time cost, my analysis is very succinct and short on the exploratory analysis and amount of models compared. The churn model got me to the final stage, however little in the way of feedback was offered. There is considerable debate in the tech industry as to whether take home exams are a fair assessment or even a reasonable ask on an individual. My assessment is that this is a lazy way to interview and a very high cost on the interviewee. If you really want to work for a particular company, then sure you might be prepared to do what it takes. I doubt I will do another take home assignment. \\n\\nThe analysis was done in [Rmarkdown](http://rmarkdown.rstudio.com/) and the output copied into this blog post following [these helpful pointers](https://notes.innovea.tech/how-to-write-latex-formula-in-ghosts-post/). If you would like to play with the data and full unabridged code please visit [this github repo](https://github.com/ucg8j/how-to-make-churn-model-in-R).\\n\\n## Aim\\n1. Please create a model that predicts which businesses are likely to churn at the start of 2015 based on the <code>vertical</code> and <code>incorporation_date</code>.\\n2. We have an inkling that a dropoff in the number of mandates added might be an advance indicator of someone churning. Please can you assess whether this might be true, and if you think it is useful, incorporate it into your model from part (1).\\n\\n```r\\n# Load packages\\nsuppressPackageStartupMessages({\\n    library(data.table)   # Fast I/O\\n    library(dplyr)        # Data munging\\n    library(tidyr)        # Data munging\\n    library(lubridate)    # Makes dates easy\\n    library(plotly)       # Interactive charts\\n    library(magrittr)     # pipe operators\\n    library(caret)        # Handy ML functions\\n    library(rpart)        # Decision Trees\\n    library(rpart.plot)   # Pretty tree plots\\n    library(ROCR)         # ML evaluation\\n    library(e1071)        # Misc stat fns\\n    library(randomForest) # rf\\n})\\n\\nset.seed(42)\\n\\n# Read data and drop row number column\\ndf <- fread(\\\"monthly_data_(2)_(2).csv\\\", drop = 1)\\n\\n# Have a glimpse of the data\\nglimpse(df)\\n```\\n\\n<pre><code>## Observations: 902\\n## Variables: 27\\n## $ company_id          &lt;int&gt; 4, 5, 6, 7, 8, 10, 11, 12, 13, 14...\\n## $ 2014-01-01_payments &lt;dbl&gt; 8, 0, 2, 3, 0, 0, 0, 0, 0, 0, 4, ...\\n## $ 2014-02-01_payments &lt;dbl&gt; 4, 0, 2, 3, 0, 0, 0, 2, 0, 11, 1,...\\n## $ 2014-03-01_payments &lt;dbl&gt; 7, 39, 1, 1, 6, 0, 0, 0, 8, 0, 1,...\\n## $ 2014-04-01_payments &lt;dbl&gt; 7, 0, 3, 7, 50, 1, 0, 0, 2, 0, 0,...\\n## $ 2014-05-01_payments &lt;dbl&gt; 1, 54, 1, 4, 119, 0, 1, 0, 0, 0, ...\\n## $ 2014-06-01_payments &lt;dbl&gt; 2, 0, 2, 1, 151, 3, 0, 0, 0, 0, 2...\\n## $ 2014-07-01_payments &lt;dbl&gt; 2, 0, 2, 7, 182, 0, 0, 0, 3, 0, 0...\\n## $ 2014-08-01_payments &lt;dbl&gt; 4, 22, 1, 2, 167, 0, 0, 0, 2, 0, ...\\n## $ 2014-09-01_payments &lt;dbl&gt; 3, 0, 1, 5, 180, 0, 0, 0, 0, 9, 5...\\n## $ 2014-10-01_payments &lt;dbl&gt; 5, 0, 2, 8, 157, 1, 0, 0, 0, 2, 1...\\n## $ 2014-11-01_payments &lt;dbl&gt; 5, 0, 1, 2, 105, 0, 0, 0, 0, 0, 0...\\n## $ 2014-12-01_payments &lt;dbl&gt; 9, 0, 3, 8, 57, 0, 0, 0, 0, 0, 0,...\\n## $ 2014-01-01_mandates &lt;dbl&gt; 0, 4, 0, 0, 0, 4, 4, 0, 0, 22, 0,...\\n## $ 2014-02-01_mandates &lt;dbl&gt; 0, 31, 1, 0, 2, 3, 8, 0, 0, 20, 1...\\n## $ 2014-03-01_mandates &lt;dbl&gt; 0, 24, 0, 0, 0, 5, 19, 0, 0, 11, ...\\n## $ 2014-04-01_mandates &lt;dbl&gt; 53, 18, 0, 1, 0, 0, 0, 0, 1, 11, ...\\n## $ 2014-05-01_mandates &lt;dbl&gt; 0, 8, 0, 0, 0, 0, 0, 0, 1, 15, 0,...\\n## $ 2014-06-01_mandates &lt;dbl&gt; 0, 7, 0, 0, 0, 0, 0, 0, 0, 13, 5,...\\n## $ 2014-07-01_mandates &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 16...\\n## $ 2014-08-01_mandates &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 8,...\\n## $ 2014-09-01_mandates &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 2,...\\n## $ 2014-10-01_mandates &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 16, 1,...\\n## $ 2014-11-01_mandates &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, ...\\n## $ 2014-12-01_mandates &lt;dbl&gt; 0, 0, 0, 3, 0, 0, 0, 0, 0, 11, 0,...\\n## $ vertical            &lt;chr&gt; &quot;gym/fitness&quot;, &quot;freelance d...\\n## $ incorporation_date  &lt;chr&gt; &quot;2003-09-25&quot;, &quot;2008-10-22&quot;, ...</code></pre>\\n\\n<h2>Data Munging</h2>\\n<p>Some data munging needs to occur for our binary classifiers to make use of the data within. This includes handling dates.</p>\\n\\n```\\n# Reshape data and create new columns\\ndf %<>%\\n    gather(key = date, value = quantity, starts_with(\\\"20\\\")) %>%\\n    separate(date, c(\\\"date\\\",\\\"paymentMandate\\\"), \\\"_\\\") %>%\\n    spread(paymentMandate, quantity) %>%\\n    mutate(incorporation_date = as.Date(incorporation_date),\\n           date = as.Date(date),\\n           incorporation_time = round(as.numeric(difftime(as.Date(\\\"2014-12-01\\\"), \\n                                                    as.Date(incorporation_date), \\n                                                    unit=\\\"weeks\\\")) / 52.25,\\n                                digits = 1)) %>%\\n    arrange(date)\\n\\n# What does the new data look like?\\nglimpse(df)\\n```\\n\\n<pre><code style=\\\"white-space: pre;\\\">## Observations: 10,824\\n## Variables: 7\\n## $ company_id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14,...\\n## $ vertical           &lt;chr&gt; &quot;gym/fitness&quot;, &quot;gym/fitness&quot;, &quot;freelance de...\\n## $ incorporation_date &lt;date&gt; 2013-05-30, 2003-09-25, 2008-10-22, 2005-0...\\n## $ date               &lt;date&gt; 2014-01-01, 2014-01-01, 2014-01-01, 2014-0...\\n## $ mandates           &lt;dbl&gt; 1, 0, 0, 0, 4, 0, 0, 0, 4, 4, 0, 0, 22, 0, ...\\n## $ payments           &lt;dbl&gt; 0, 1, 6, 8, 0, 2, 3, 0, 0, 0, 0, 0, 0, 4, 4...\\n## $ incorporation_time &lt;dbl&gt; 1.5, 11.2, 6.1, 9.4, 1.2, 9.1, 4.0, 2.9, 1....</code></pre>\\n\\n<h3>What is Churn</h3>\\n<p>For the purposes of this project I have defined ‘churn’ as zero mandates and payments for the first instance of that after the last mandate/payment made.</p>\\n\\n<pre class=\\\"r\\\" ><code># Create binary 'churn' column\\ndf$churn &lt;- 0\\n\\n# For use in for loop - upper bound of data\\nmax.date &lt;- max(df$date)\\n\\n# Mark all companies as churned in the month immediately their last activity\\nfor (company in unique(df$company_id)) {\\n\\n    # Subset data to company\\n    df.sub &lt;- subset(df, df$company_id == company)\\n    \\n    # Index of last positive mandate OR payment\\n    last.pos.idx &lt;- tail(which(df.sub$mandates != 0 | df.sub$payments != 0), 1)\\n    \\n    # Get date of last activity\\n    last.activity.date &lt;- df.sub$date[last.pos.idx]\\n    \\n    # If less than max.date of dataset mark churn ELSE do nothing i.e. positive at end of period\\n    if (last.activity.date &lt; max.date) {\\n        \\n        # Get churn date (last positive month plus 1mth)\\n        churn.date &lt;- last.activity.date %m+% months(1)\\n        \\n        # Mark month of churn as 1\\n        df[df$date == churn.date &amp; df$company_id == company, ]$churn &lt;- 1\\n    }\\n}\\n\\n# Multiple rows per company, filter for last month or churn month values...\\n# Get churners\\ndf %&gt;% filter(churn == 1) -&gt; churners\\n\\n# Get max date row of remainers (non-churners)\\ndf %&gt;% \\n    filter(churn == 0 &amp; !(company_id %in% churners$company_id) &amp; date == max(date)) -&gt; remainers\\n\\n# Combine and variables coded ready for modelling\\nchurners %&gt;% \\n    rbind(remainers) %&gt;%\\n    mutate(vertical = as.factor(vertical),\\n           churn    = as.factor(churn)) -&gt; model.df</code></pre>\\n\\n<h3>Churns over the year</h3>\\n<pre class=\\\"r\\\"><code># Plot churners\\nmodel.df %&gt;%\\n    filter(churn == 1) %&gt;%\\n    group_by(date) %&gt;%\\n    summarise(n = n()) %&gt;%\\n    plot_ly( x = ~date, y = ~n, type = 'scatter', mode = 'lines')</code></pre>\\n    \\n![](/content/images/2017/11/churns-over-year.png)\\n\\n<p>Could do more on exploratory but this is the easiest to cut back for this report.</p>\\n\\n<h3>Balance of the data</h3>\\n<p>The proportion of churn is 32.04%. Representing an imbalanced dataset. Accuracy is an inappropriate measure (I could get 67.96% accuracy predicting no businesses leave), so I will focus on recall and accuracy.</p>\\n<pre class=\\\"r\\\"><code># Loyal vs Churn\\ntable(model.df$churn)\\n## \\n##   0   1 \\n## 613 289</code></pre>\\n\\n<h2>Model</h2>\\n<p>Survival models and binary classifiers are common approaches to ‘Churn’ models. I will approach the model using the latter, though if I had more time I would investigate other classifiers and a survival model. I will limit the range of models to a logistic, a decision tree and an ensemble.</p>\\n\\n<h3>Split Data</h3>\\n\\n<pre class=\\\"r\\\"><code style=\\\"white-space: pre;\\\"># 80/20 train test split\\nindex    &lt;- createDataPartition(model.df$churn, p = 0.8, list = FALSE)\\ntrain.df &lt;- model.df[index, ]\\ntest.df  &lt;- model.df[-index, ]\\n\\n# Check balance of the training split\\ntable(train.df$churn)\\n## \\n##   0   1 \\n## 491 232</code></pre>\\n<pre class=\\\"r\\\"><code># Check balance of the test split\\ntable(test.df$churn)\\n## \\n##   0   1 \\n## 122  57</code></pre>\\n\\n\\n<h3>Logistic</h3>\\n\\n<pre class=\\\"r\\\"><code style=\\\"white-space: pre;\\\"># Run model\\nLogistic.model &lt;- glm(churn ~ incorporation_time + vertical, \\n                      data   = train.df, \\n                      family = binomial(link = 'logit'))\\n\\n# Predict\\nlog.pred &lt;- predict(Logistic.model, newdata = test.df, type = 'response')\\n\\n# Convert probs to binary\\nlog.pred &lt;- as.factor(ifelse(log.pred &gt; 0.5, 1, 0))\\n\\n# Evaluation Metrics\\nlog.result    &lt;- confusionMatrix(data = log.pred, test.df$churn)\\nlog.precision &lt;- log.result$byClass['Pos Pred Value']    \\nlog.recall    &lt;- log.result$byClass['Sensitivity']\\nlog.F1        &lt;- log.result$byClass['F1']</code></pre>\\n\\n\\n<h3>Decision Tree</h3>\\n\\n<pre class=\\\"r\\\"><code># Train model\\ntree.model &lt;- rpart(churn ~ incorporation_time + vertical,\\n                    data = train.df,\\n                    method = &quot;class&quot;,\\n                    control = rpart.control(xval = 10))\\n\\n# Plot\\nrpart.plot(tree.model)</code></pre>\\n\\n![](/content/images/2017/11/decision-tree.png)\\n\\n\\n<pre class=\\\"r\\\"><code style=\\\"white-space: pre;\\\"># Evaluation metrics\\ntree.pred      &lt;- predict(tree.model, newdata = test.df, type = &quot;class&quot;)\\ntree.result    &lt;- confusionMatrix(data = tree.pred, test.df$churn)\\ntree.precision &lt;- tree.result$byClass['Pos Pred Value']    \\ntree.recall    &lt;- tree.result$byClass['Sensitivity']\\ntree.F1        &lt;- tree.result$byClass['F1']</code></pre>\\n\\n\\n<h3>Random Forest (Ensemble)</h3>\\n\\n<pre class=\\\"r\\\"><code># Train model\\nforest.model &lt;- randomForest(churn ~ incorporation_time + vertical, \\n                       data = train.df, \\n                       ntree=200, \\n                       type=&quot;classification&quot;)\\n\\n# See error reduction with number of trees ( not much gained beyond ~25 trees)\\nplot(forest.model)</code></pre>\\n\\n![](/content/images/2017/11/random-forest-error-reduction.png)\\n\\n<pre class=\\\"r\\\"><code># Look at the variable Importance from the random forest\\nvarImpPlot(forest.model, sort = T, main=&quot;Variable Importance&quot;)</code></pre>\\n\\n![](/content/images/2017/11/random-forest-variable-importance.png)\\n\\n<pre class=\\\"r\\\"><code style=\\\"white-space: pre;\\\"># Evaluation metrics\\nforest.pred      &lt;- predict(forest.model, newdata = test.df, type = &quot;class&quot;)\\nforest.result    &lt;- confusionMatrix(data = forest.pred, test.df$churn)\\nforest.precision &lt;- forest.result$byClass['Pos Pred Value']    \\nforest.recall    &lt;- forest.result$byClass['Sensitivity']\\nforest.F1        &lt;- forest.result$byClass['F1']</code></pre>\\n\\n\\n<h3>Evaluation Metrics</h3>\\n<pre class=\\\"r\\\"><code>log.precision; \\n## Pos Pred Value \\n##      0.8333333\\ntree.precision; \\n## Pos Pred Value \\n##      0.8088235\\nforest.precision;\\n## Pos Pred Value \\n##      0.8134328</code></pre>\\n\\n<pre class=\\\"r\\\"><code>log.recall; \\n## Sensitivity \\n##   0.9016393\\ntree.recall;\\n## Sensitivity \\n##   0.9016393\\nforest.recall;\\n## Sensitivity \\n##   0.8934426</code></pre>\\n\\n<pre class=\\\"r\\\"><code>log.F1;\\n##        F1 \\n## 0.8661417\\ntree.F1;\\n##        F1 \\n## 0.8527132\\nforest.F1;\\n##        F1 \\n## 0.8515625</code></pre>\\n\\n<p>Surprisingly, the logistic regression model performs the best, with the top precision score and equal recall score with that of the decision tree. With more time, I’d see if tweaks to the decision tree and random forest models could change this. Its also possible over/undersampling could help. From here on I will use the logistic regression model.</p>\\n\\n\\n<h2>Examine the incorporation of time information</h2>\\n<p>Per the second aim, examine the inclusion of time information. This requires more data munging. Taking code used from above, I will extend to include the derivation of a very basic time period variable.</p>\\n\\n<pre class=\\\"r\\\"><code># Create binary 'leading_indicator' column\\nmodel.df$leading_indicator &lt;- 0\\n\\n# Min date for which a leading_indicator can be calculated (lower limit of data)\\nmin.date &lt;- min(df$date)\\n\\n# If month before 'churn' (churn-1), is below the level of mandates of the month 2 months prior (churn-2) then\\n# make leading_indicator == 1\\nfor (company in churners$company_id) {\\n\\n    # Subset data to company\\n    df.sub &lt;- subset(df, df$company_id == company)\\n    \\n    # Get month prior to churn\\n    month.prior &lt;- df.sub$date[df.sub$churn == 1] %m-% months(1)\\n\\n    # Get two months prior to churn\\n    two.month.prior &lt;- df.sub$date[df.sub$churn == 1] %m-% months(2)\\n\\n    # If two months prior is within dataset date range and level of mandates is greater than 0\\n    if ((two.month.prior &gt; min.date) &amp;&amp; (df.sub$mandates[df.sub$date == two.month.prior] &gt; 0)) {\\n        \\n        # Compare number of mandates 1 month prior to 2 months prior, if less, mark 'leading_indicator' as '1' \\n        if (df.sub$mandates[df.sub$date == month.prior] &lt; df.sub$mandates[df.sub$date == two.month.prior]) {\\n            model.df[model.df$company_id == company, ]$leading_indicator &lt;- 1\\n        }\\n    }\\n}</code></pre>\\n\\n\\n<h3>Of the churners, how many have a leading indicator?</h3>\\n<pre class=\\\"r\\\"><code>model.df %&gt;% \\n    filter(churn == 1) %&gt;%\\n    group_by(leading_indicator) %&gt;%\\n    summarise(n=n()) %&gt;%\\n    mutate(percent = round(n / sum(n) * 100, 1))</code></pre>\\n<pre><code>## # A tibble: 2 x 3\\n##   leading_indicator     n percent\\n##               &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\\n## 1                 0   264    91.3\\n## 2                 1    25     8.7</code></pre>\\n\\n\\n<h3>Re-train model and evaluate</h3>\\n<pre class=\\\"r\\\"><code style=\\\"white-space: pre;\\\"># Re-split data so 'leading_indicator' is in columns (the index remains the same)\\ntrain.df &lt;- model.df[index, ]\\ntest.df  &lt;- model.df[-index, ]</code></pre>\\n\\n<h3>Logistic</h3>\\n\\n<pre class=\\\"r\\\"><code style=\\\"white-space: pre;\\\"># Run model\\nlead.logistic.model &lt;- glm(churn ~ incorporation_time + vertical + leading_indicator, \\n                      data   = train.df, \\n                      family = binomial(link = 'logit'))\\n\\n# Predict\\nlog.pred &lt;- predict(lead.logistic.model, newdata = test.df, type = 'response')\\n\\n# Convert probs to binary\\nlog.pred &lt;- as.factor(ifelse(log.pred &gt; 0.5, 1, 0))\\n\\n# Evaluation Metrics\\nlead.log.result    &lt;- confusionMatrix(data = log.pred, test.df$churn)\\nlead.log.precision &lt;- log.result$byClass['Pos Pred Value']\\nlead.log.recall    &lt;- log.result$byClass['Sensitivity']\\nlead.log.F1        &lt;- log.result$byClass['F1']</code></pre>\\n\\n\\n\\n<h3>Evaluation Metrics</h3>\\n<p>Compare the precision and recall of the logistic model with and without the <code>lead_indicator</code>.</p>\\n<pre class=\\\"r\\\"><code>log.precision\\n## Pos Pred Value \\n##      0.8333333</code></pre>\\n\\n<pre class=\\\"r\\\"><code>lead.log.precision\\n## Pos Pred Value \\n##      0.8333333</code></pre>\\n\\n<pre class=\\\"r\\\"><code>log.recall\\n## Sensitivity \\n##   0.9016393</code></pre>\\n<pre class=\\\"r\\\"><code>lead.log.recall\\n## Sensitivity \\n##   0.9016393</code></pre>\\n\\n<pre><code style=\\\"white-space: pre;\\\"># Have a look at the model coefficients and p-values\\nsummary(lead.logistic.model)\\n\\n## Call:\\n## glm(formula = churn ~ incorporation_time + vertical + leading_indicator, \\n##     family = binomial(link = &quot;logit&quot;), data = train.df)\\n## \\n## Deviance Residuals: \\n##     Min       1Q   Median       3Q      Max  \\n## -1.4429  -0.8252  -0.4957   0.9938   2.5564  \\n## \\n## Coefficients:\\n##                              Estimate Std. Error z value Pr(&gt;|z|)    \\n## (Intercept)                   0.74066    0.20007   3.702 0.000214 ***\\n## incorporation_time           -0.26140    0.03045  -8.584  &lt; 2e-16 ***\\n## verticalfreelance developer   0.12614    0.21737   0.580 0.561714    \\n## verticalgym/fitness          -0.91099    0.22342  -4.077 4.55e-05 ***\\n## leading_indicator            18.06219  482.97671   0.037 0.970168    \\n## ---\\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\\n## \\n## (Dispersion parameter for binomial family taken to be 1)\\n## \\n##     Null deviance: 907.42  on 722  degrees of freedom\\n## Residual deviance: 736.18  on 718  degrees of freedom\\n## AIC: 746.18\\n## \\n## Number of Fisher Scoring iterations: 15</code></pre>\\n<p>Generally, it is best to minimise the <code>AIC</code>. The <code>Logistic.model</code> model AIC is 800.5 compared with the <code>lead.logistic.model</code> incorporating the <code>lead_indicator</code> is 746.18. The <code>lead_indicator</code> has not changed the predictive power on this dataset, but since the <code>AIC</code> favours a better model fit whilst penalising for additional predictors, the model to choose is the <code>lead.logistic.model</code>.</p>\\n\\n\\n<h4>Re-train model and save</h4>\\n<pre class=\\\"r\\\"><code style=\\\"white-space: pre;\\\"># Re-train on all data\\nfinal.model &lt;- glm(churn ~ incorporation_time + vertical + leading_indicator, \\n                      data   = model.df, \\n                      family = binomial(link = 'logit'))\\n\\nsave(final.model, file = &quot;model.rda&quot;)</code></pre>\\n\\nIf you've found this content helpful why not...<br>\\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\\n  <link href=\\\"https://fonts.googleapis.com/css?family=Cookie\\\" rel=\\\"stylesheet\\\"><a class=\\\"bmc-button\\\" target=\\\"_blank\\\" href=\\\"https://www.buymeacoffee.com/6uRXFwMJD\\\">\\n  <span style=\\\"margin-left:5px\\\">buy me a coffee</span></a>\\n<br>\\n\\n<h2>Recommendations for further investigation/Comments</h2>\\n<ul>\\n<li>The <code>lead_indicator</code> was a quick and dirty test, I’d spend more time looking at a better construction (e.g. moving avg)</li>\\n<li>If the cost of rentention activities vs losing a customer was known then an the optimal trade-off in terms of business cost could be found.</li>\\n<li>Incorporating additional data, e.g. CRM data showing interactions. Potentially assessing sales/account staff.</li>\\n<li>If it were a production model prompting staff to do retention calls, evaluate the impact of such calls through modelling e.g. a/b testing</li>\\n<li>Test model performance with a change to balancing the classes e.g. under/oversampling, boostrap samples. This may explain the relative underperformance of tree based models in this exercise.</li>\\n<li>Try other binary classifier models.</li>\\n</ul>\\n\"}]],\"markups\":[],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>The following post details how to make a churn model in R. It was part of an interview process for which a take home assignment was one of the stages. The company stated this should take 2hrs, which is entirely unrealistic. To minimise the time cost, my analysis is very succinct and short on the exploratory analysis and amount of models compared. The churn model got me to the final stage, however little in the way of feedback was offered. There is considerable debate in the tech industry as to whether take home exams are a fair assessment or even a reasonable ask on an individual. My assessment is that this is a lazy way to interview and a very high cost on the interviewee. If you really want to work for a particular company, then sure you might be prepared to do what it takes. I doubt I will do another take home assignment.</p>\n<p>The analysis was done in <a href=\"http://rmarkdown.rstudio.com/\">Rmarkdown</a> and the output copied into this blog post following <a href=\"https://notes.innovea.tech/how-to-write-latex-formula-in-ghosts-post/\">these helpful pointers</a>. If you would like to play with the data and full unabridged code please visit <a href=\"https://github.com/ucg8j/how-to-make-churn-model-in-R\">this github repo</a>.</p>\n<h2 id=\"aim\">Aim</h2>\n<ol>\n<li>Please create a model that predicts which businesses are likely to churn at the start of 2015 based on the <code>vertical</code> and <code>incorporation_date</code>.</li>\n<li>We have an inkling that a dropoff in the number of mandates added might be an advance indicator of someone churning. Please can you assess whether this might be true, and if you think it is useful, incorporate it into your model from part (1).</li>\n</ol>\n<pre><code class=\"language-r\"># Load packages\nsuppressPackageStartupMessages({\n    library(data.table)   # Fast I/O\n    library(dplyr)        # Data munging\n    library(tidyr)        # Data munging\n    library(lubridate)    # Makes dates easy\n    library(plotly)       # Interactive charts\n    library(magrittr)     # pipe operators\n    library(caret)        # Handy ML functions\n    library(rpart)        # Decision Trees\n    library(rpart.plot)   # Pretty tree plots\n    library(ROCR)         # ML evaluation\n    library(e1071)        # Misc stat fns\n    library(randomForest) # rf\n})\n\nset.seed(42)\n\n# Read data and drop row number column\ndf &lt;- fread(&quot;monthly_data_(2)_(2).csv&quot;, drop = 1)\n\n# Have a glimpse of the data\nglimpse(df)\n</code></pre>\n<pre><code>## Observations: 902\n## Variables: 27\n## $ company_id          &lt;int&gt; 4, 5, 6, 7, 8, 10, 11, 12, 13, 14...\n## $ 2014-01-01_payments &lt;dbl&gt; 8, 0, 2, 3, 0, 0, 0, 0, 0, 0, 4, ...\n## $ 2014-02-01_payments &lt;dbl&gt; 4, 0, 2, 3, 0, 0, 0, 2, 0, 11, 1,...\n## $ 2014-03-01_payments &lt;dbl&gt; 7, 39, 1, 1, 6, 0, 0, 0, 8, 0, 1,...\n## $ 2014-04-01_payments &lt;dbl&gt; 7, 0, 3, 7, 50, 1, 0, 0, 2, 0, 0,...\n## $ 2014-05-01_payments &lt;dbl&gt; 1, 54, 1, 4, 119, 0, 1, 0, 0, 0, ...\n## $ 2014-06-01_payments &lt;dbl&gt; 2, 0, 2, 1, 151, 3, 0, 0, 0, 0, 2...\n## $ 2014-07-01_payments &lt;dbl&gt; 2, 0, 2, 7, 182, 0, 0, 0, 3, 0, 0...\n## $ 2014-08-01_payments &lt;dbl&gt; 4, 22, 1, 2, 167, 0, 0, 0, 2, 0, ...\n## $ 2014-09-01_payments &lt;dbl&gt; 3, 0, 1, 5, 180, 0, 0, 0, 0, 9, 5...\n## $ 2014-10-01_payments &lt;dbl&gt; 5, 0, 2, 8, 157, 1, 0, 0, 0, 2, 1...\n## $ 2014-11-01_payments &lt;dbl&gt; 5, 0, 1, 2, 105, 0, 0, 0, 0, 0, 0...\n## $ 2014-12-01_payments &lt;dbl&gt; 9, 0, 3, 8, 57, 0, 0, 0, 0, 0, 0,...\n## $ 2014-01-01_mandates &lt;dbl&gt; 0, 4, 0, 0, 0, 4, 4, 0, 0, 22, 0,...\n## $ 2014-02-01_mandates &lt;dbl&gt; 0, 31, 1, 0, 2, 3, 8, 0, 0, 20, 1...\n## $ 2014-03-01_mandates &lt;dbl&gt; 0, 24, 0, 0, 0, 5, 19, 0, 0, 11, ...\n## $ 2014-04-01_mandates &lt;dbl&gt; 53, 18, 0, 1, 0, 0, 0, 0, 1, 11, ...\n## $ 2014-05-01_mandates &lt;dbl&gt; 0, 8, 0, 0, 0, 0, 0, 0, 1, 15, 0,...\n## $ 2014-06-01_mandates &lt;dbl&gt; 0, 7, 0, 0, 0, 0, 0, 0, 0, 13, 5,...\n## $ 2014-07-01_mandates &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 16...\n## $ 2014-08-01_mandates &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 8,...\n## $ 2014-09-01_mandates &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 2,...\n## $ 2014-10-01_mandates &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 16, 1,...\n## $ 2014-11-01_mandates &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, ...\n## $ 2014-12-01_mandates &lt;dbl&gt; 0, 0, 0, 3, 0, 0, 0, 0, 0, 11, 0,...\n## $ vertical            &lt;chr&gt; &quot;gym/fitness&quot;, &quot;freelance d...\n## $ incorporation_date  &lt;chr&gt; &quot;2003-09-25&quot;, &quot;2008-10-22&quot;, ...</code></pre>\n<h2>Data Munging</h2>\n<p>Some data munging needs to occur for our binary classifiers to make use of the data within. This includes handling dates.</p>\n<pre><code># Reshape data and create new columns\ndf %&lt;&gt;%\n    gather(key = date, value = quantity, starts_with(&quot;20&quot;)) %&gt;%\n    separate(date, c(&quot;date&quot;,&quot;paymentMandate&quot;), &quot;_&quot;) %&gt;%\n    spread(paymentMandate, quantity) %&gt;%\n    mutate(incorporation_date = as.Date(incorporation_date),\n           date = as.Date(date),\n           incorporation_time = round(as.numeric(difftime(as.Date(&quot;2014-12-01&quot;), \n                                                    as.Date(incorporation_date), \n                                                    unit=&quot;weeks&quot;)) / 52.25,\n                                digits = 1)) %&gt;%\n    arrange(date)\n\n# What does the new data look like?\nglimpse(df)\n</code></pre>\n<pre><code style=\"white-space: pre;\">## Observations: 10,824\n## Variables: 7\n## $ company_id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14,...\n## $ vertical           &lt;chr&gt; &quot;gym/fitness&quot;, &quot;gym/fitness&quot;, &quot;freelance de...\n## $ incorporation_date &lt;date&gt; 2013-05-30, 2003-09-25, 2008-10-22, 2005-0...\n## $ date               &lt;date&gt; 2014-01-01, 2014-01-01, 2014-01-01, 2014-0...\n## $ mandates           &lt;dbl&gt; 1, 0, 0, 0, 4, 0, 0, 0, 4, 4, 0, 0, 22, 0, ...\n## $ payments           &lt;dbl&gt; 0, 1, 6, 8, 0, 2, 3, 0, 0, 0, 0, 0, 0, 4, 4...\n## $ incorporation_time &lt;dbl&gt; 1.5, 11.2, 6.1, 9.4, 1.2, 9.1, 4.0, 2.9, 1....</code></pre>\n<h3>What is Churn</h3>\n<p>For the purposes of this project I have defined ‘churn’ as zero mandates and payments for the first instance of that after the last mandate/payment made.</p>\n<pre class=\"r\" ><code># Create binary 'churn' column\ndf$churn &lt;- 0\n\n# For use in for loop - upper bound of data\nmax.date &lt;- max(df$date)\n\n# Mark all companies as churned in the month immediately their last activity\nfor (company in unique(df$company_id)) {\n\n    # Subset data to company\n    df.sub &lt;- subset(df, df$company_id == company)\n    \n    # Index of last positive mandate OR payment\n    last.pos.idx &lt;- tail(which(df.sub$mandates != 0 | df.sub$payments != 0), 1)\n    \n    # Get date of last activity\n    last.activity.date &lt;- df.sub$date[last.pos.idx]\n    \n    # If less than max.date of dataset mark churn ELSE do nothing i.e. positive at end of period\n    if (last.activity.date &lt; max.date) {\n        \n        # Get churn date (last positive month plus 1mth)\n        churn.date &lt;- last.activity.date %m+% months(1)\n        \n        # Mark month of churn as 1\n        df[df$date == churn.date &amp; df$company_id == company, ]$churn &lt;- 1\n    }\n}\n\n# Multiple rows per company, filter for last month or churn month values...\n# Get churners\ndf %&gt;% filter(churn == 1) -&gt; churners\n\n# Get max date row of remainers (non-churners)\ndf %&gt;% \n    filter(churn == 0 &amp; !(company_id %in% churners$company_id) &amp; date == max(date)) -&gt; remainers\n\n# Combine and variables coded ready for modelling\nchurners %&gt;% \n    rbind(remainers) %&gt;%\n    mutate(vertical = as.factor(vertical),\n           churn    = as.factor(churn)) -&gt; model.df</code></pre>\n<h3>Churns over the year</h3>\n<pre class=\"r\"><code># Plot churners\nmodel.df %&gt;%\n    filter(churn == 1) %&gt;%\n    group_by(date) %&gt;%\n    summarise(n = n()) %&gt;%\n    plot_ly( x = ~date, y = ~n, type = 'scatter', mode = 'lines')</code></pre>\n<p><img src=\"/content/images/2017/11/churns-over-year.png\" alt=\"\"></p>\n<p>Could do more on exploratory but this is the easiest to cut back for this report.</p>\n<h3>Balance of the data</h3>\n<p>The proportion of churn is 32.04%. Representing an imbalanced dataset. Accuracy is an inappropriate measure (I could get 67.96% accuracy predicting no businesses leave), so I will focus on recall and accuracy.</p>\n<pre class=\"r\"><code># Loyal vs Churn\ntable(model.df$churn)\n## \n##   0   1 \n## 613 289</code></pre>\n<h2>Model</h2>\n<p>Survival models and binary classifiers are common approaches to ‘Churn’ models. I will approach the model using the latter, though if I had more time I would investigate other classifiers and a survival model. I will limit the range of models to a logistic, a decision tree and an ensemble.</p>\n<h3>Split Data</h3>\n<pre class=\"r\"><code style=\"white-space: pre;\"># 80/20 train test split\nindex    &lt;- createDataPartition(model.df$churn, p = 0.8, list = FALSE)\ntrain.df &lt;- model.df[index, ]\ntest.df  &lt;- model.df[-index, ]\n\n# Check balance of the training split\ntable(train.df$churn)\n## \n##   0   1 \n## 491 232</code></pre>\n<pre class=\"r\"><code># Check balance of the test split\ntable(test.df$churn)\n## \n##   0   1 \n## 122  57</code></pre>\n<h3>Logistic</h3>\n<pre class=\"r\"><code style=\"white-space: pre;\"># Run model\nLogistic.model &lt;- glm(churn ~ incorporation_time + vertical, \n                      data   = train.df, \n                      family = binomial(link = 'logit'))\n\n# Predict\nlog.pred &lt;- predict(Logistic.model, newdata = test.df, type = 'response')\n\n# Convert probs to binary\nlog.pred &lt;- as.factor(ifelse(log.pred &gt; 0.5, 1, 0))\n\n# Evaluation Metrics\nlog.result    &lt;- confusionMatrix(data = log.pred, test.df$churn)\nlog.precision &lt;- log.result$byClass['Pos Pred Value']    \nlog.recall    &lt;- log.result$byClass['Sensitivity']\nlog.F1        &lt;- log.result$byClass['F1']</code></pre>\n<h3>Decision Tree</h3>\n<pre class=\"r\"><code># Train model\ntree.model &lt;- rpart(churn ~ incorporation_time + vertical,\n                    data = train.df,\n                    method = &quot;class&quot;,\n                    control = rpart.control(xval = 10))\n\n# Plot\nrpart.plot(tree.model)</code></pre>\n<p><img src=\"/content/images/2017/11/decision-tree.png\" alt=\"\"></p>\n<pre class=\"r\"><code style=\"white-space: pre;\"># Evaluation metrics\ntree.pred      &lt;- predict(tree.model, newdata = test.df, type = &quot;class&quot;)\ntree.result    &lt;- confusionMatrix(data = tree.pred, test.df$churn)\ntree.precision &lt;- tree.result$byClass['Pos Pred Value']    \ntree.recall    &lt;- tree.result$byClass['Sensitivity']\ntree.F1        &lt;- tree.result$byClass['F1']</code></pre>\n<h3>Random Forest (Ensemble)</h3>\n<pre class=\"r\"><code># Train model\nforest.model &lt;- randomForest(churn ~ incorporation_time + vertical, \n                       data = train.df, \n                       ntree=200, \n                       type=&quot;classification&quot;)\n\n# See error reduction with number of trees ( not much gained beyond ~25 trees)\nplot(forest.model)</code></pre>\n<p><img src=\"/content/images/2017/11/random-forest-error-reduction.png\" alt=\"\"></p>\n<pre class=\"r\"><code># Look at the variable Importance from the random forest\nvarImpPlot(forest.model, sort = T, main=&quot;Variable Importance&quot;)</code></pre>\n<p><img src=\"/content/images/2017/11/random-forest-variable-importance.png\" alt=\"\"></p>\n<pre class=\"r\"><code style=\"white-space: pre;\"># Evaluation metrics\nforest.pred      &lt;- predict(forest.model, newdata = test.df, type = &quot;class&quot;)\nforest.result    &lt;- confusionMatrix(data = forest.pred, test.df$churn)\nforest.precision &lt;- forest.result$byClass['Pos Pred Value']    \nforest.recall    &lt;- forest.result$byClass['Sensitivity']\nforest.F1        &lt;- forest.result$byClass['F1']</code></pre>\n<h3>Evaluation Metrics</h3>\n<pre class=\"r\"><code>log.precision; \n## Pos Pred Value \n##      0.8333333\ntree.precision; \n## Pos Pred Value \n##      0.8088235\nforest.precision;\n## Pos Pred Value \n##      0.8134328</code></pre>\n<pre class=\"r\"><code>log.recall; \n## Sensitivity \n##   0.9016393\ntree.recall;\n## Sensitivity \n##   0.9016393\nforest.recall;\n## Sensitivity \n##   0.8934426</code></pre>\n<pre class=\"r\"><code>log.F1;\n##        F1 \n## 0.8661417\ntree.F1;\n##        F1 \n## 0.8527132\nforest.F1;\n##        F1 \n## 0.8515625</code></pre>\n<p>Surprisingly, the logistic regression model performs the best, with the top precision score and equal recall score with that of the decision tree. With more time, I’d see if tweaks to the decision tree and random forest models could change this. Its also possible over/undersampling could help. From here on I will use the logistic regression model.</p>\n<h2>Examine the incorporation of time information</h2>\n<p>Per the second aim, examine the inclusion of time information. This requires more data munging. Taking code used from above, I will extend to include the derivation of a very basic time period variable.</p>\n<pre class=\"r\"><code># Create binary 'leading_indicator' column\nmodel.df$leading_indicator &lt;- 0\n\n# Min date for which a leading_indicator can be calculated (lower limit of data)\nmin.date &lt;- min(df$date)\n\n# If month before 'churn' (churn-1), is below the level of mandates of the month 2 months prior (churn-2) then\n# make leading_indicator == 1\nfor (company in churners$company_id) {\n\n    # Subset data to company\n    df.sub &lt;- subset(df, df$company_id == company)\n    \n    # Get month prior to churn\n    month.prior &lt;- df.sub$date[df.sub$churn == 1] %m-% months(1)\n\n    # Get two months prior to churn\n    two.month.prior &lt;- df.sub$date[df.sub$churn == 1] %m-% months(2)\n\n    # If two months prior is within dataset date range and level of mandates is greater than 0\n    if ((two.month.prior &gt; min.date) &amp;&amp; (df.sub$mandates[df.sub$date == two.month.prior] &gt; 0)) {\n        \n        # Compare number of mandates 1 month prior to 2 months prior, if less, mark 'leading_indicator' as '1' \n        if (df.sub$mandates[df.sub$date == month.prior] &lt; df.sub$mandates[df.sub$date == two.month.prior]) {\n            model.df[model.df$company_id == company, ]$leading_indicator &lt;- 1\n        }\n    }\n}</code></pre>\n<h3>Of the churners, how many have a leading indicator?</h3>\n<pre class=\"r\"><code>model.df %&gt;% \n    filter(churn == 1) %&gt;%\n    group_by(leading_indicator) %&gt;%\n    summarise(n=n()) %&gt;%\n    mutate(percent = round(n / sum(n) * 100, 1))</code></pre>\n<pre><code>## # A tibble: 2 x 3\n##   leading_indicator     n percent\n##               &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1                 0   264    91.3\n## 2                 1    25     8.7</code></pre>\n<h3>Re-train model and evaluate</h3>\n<pre class=\"r\"><code style=\"white-space: pre;\"># Re-split data so 'leading_indicator' is in columns (the index remains the same)\ntrain.df &lt;- model.df[index, ]\ntest.df  &lt;- model.df[-index, ]</code></pre>\n<h3>Logistic</h3>\n<pre class=\"r\"><code style=\"white-space: pre;\"># Run model\nlead.logistic.model &lt;- glm(churn ~ incorporation_time + vertical + leading_indicator, \n                      data   = train.df, \n                      family = binomial(link = 'logit'))\n\n# Predict\nlog.pred &lt;- predict(lead.logistic.model, newdata = test.df, type = 'response')\n\n# Convert probs to binary\nlog.pred &lt;- as.factor(ifelse(log.pred &gt; 0.5, 1, 0))\n\n# Evaluation Metrics\nlead.log.result    &lt;- confusionMatrix(data = log.pred, test.df$churn)\nlead.log.precision &lt;- log.result$byClass['Pos Pred Value']\nlead.log.recall    &lt;- log.result$byClass['Sensitivity']\nlead.log.F1        &lt;- log.result$byClass['F1']</code></pre>\n<h3>Evaluation Metrics</h3>\n<p>Compare the precision and recall of the logistic model with and without the <code>lead_indicator</code>.</p>\n<pre class=\"r\"><code>log.precision\n## Pos Pred Value \n##      0.8333333</code></pre>\n<pre class=\"r\"><code>lead.log.precision\n## Pos Pred Value \n##      0.8333333</code></pre>\n<pre class=\"r\"><code>log.recall\n## Sensitivity \n##   0.9016393</code></pre>\n<pre class=\"r\"><code>lead.log.recall\n## Sensitivity \n##   0.9016393</code></pre>\n<pre><code style=\"white-space: pre;\"># Have a look at the model coefficients and p-values\nsummary(lead.logistic.model)\n\n## Call:\n## glm(formula = churn ~ incorporation_time + vertical + leading_indicator, \n##     family = binomial(link = &quot;logit&quot;), data = train.df)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.4429  -0.8252  -0.4957   0.9938   2.5564  \n## \n## Coefficients:\n##                              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)                   0.74066    0.20007   3.702 0.000214 ***\n## incorporation_time           -0.26140    0.03045  -8.584  &lt; 2e-16 ***\n## verticalfreelance developer   0.12614    0.21737   0.580 0.561714    \n## verticalgym/fitness          -0.91099    0.22342  -4.077 4.55e-05 ***\n## leading_indicator            18.06219  482.97671   0.037 0.970168    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 907.42  on 722  degrees of freedom\n## Residual deviance: 736.18  on 718  degrees of freedom\n## AIC: 746.18\n## \n## Number of Fisher Scoring iterations: 15</code></pre>\n<p>Generally, it is best to minimise the <code>AIC</code>. The <code>Logistic.model</code> model AIC is 800.5 compared with the <code>lead.logistic.model</code> incorporating the <code>lead_indicator</code> is 746.18. The <code>lead_indicator</code> has not changed the predictive power on this dataset, but since the <code>AIC</code> favours a better model fit whilst penalising for additional predictors, the model to choose is the <code>lead.logistic.model</code>.</p>\n<h4>Re-train model and save</h4>\n<pre class=\"r\"><code style=\"white-space: pre;\"># Re-train on all data\nfinal.model &lt;- glm(churn ~ incorporation_time + vertical + leading_indicator, \n                      data   = model.df, \n                      family = binomial(link = 'logit'))\n<p>save(final.model, file = &quot;model.rda&quot;)</code></pre></p>\n<p>If you've found this content helpful why not...<br></p>\n<style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#000000 !important;background-color:#FFDD00 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#000000 !important;}</style>\n  <link href=\"https://fonts.googleapis.com/css?family=Cookie\" rel=\"stylesheet\"><a class=\"bmc-button\" target=\"_blank\" href=\"https://www.buymeacoffee.com/6uRXFwMJD\">\n  <span style=\"margin-left:5px\">buy me a coffee</span></a>\n<br>\n<h2>Recommendations for further investigation/Comments</h2>\n<ul>\n<li>The <code>lead_indicator</code> was a quick and dirty test, I’d spend more time looking at a better construction (e.g. moving avg)</li>\n<li>If the cost of rentention activities vs losing a customer was known then an the optimal trade-off in terms of business cost could be found.</li>\n<li>Incorporating additional data, e.g. CRM data showing interactions. Potentially assessing sales/account staff.</li>\n<li>If it were a production model prompting staff to do retention calls, evaluate the impact of such calls through modelling e.g. a/b testing</li>\n<li>Test model performance with a change to balancing the classes e.g. under/oversampling, boostrap samples. This may explain the relative underperformance of tree based models in this exercise.</li>\n<li>Try other binary classifier models.</li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"44","plaintext":"The following post details how to make a churn model in R. It was part of an\ninterview process for which a take home assignment was one of the stages. The\ncompany stated this should take 2hrs, which is entirely unrealistic. To minimise\nthe time cost, my analysis is very succinct and short on the exploratory\nanalysis and amount of models compared. The churn model got me to the final\nstage, however little in the way of feedback was offered. There is considerable\ndebate in the tech industry as to whether take home exams are a fair assessment\nor even a reasonable ask on an individual. My assessment is that this is a lazy\nway to interview and a very high cost on the interviewee. If you really want to\nwork for a particular company, then sure you might be prepared to do what it\ntakes. I doubt I will do another take home assignment.\n\nThe analysis was done in Rmarkdown [http://rmarkdown.rstudio.com/] and the\noutput copied into this blog post following these helpful pointers\n[https://notes.innovea.tech/how-to-write-latex-formula-in-ghosts-post/]. If you\nwould like to play with the data and full unabridged code please visit this\ngithub repo [https://github.com/ucg8j/how-to-make-churn-model-in-R].\n\nAim\n 1. Please create a model that predicts which businesses are likely to churn at\n    the start of 2015 based on the vertical and incorporation_date.\n 2. We have an inkling that a dropoff in the number of mandates added might be\n    an advance indicator of someone churning. Please can you assess whether this\n    might be true, and if you think it is useful, incorporate it into your model\n    from part (1).\n\n# Load packages\nsuppressPackageStartupMessages({\n    library(data.table)   # Fast I/O\n    library(dplyr)        # Data munging\n    library(tidyr)        # Data munging\n    library(lubridate)    # Makes dates easy\n    library(plotly)       # Interactive charts\n    library(magrittr)     # pipe operators\n    library(caret)        # Handy ML functions\n    library(rpart)        # Decision Trees\n    library(rpart.plot)   # Pretty tree plots\n    library(ROCR)         # ML evaluation\n    library(e1071)        # Misc stat fns\n    library(randomForest) # rf\n})\n\nset.seed(42)\n\n# Read data and drop row number column\ndf <- fread(\"monthly_data_(2)_(2).csv\", drop = 1)\n\n# Have a glimpse of the data\nglimpse(df)\n\n\n## Observations: 902\n## Variables: 27\n## $ company_id          <int> 4, 5, 6, 7, 8, 10, 11, 12, 13, 14...\n## $ 2014-01-01_payments <dbl> 8, 0, 2, 3, 0, 0, 0, 0, 0, 0, 4, ...\n## $ 2014-02-01_payments <dbl> 4, 0, 2, 3, 0, 0, 0, 2, 0, 11, 1,...\n## $ 2014-03-01_payments <dbl> 7, 39, 1, 1, 6, 0, 0, 0, 8, 0, 1,...\n## $ 2014-04-01_payments <dbl> 7, 0, 3, 7, 50, 1, 0, 0, 2, 0, 0,...\n## $ 2014-05-01_payments <dbl> 1, 54, 1, 4, 119, 0, 1, 0, 0, 0, ...\n## $ 2014-06-01_payments <dbl> 2, 0, 2, 1, 151, 3, 0, 0, 0, 0, 2...\n## $ 2014-07-01_payments <dbl> 2, 0, 2, 7, 182, 0, 0, 0, 3, 0, 0...\n## $ 2014-08-01_payments <dbl> 4, 22, 1, 2, 167, 0, 0, 0, 2, 0, ...\n## $ 2014-09-01_payments <dbl> 3, 0, 1, 5, 180, 0, 0, 0, 0, 9, 5...\n## $ 2014-10-01_payments <dbl> 5, 0, 2, 8, 157, 1, 0, 0, 0, 2, 1...\n## $ 2014-11-01_payments <dbl> 5, 0, 1, 2, 105, 0, 0, 0, 0, 0, 0...\n## $ 2014-12-01_payments <dbl> 9, 0, 3, 8, 57, 0, 0, 0, 0, 0, 0,...\n## $ 2014-01-01_mandates <dbl> 0, 4, 0, 0, 0, 4, 4, 0, 0, 22, 0,...\n## $ 2014-02-01_mandates <dbl> 0, 31, 1, 0, 2, 3, 8, 0, 0, 20, 1...\n## $ 2014-03-01_mandates <dbl> 0, 24, 0, 0, 0, 5, 19, 0, 0, 11, ...\n## $ 2014-04-01_mandates <dbl> 53, 18, 0, 1, 0, 0, 0, 0, 1, 11, ...\n## $ 2014-05-01_mandates <dbl> 0, 8, 0, 0, 0, 0, 0, 0, 1, 15, 0,...\n## $ 2014-06-01_mandates <dbl> 0, 7, 0, 0, 0, 0, 0, 0, 0, 13, 5,...\n## $ 2014-07-01_mandates <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 16...\n## $ 2014-08-01_mandates <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 8,...\n## $ 2014-09-01_mandates <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 2,...\n## $ 2014-10-01_mandates <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 16, 1,...\n## $ 2014-11-01_mandates <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, ...\n## $ 2014-12-01_mandates <dbl> 0, 0, 0, 3, 0, 0, 0, 0, 0, 11, 0,...\n## $ vertical            <chr> \"gym/fitness\", \"freelance d...\n## $ incorporation_date  <chr> \"2003-09-25\", \"2008-10-22\", ...\n\nData Munging\nSome data munging needs to occur for our binary classifiers to make use of the\ndata within. This includes handling dates.\n\n# Reshape data and create new columns\ndf %<>%\n    gather(key = date, value = quantity, starts_with(\"20\")) %>%\n    separate(date, c(\"date\",\"paymentMandate\"), \"_\") %>%\n    spread(paymentMandate, quantity) %>%\n    mutate(incorporation_date = as.Date(incorporation_date),\n           date = as.Date(date),\n           incorporation_time = round(as.numeric(difftime(as.Date(\"2014-12-01\"), \n                                                    as.Date(incorporation_date), \n                                                    unit=\"weeks\")) / 52.25,\n                                digits = 1)) %>%\n    arrange(date)\n\n# What does the new data look like?\nglimpse(df)\n\n\n## Observations: 10,824\n## Variables: 7\n## $ company_id         <int> 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14,...\n## $ vertical           <chr> \"gym/fitness\", \"gym/fitness\", \"freelance de...\n## $ incorporation_date <date> 2013-05-30, 2003-09-25, 2008-10-22, 2005-0...\n## $ date               <date> 2014-01-01, 2014-01-01, 2014-01-01, 2014-0...\n## $ mandates           <dbl> 1, 0, 0, 0, 4, 0, 0, 0, 4, 4, 0, 0, 22, 0, ...\n## $ payments           <dbl> 0, 1, 6, 8, 0, 2, 3, 0, 0, 0, 0, 0, 0, 4, 4...\n## $ incorporation_time <dbl> 1.5, 11.2, 6.1, 9.4, 1.2, 9.1, 4.0, 2.9, 1....\n\nWhat is Churn\nFor the purposes of this project I have defined ‘churn’ as zero mandates and\npayments for the first instance of that after the last mandate/payment made.\n\n# Create binary 'churn' column\ndf$churn <- 0\n\n# For use in for loop - upper bound of data\nmax.date <- max(df$date)\n\n# Mark all companies as churned in the month immediately their last activity\nfor (company in unique(df$company_id)) {\n\n    # Subset data to company\n    df.sub <- subset(df, df$company_id == company)\n    \n    # Index of last positive mandate OR payment\n    last.pos.idx <- tail(which(df.sub$mandates != 0 | df.sub$payments != 0), 1)\n    \n    # Get date of last activity\n    last.activity.date <- df.sub$date[last.pos.idx]\n    \n    # If less than max.date of dataset mark churn ELSE do nothing i.e. positive at end of period\n    if (last.activity.date < max.date) {\n        \n        # Get churn date (last positive month plus 1mth)\n        churn.date <- last.activity.date %m+% months(1)\n        \n        # Mark month of churn as 1\n        df[df$date == churn.date & df$company_id == company, ]$churn <- 1\n    }\n}\n\n# Multiple rows per company, filter for last month or churn month values...\n# Get churners\ndf %>% filter(churn == 1) -> churners\n\n# Get max date row of remainers (non-churners)\ndf %>% \n    filter(churn == 0 & !(company_id %in% churners$company_id) & date == max(date)) -> remainers\n\n# Combine and variables coded ready for modelling\nchurners %>% \n    rbind(remainers) %>%\n    mutate(vertical = as.factor(vertical),\n           churn    = as.factor(churn)) -> model.df\n\nChurns over the year\n# Plot churners\nmodel.df %>%\n    filter(churn == 1) %>%\n    group_by(date) %>%\n    summarise(n = n()) %>%\n    plot_ly( x = ~date, y = ~n, type = 'scatter', mode = 'lines')\n\n\n\nCould do more on exploratory but this is the easiest to cut back for this\nreport.\n\nBalance of the data\nThe proportion of churn is 32.04%. Representing an imbalanced dataset. Accuracy\nis an inappropriate measure (I could get 67.96% accuracy predicting no\nbusinesses leave), so I will focus on recall and accuracy.\n\n# Loyal vs Churn\ntable(model.df$churn)\n## \n##   0   1 \n## 613 289\n\nModel\nSurvival models and binary classifiers are common approaches to ‘Churn’ models.\nI will approach the model using the latter, though if I had more time I would\ninvestigate other classifiers and a survival model. I will limit the range of\nmodels to a logistic, a decision tree and an ensemble.\n\nSplit Data\n# 80/20 train test split\nindex    <- createDataPartition(model.df$churn, p = 0.8, list = FALSE)\ntrain.df <- model.df[index, ]\ntest.df  <- model.df[-index, ]\n\n# Check balance of the training split\ntable(train.df$churn)\n## \n##   0   1 \n## 491 232\n\n# Check balance of the test split\ntable(test.df$churn)\n## \n##   0   1 \n## 122  57\n\nLogistic\n# Run model\nLogistic.model <- glm(churn ~ incorporation_time + vertical, \n                      data   = train.df, \n                      family = binomial(link = 'logit'))\n\n# Predict\nlog.pred <- predict(Logistic.model, newdata = test.df, type = 'response')\n\n# Convert probs to binary\nlog.pred <- as.factor(ifelse(log.pred > 0.5, 1, 0))\n\n# Evaluation Metrics\nlog.result    <- confusionMatrix(data = log.pred, test.df$churn)\nlog.precision <- log.result$byClass['Pos Pred Value']    \nlog.recall    <- log.result$byClass['Sensitivity']\nlog.F1        <- log.result$byClass['F1']\n\nDecision Tree\n# Train model\ntree.model <- rpart(churn ~ incorporation_time + vertical,\n                    data = train.df,\n                    method = \"class\",\n                    control = rpart.control(xval = 10))\n\n# Plot\nrpart.plot(tree.model)\n\n\n\n# Evaluation metrics\ntree.pred      <- predict(tree.model, newdata = test.df, type = \"class\")\ntree.result    <- confusionMatrix(data = tree.pred, test.df$churn)\ntree.precision <- tree.result$byClass['Pos Pred Value']    \ntree.recall    <- tree.result$byClass['Sensitivity']\ntree.F1        <- tree.result$byClass['F1']\n\nRandom Forest (Ensemble)\n# Train model\nforest.model <- randomForest(churn ~ incorporation_time + vertical, \n                       data = train.df, \n                       ntree=200, \n                       type=\"classification\")\n\n# See error reduction with number of trees ( not much gained beyond ~25 trees)\nplot(forest.model)\n\n\n\n# Look at the variable Importance from the random forest\nvarImpPlot(forest.model, sort = T, main=\"Variable Importance\")\n\n\n\n# Evaluation metrics\nforest.pred      <- predict(forest.model, newdata = test.df, type = \"class\")\nforest.result    <- confusionMatrix(data = forest.pred, test.df$churn)\nforest.precision <- forest.result$byClass['Pos Pred Value']    \nforest.recall    <- forest.result$byClass['Sensitivity']\nforest.F1        <- forest.result$byClass['F1']\n\nEvaluation Metrics\nlog.precision; \n## Pos Pred Value \n##      0.8333333\ntree.precision; \n## Pos Pred Value \n##      0.8088235\nforest.precision;\n## Pos Pred Value \n##      0.8134328\n\nlog.recall; \n## Sensitivity \n##   0.9016393\ntree.recall;\n## Sensitivity \n##   0.9016393\nforest.recall;\n## Sensitivity \n##   0.8934426\n\nlog.F1;\n##        F1 \n## 0.8661417\ntree.F1;\n##        F1 \n## 0.8527132\nforest.F1;\n##        F1 \n## 0.8515625\n\nSurprisingly, the logistic regression model performs the best, with the top\nprecision score and equal recall score with that of the decision tree. With more\ntime, I’d see if tweaks to the decision tree and random forest models could\nchange this. Its also possible over/undersampling could help. From here on I\nwill use the logistic regression model.\n\nExamine the incorporation of time information\nPer the second aim, examine the inclusion of time information. This requires\nmore data munging. Taking code used from above, I will extend to include the\nderivation of a very basic time period variable.\n\n# Create binary 'leading_indicator' column\nmodel.df$leading_indicator <- 0\n\n# Min date for which a leading_indicator can be calculated (lower limit of data)\nmin.date <- min(df$date)\n\n# If month before 'churn' (churn-1), is below the level of mandates of the month 2 months prior (churn-2) then\n# make leading_indicator == 1\nfor (company in churners$company_id) {\n\n    # Subset data to company\n    df.sub <- subset(df, df$company_id == company)\n    \n    # Get month prior to churn\n    month.prior <- df.sub$date[df.sub$churn == 1] %m-% months(1)\n\n    # Get two months prior to churn\n    two.month.prior <- df.sub$date[df.sub$churn == 1] %m-% months(2)\n\n    # If two months prior is within dataset date range and level of mandates is greater than 0\n    if ((two.month.prior > min.date) && (df.sub$mandates[df.sub$date == two.month.prior] > 0)) {\n        \n        # Compare number of mandates 1 month prior to 2 months prior, if less, mark 'leading_indicator' as '1' \n        if (df.sub$mandates[df.sub$date == month.prior] < df.sub$mandates[df.sub$date == two.month.prior]) {\n            model.df[model.df$company_id == company, ]$leading_indicator <- 1\n        }\n    }\n}\n\nOf the churners, how many have a leading indicator?\nmodel.df %>% \n    filter(churn == 1) %>%\n    group_by(leading_indicator) %>%\n    summarise(n=n()) %>%\n    mutate(percent = round(n / sum(n) * 100, 1))\n\n## # A tibble: 2 x 3\n##   leading_indicator     n percent\n##               <dbl> <int>   <dbl>\n## 1                 0   264    91.3\n## 2                 1    25     8.7\n\nRe-train model and evaluate\n# Re-split data so 'leading_indicator' is in columns (the index remains the same)\ntrain.df <- model.df[index, ]\ntest.df  <- model.df[-index, ]\n\nLogistic\n# Run model\nlead.logistic.model <- glm(churn ~ incorporation_time + vertical + leading_indicator, \n                      data   = train.df, \n                      family = binomial(link = 'logit'))\n\n# Predict\nlog.pred <- predict(lead.logistic.model, newdata = test.df, type = 'response')\n\n# Convert probs to binary\nlog.pred <- as.factor(ifelse(log.pred > 0.5, 1, 0))\n\n# Evaluation Metrics\nlead.log.result    <- confusionMatrix(data = log.pred, test.df$churn)\nlead.log.precision <- log.result$byClass['Pos Pred Value']\nlead.log.recall    <- log.result$byClass['Sensitivity']\nlead.log.F1        <- log.result$byClass['F1']\n\nEvaluation Metrics\nCompare the precision and recall of the logistic model with and without the \nlead_indicator.\n\nlog.precision\n## Pos Pred Value \n##      0.8333333\n\nlead.log.precision\n## Pos Pred Value \n##      0.8333333\n\nlog.recall\n## Sensitivity \n##   0.9016393\n\nlead.log.recall\n## Sensitivity \n##   0.9016393\n\n# Have a look at the model coefficients and p-values\nsummary(lead.logistic.model)\n\n## Call:\n## glm(formula = churn ~ incorporation_time + vertical + leading_indicator, \n##     family = binomial(link = \"logit\"), data = train.df)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.4429  -0.8252  -0.4957   0.9938   2.5564  \n## \n## Coefficients:\n##                              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)                   0.74066    0.20007   3.702 0.000214 ***\n## incorporation_time           -0.26140    0.03045  -8.584  < 2e-16 ***\n## verticalfreelance developer   0.12614    0.21737   0.580 0.561714    \n## verticalgym/fitness          -0.91099    0.22342  -4.077 4.55e-05 ***\n## leading_indicator            18.06219  482.97671   0.037 0.970168    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 907.42  on 722  degrees of freedom\n## Residual deviance: 736.18  on 718  degrees of freedom\n## AIC: 746.18\n## \n## Number of Fisher Scoring iterations: 15\n\nGenerally, it is best to minimise the AIC. The Logistic.model model AIC is 800.5\ncompared with the lead.logistic.model incorporating the lead_indicator is\n746.18. The lead_indicator has not changed the predictive power on this dataset,\nbut since the AIC favours a better model fit whilst penalising for additional\npredictors, the model to choose is the lead.logistic.model.\n\nRe-train model and save\n# Re-train on all data\nfinal.model <- glm(churn ~ incorporation_time + vertical + leading_indicator, \n                      data   = model.df, \n                      family = binomial(link = 'logit'))\nsave(final.model, file = \"model.rda\")\n\n\n\n\n\nIf you've found this content helpful why not...\n\n\nbuy me a coffee [https://www.buymeacoffee.com/6uRXFwMJD]\nRecommendations for further investigation/Comments\n * The lead_indicator was a quick and dirty test, I’d spend more time looking at\n   a better construction (e.g. moving avg)\n * If the cost of rentention activities vs losing a customer was known then an\n   the optimal trade-off in terms of business cost could be found.\n * Incorporating additional data, e.g. CRM data showing interactions.\n   Potentially assessing sales/account staff.\n * If it were a production model prompting staff to do retention calls, evaluate\n   the impact of such calls through modelling e.g. a/b testing\n * Test model performance with a change to balancing the classes e.g.\n   under/oversampling, boostrap samples. This may explain the relative\n   underperformance of tree based models in this exercise.\n * Try other binary classifier models.","feature_image":"/content/images/2017/11/brighton-palace-pier.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-11-21 10:34:34","created_by":"1","updated_at":"2020-01-02 13:57:13","updated_by":null,"published_at":"2017-11-21 11:11:24","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd05","uuid":"49217964-723f-45c8-8938-5883fb633c64","title":"Universal Grammars and NLP","slug":"universal-grammars-and-nlp","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}","html":null,"comment_id":"45","plaintext":null,"feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-11-23 15:01:43","created_by":"1","updated_at":"2017-11-24 09:23:55","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd06","uuid":"5ae0ea6c-3f02-4c3f-a6da-da401af5711a","title":"Fallacies of Argument with Stick Figures","slug":"fallacies-of-argument-with-stick-figures","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}","html":null,"comment_id":"46","plaintext":null,"feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-11-25 18:50:46","created_by":"1","updated_at":"2017-11-26 18:17:42","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd07","uuid":"61f0c222-e16a-4039-a2ed-f17afced477c","title":"London","slug":"london-vs-sydney","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Terror is noticeable\\n\\nno walks on pavements orderly. Screwed up since generally this is determined by the side of the road people drive on. However, the tube says stand to the right...\\n\\nbanking... fucking non-memorable information... just\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Terror is noticeable</p>\n<p>no walks on pavements orderly. Screwed up since generally this is determined by the side of the road people drive on. However, the tube says stand to the right...</p>\n<p>banking... fucking non-memorable information... just</p>\n<!--kg-card-end: markdown-->","comment_id":"47","plaintext":"Terror is noticeable\n\nno walks on pavements orderly. Screwed up since generally this is determined by\nthe side of the road people drive on. However, the tube says stand to the\nright...\n\nbanking... fucking non-memorable information... just","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-11-26 18:17:54","created_by":"1","updated_at":"2017-11-27 09:54:20","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd08","uuid":"040a64e7-c6c9-4269-9c27-76fa0e8b0abe","title":"Docker in 5 Minutes","slug":"docker-in-5-minutes","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"https://embano1.github.io/post/scratch/\\n\\n## Tips\\nrunning out of space on a VM\\n```\\ndocker ps --filter status=dead --filter status=exited -aq \\\\\\n  | xargs docker rm -v\\n```\\nOR\\n```\\nsudo service docker restart \\n```\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><a href=\"https://embano1.github.io/post/scratch/\">https://embano1.github.io/post/scratch/</a></p>\n<h2 id=\"tips\">Tips</h2>\n<p>running out of space on a VM</p>\n<pre><code>docker ps --filter status=dead --filter status=exited -aq \\\n  | xargs docker rm -v\n</code></pre>\n<p>OR</p>\n<pre><code>sudo service docker restart \n</code></pre>\n<!--kg-card-end: markdown-->","comment_id":"48","plaintext":"https://embano1.github.io/post/scratch/\n\nTips\nrunning out of space on a VM\n\ndocker ps --filter status=dead --filter status=exited -aq \\\n  | xargs docker rm -v\n\n\nOR\n\nsudo service docker restart","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2017-11-27 09:54:56","created_by":"1","updated_at":"2017-12-06 15:46:41","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd09","uuid":"f5655b19-da26-43f9-aec8-e448885f0320","title":"The Meta 100 best non-fiction Books to read of all time","slug":"the-meta-100-best-non-fiction-books-to-read-of-all-time","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Aggregate several lists into one. Then rank list by mentions and placement in lists.\\n\\ninclude wordcloud of top 100\\n\\nhttps://en.wikipedia.org/wiki/Lists_of_100_best_books\\n\\n[Time's Magazine](https://www.goodreads.com/list/show/12719.Time_Magazine_s_All_TIME_100_Best_Non_Fiction_Books)\\n\\n[The Guardian's](https://www.theguardian.com/books/2017/dec/31/the-100-best-nonfiction-books-of-all-time-the-full-list)\\n\\n[Modern Library](http://www.modernlibrary.com/top-100/100-best-nonfiction/)\\n\\n[This looks pretty good... is the methodology open?](http://thegreatestbooks.org/nonfiction)\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Aggregate several lists into one. Then rank list by mentions and placement in lists.</p>\n<p>include wordcloud of top 100</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Lists_of_100_best_books\">https://en.wikipedia.org/wiki/Lists_of_100_best_books</a></p>\n<p><a href=\"https://www.goodreads.com/list/show/12719.Time_Magazine_s_All_TIME_100_Best_Non_Fiction_Books\">Time's Magazine</a></p>\n<p><a href=\"https://www.theguardian.com/books/2017/dec/31/the-100-best-nonfiction-books-of-all-time-the-full-list\">The Guardian's</a></p>\n<p><a href=\"http://www.modernlibrary.com/top-100/100-best-nonfiction/\">Modern Library</a></p>\n<p><a href=\"http://thegreatestbooks.org/nonfiction\">This looks pretty good... is the methodology open?</a></p>\n<!--kg-card-end: markdown-->","comment_id":"49","plaintext":"Aggregate several lists into one. Then rank list by mentions and placement in\nlists.\n\ninclude wordcloud of top 100\n\nhttps://en.wikipedia.org/wiki/Lists_of_100_best_books\n\nTime's Magazine\n[https://www.goodreads.com/list/show/12719.Time_Magazine_s_All_TIME_100_Best_Non_Fiction_Books]\n\nThe Guardian's\n[https://www.theguardian.com/books/2017/dec/31/the-100-best-nonfiction-books-of-all-time-the-full-list]\n\nModern Library [http://www.modernlibrary.com/top-100/100-best-nonfiction/]\n\nThis looks pretty good... is the methodology open?\n[http://thegreatestbooks.org/nonfiction]","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2018-01-20 13:45:20","created_by":"1","updated_at":"2018-01-22 22:26:13","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd0a","uuid":"cada716b-e9df-412f-bc61-36a9ab6ecf24","title":"Persistent Data in a Conatinerised Environment","slug":"persistent-data-in-a-conatinerised-environment","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"https://gravitational.com/blog/running-postgresql-on-kubernetes/\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><a href=\"https://gravitational.com/blog/running-postgresql-on-kubernetes/\">https://gravitational.com/blog/running-postgresql-on-kubernetes/</a></p>\n<!--kg-card-end: markdown-->","comment_id":"50","plaintext":"https://gravitational.com/blog/running-postgresql-on-kubernetes/","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2018-01-23 09:46:03","created_by":"1","updated_at":"2018-01-23 09:46:06","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd0b","uuid":"7c6f13dd-709d-4d36-93d1-31f05a18cfa8","title":"How to Make a Local Backup of a Remote Postgres db Using SSL","slug":"how-to-make-a-local-backup-of-a-remote-postgres-db-using-ssl","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Here I've made a local copy of a remote postgres database (db) that requires an SSL connection. Ensuring you use an SSL connection with a remote db is one way to mitigate man-in-the-middle attacks whilst you transfer potentially sensitive data across the net.\\n\\nOne piece of kit I haven't come across before is [postgres.app](https://postgresapp.com/), I've installed Postgres on a MacBook a year or two ago and found it quite a fiddly error prone process. Postgres.app makes the setup incredibly easy, providing a nice little GUI to create and run local db servers and sorts out all the command line tools associated with Postgres.\\n\\n```\\n# set SSL remote\\nexport PGSSLMODE=require \\n\\n# get user/roles\\npg_dumpall -h hostname -U username postgres --globals-only > globals.sql\\n\\n# download data\\npg_dump -C -h acrotrendproduct-postgresql.postgres.database.azure.com -U acrotrend@acrotrendproduct-postgresql postgres > gsd_dump2.sql\\n\\n# unset\\nunset PGSSLMODE\\n\\n# populate user/roles\\npsql -h localhost -U luke.singham -d postgres -p 5432 -f globals.sql\\n\\n# populate\\npsql -h localhost -U luke.singham -d postgres -p 5432 < gsd_dump.sql\\n```\\n\\n#### Further Resources\\n- [Very helpful SO on the difference between `pg_dump` and `pg_dumpall`](https://stackoverflow.com/a/16619878/3691003)\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Here I've made a local copy of a remote postgres database (db) that requires an SSL connection. Ensuring you use an SSL connection with a remote db is one way to mitigate man-in-the-middle attacks whilst you transfer potentially sensitive data across the net.</p>\n<p>One piece of kit I haven't come across before is <a href=\"https://postgresapp.com/\">postgres.app</a>, I've installed Postgres on a MacBook a year or two ago and found it quite a fiddly error prone process. Postgres.app makes the setup incredibly easy, providing a nice little GUI to create and run local db servers and sorts out all the command line tools associated with Postgres.</p>\n<pre><code># set SSL remote\nexport PGSSLMODE=require \n\n# get user/roles\npg_dumpall -h hostname -U username postgres --globals-only &gt; globals.sql\n\n# download data\npg_dump -C -h acrotrendproduct-postgresql.postgres.database.azure.com -U acrotrend@acrotrendproduct-postgresql postgres &gt; gsd_dump2.sql\n\n# unset\nunset PGSSLMODE\n\n# populate user/roles\npsql -h localhost -U luke.singham -d postgres -p 5432 -f globals.sql\n\n# populate\npsql -h localhost -U luke.singham -d postgres -p 5432 &lt; gsd_dump.sql\n</code></pre>\n<h4 id=\"furtherresources\">Further Resources</h4>\n<ul>\n<li><a href=\"https://stackoverflow.com/a/16619878/3691003\">Very helpful SO on the difference between <code>pg_dump</code> and <code>pg_dumpall</code></a></li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"51","plaintext":"Here I've made a local copy of a remote postgres database (db) that requires an\nSSL connection. Ensuring you use an SSL connection with a remote db is one way\nto mitigate man-in-the-middle attacks whilst you transfer potentially sensitive\ndata across the net.\n\nOne piece of kit I haven't come across before is postgres.app\n[https://postgresapp.com/], I've installed Postgres on a MacBook a year or two\nago and found it quite a fiddly error prone process. Postgres.app makes the\nsetup incredibly easy, providing a nice little GUI to create and run local db\nservers and sorts out all the command line tools associated with Postgres.\n\n# set SSL remote\nexport PGSSLMODE=require \n\n# get user/roles\npg_dumpall -h hostname -U username postgres --globals-only > globals.sql\n\n# download data\npg_dump -C -h acrotrendproduct-postgresql.postgres.database.azure.com -U acrotrend@acrotrendproduct-postgresql postgres > gsd_dump2.sql\n\n# unset\nunset PGSSLMODE\n\n# populate user/roles\npsql -h localhost -U luke.singham -d postgres -p 5432 -f globals.sql\n\n# populate\npsql -h localhost -U luke.singham -d postgres -p 5432 < gsd_dump.sql\n\n\nFurther Resources\n * Very helpful SO on the difference between pg_dump and pg_dumpall\n   [https://stackoverflow.com/a/16619878/3691003]","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2018-01-26 10:35:37","created_by":"1","updated_at":"2018-01-29 12:03:24","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd0c","uuid":"953ff92c-6c0b-4a21-aa93-2456ab0214a9","title":"What's different about Java?","slug":"whats-different-about-java","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Ternery - \\n\\nConstructors - \\n\\nFor Loops: used to repeatedly run a block of code\\nFor Each Loops: a concise version of a for loop\\nArrayList: stores a list of data\\nHashMap: stores keys and associated values like a dictionary\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Ternery -</p>\n<p>Constructors -</p>\n<p>For Loops: used to repeatedly run a block of code<br>\nFor Each Loops: a concise version of a for loop<br>\nArrayList: stores a list of data<br>\nHashMap: stores keys and associated values like a dictionary</p>\n<!--kg-card-end: markdown-->","comment_id":"52","plaintext":"Ternery -\n\nConstructors -\n\nFor Loops: used to repeatedly run a block of code\nFor Each Loops: a concise version of a for loop\nArrayList: stores a list of data\nHashMap: stores keys and associated values like a dictionary","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2018-01-26 11:40:50","created_by":"1","updated_at":"2018-03-09 09:45:02","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd0d","uuid":"f3be673e-398a-445f-982d-aab6d14c1d77","title":"Laser Eye Surgery - International Cost Comparison","slug":"laser-eye-surgery-international-cost-comparison","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}","html":null,"comment_id":"53","plaintext":null,"feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2018-01-26 14:59:24","created_by":"1","updated_at":"2018-01-26 14:59:24","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd0e","uuid":"04f68a16-1246-46d8-a968-beec58f046a2","title":"DMLs, DDLs, ORMs... setting up databases","slug":"dmls-ddls-orms-setting-up-databases","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"# definitions of dmls and ddls\\nhttp://www.tomjewett.com/dbdesign/dbdesign.php?page=ddldml.php\\n\\nmigrate to a new db\\n```\\n# get user/roles\\npg_dumpall -h acrotrendproduct-postgresql.postgres.database.azure.com -U acrotrend@acrotrendproduct-postgresql postgres --globals-only > globals.sql\\n\\n# populate user/roles\\npsql -h localhost -U luke.singham -d gsd2901 -p 5432 -f globals29.1.18.sql\\n\\n# download data\\npg_dump -C -h acrotrendproduct-postgresql.postgres.database.azure.com -U acrotrend@acrotrendproduct-postgresql postgres > gsd_dump2.sql\\n\\n# unset\\nunset PGSSLMODE\\n\\n# populate\\npsql -h localhost -U luke.singham -d postgres -p 5432 < gsd_dump.sql\\n\\npg_dump -C -h acrotrendproduct-postgresql.postgres.database.azure.com -U acrotrend@acrotrendproduct-postgresql postgres > gsd_dump29.1.18.sql\\n\\n```\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><h1 id=\"definitionsofdmlsandddls\">definitions of dmls and ddls</h1>\n<p><a href=\"http://www.tomjewett.com/dbdesign/dbdesign.php?page=ddldml.php\">http://www.tomjewett.com/dbdesign/dbdesign.php?page=ddldml.php</a></p>\n<p>migrate to a new db</p>\n<pre><code># get user/roles\npg_dumpall -h acrotrendproduct-postgresql.postgres.database.azure.com -U acrotrend@acrotrendproduct-postgresql postgres --globals-only &gt; globals.sql\n\n# populate user/roles\npsql -h localhost -U luke.singham -d gsd2901 -p 5432 -f globals29.1.18.sql\n\n# download data\npg_dump -C -h acrotrendproduct-postgresql.postgres.database.azure.com -U acrotrend@acrotrendproduct-postgresql postgres &gt; gsd_dump2.sql\n\n# unset\nunset PGSSLMODE\n\n# populate\npsql -h localhost -U luke.singham -d postgres -p 5432 &lt; gsd_dump.sql\n\npg_dump -C -h acrotrendproduct-postgresql.postgres.database.azure.com -U acrotrend@acrotrendproduct-postgresql postgres &gt; gsd_dump29.1.18.sql\n\n</code></pre>\n<!--kg-card-end: markdown-->","comment_id":"54","plaintext":"definitions of dmls and ddls\nhttp://www.tomjewett.com/dbdesign/dbdesign.php?page=ddldml.php\n\nmigrate to a new db\n\n# get user/roles\npg_dumpall -h acrotrendproduct-postgresql.postgres.database.azure.com -U acrotrend@acrotrendproduct-postgresql postgres --globals-only > globals.sql\n\n# populate user/roles\npsql -h localhost -U luke.singham -d gsd2901 -p 5432 -f globals29.1.18.sql\n\n# download data\npg_dump -C -h acrotrendproduct-postgresql.postgres.database.azure.com -U acrotrend@acrotrendproduct-postgresql postgres > gsd_dump2.sql\n\n# unset\nunset PGSSLMODE\n\n# populate\npsql -h localhost -U luke.singham -d postgres -p 5432 < gsd_dump.sql\n\npg_dump -C -h acrotrendproduct-postgresql.postgres.database.azure.com -U acrotrend@acrotrendproduct-postgresql postgres > gsd_dump29.1.18.sql","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2018-01-30 11:37:24","created_by":"1","updated_at":"2018-03-16 11:32:38","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd0f","uuid":"19cb73c7-c58c-48ac-9eff-a49ef6ec1bc8","title":"Introduction to React","slug":"introduction-to-react","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"## JSX\\n\\n## Virtual DOM\\n\\n##\\n\\n## Javascript\\n>The simple answer is that this refers to an instance of IceCreamGuy. The less simple answer is that this refers to the object on which this's enclosing method, in this case .render(), is called. It is almost inevitable that this object will be an instance of IceCreamGuy, but technically it could be something else...\\n... You don't need those parentheses because .food is a getter method. You can tell this from the get in the above class declaration body.\\n\\nThere's nothing React-specific about getter methods, nor about this behaving in this way! However, in React you will see this used in this way almost constantly.\\n\\nReact components are complicated. Their syntax is complicated, and the reasoning behind their syntax is especially complicated.\\n\\n\\ncomponents can interact: a component can render another component.\\n\\nIn this lesson, you will learn another way that components can interact: a component can pass information to another component.\\n\\nInformation that gets passed from one component to another is known as \\\"props.\\\"\\n\\n Here's how the naming convention works: first, think about what type of event you are listening for. In our example, the event type was \\\"click.\\\"\\n\\nIf you are listening for a \\\"click\\\" event, then you name your event handler handleClick. If you are listening for a \\\"keyPress\\\" event, then you name your event handler handleKeyPress:\\n\\nprops is quite possibly the longest and most difficult lesson in all of our React courses. Congratulations on getting this far!\\n\\nHere are some of the skills that you have learned:\\n\\nPassing a prop by giving an attribute to a component instance\\nAccessing a passed-in prop via this.props.prop-name\\nDisplaying a prop\\nUsing a prop to make decisions about what to display\\nDefining an event handler in a component class\\nPassing an event handler as a prop\\nReceiving a prop event handler and attaching it to an event listener\\nNaming event handlers and event handler attributes according to convention\\nthis.props.children\\ngetDefaultProps\\n\\n## state\\nUnlike props, a component's state is not passed in from the outside. A component decides its own state.\\n\\nFor an in-depth explanation of this kind of binding trickery, begin with the React docs. For the less curious, just know that in React, whenever you define an event handler that uses this, you need to add this.methodName = this.methodName.bind(this) to your constructor function.\\n\\nAny time that you call this.setState(), this.setState() AUTOMATICALLY calls .render() as soon as the state has changed.\\nThat is why you can't call this.setState() from inside of the .render() method! this.setState() automatically calls .render(). If .render() calls this.setState(), then an infinite loop is created.\\n\\n \\n\\nSource: Codeadamemy\\n\\n## constructors\\n- `super()` is necessary for constructors to assign methods and values like `props` to the component.\\n- `this.` is uninitialised without a call to `super()`\\n- `super(props)`is necessary if you want to access `this.props` inside the constructor\\n\\n## TODO - google 'types of methods in js' e.g. getter method\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><h2 id=\"jsx\">JSX</h2>\n<h2 id=\"virtualdom\">Virtual DOM</h2>\n<h2 id=\"\"></h2>\n<h2 id=\"javascript\">Javascript</h2>\n<blockquote>\n<p>The simple answer is that this refers to an instance of IceCreamGuy. The less simple answer is that this refers to the object on which this's enclosing method, in this case .render(), is called. It is almost inevitable that this object will be an instance of IceCreamGuy, but technically it could be something else...<br>\n... You don't need those parentheses because .food is a getter method. You can tell this from the get in the above class declaration body.</p>\n</blockquote>\n<p>There's nothing React-specific about getter methods, nor about this behaving in this way! However, in React you will see this used in this way almost constantly.</p>\n<p>React components are complicated. Their syntax is complicated, and the reasoning behind their syntax is especially complicated.</p>\n<p>components can interact: a component can render another component.</p>\n<p>In this lesson, you will learn another way that components can interact: a component can pass information to another component.</p>\n<p>Information that gets passed from one component to another is known as &quot;props.&quot;</p>\n<p>Here's how the naming convention works: first, think about what type of event you are listening for. In our example, the event type was &quot;click.&quot;</p>\n<p>If you are listening for a &quot;click&quot; event, then you name your event handler handleClick. If you are listening for a &quot;keyPress&quot; event, then you name your event handler handleKeyPress:</p>\n<p>props is quite possibly the longest and most difficult lesson in all of our React courses. Congratulations on getting this far!</p>\n<p>Here are some of the skills that you have learned:</p>\n<p>Passing a prop by giving an attribute to a component instance<br>\nAccessing a passed-in prop via this.props.prop-name<br>\nDisplaying a prop<br>\nUsing a prop to make decisions about what to display<br>\nDefining an event handler in a component class<br>\nPassing an event handler as a prop<br>\nReceiving a prop event handler and attaching it to an event listener<br>\nNaming event handlers and event handler attributes according to convention<br>\nthis.props.children<br>\ngetDefaultProps</p>\n<h2 id=\"state\">state</h2>\n<p>Unlike props, a component's state is not passed in from the outside. A component decides its own state.</p>\n<p>For an in-depth explanation of this kind of binding trickery, begin with the React docs. For the less curious, just know that in React, whenever you define an event handler that uses this, you need to add this.methodName = this.methodName.bind(this) to your constructor function.</p>\n<p>Any time that you call this.setState(), this.setState() AUTOMATICALLY calls .render() as soon as the state has changed.<br>\nThat is why you can't call this.setState() from inside of the .render() method! this.setState() automatically calls .render(). If .render() calls this.setState(), then an infinite loop is created.</p>\n<p>Source: Codeadamemy</p>\n<h2 id=\"constructors\">constructors</h2>\n<ul>\n<li><code>super()</code> is necessary for constructors to assign methods and values like <code>props</code> to the component.</li>\n<li><code>this.</code> is uninitialised without a call to <code>super()</code></li>\n<li><code>super(props)</code>is necessary if you want to access <code>this.props</code> inside the constructor</li>\n</ul>\n<h2 id=\"todogoogletypesofmethodsinjseggettermethod\">TODO - google 'types of methods in js' e.g. getter method</h2>\n<!--kg-card-end: markdown-->","comment_id":"55","plaintext":"JSX\nVirtual DOM\n\nJavascript\n> The simple answer is that this refers to an instance of IceCreamGuy. The less\nsimple answer is that this refers to the object on which this's enclosing\nmethod, in this case .render(), is called. It is almost inevitable that this\nobject will be an instance of IceCreamGuy, but technically it could be something\nelse...\n... You don't need those parentheses because .food is a getter method. You can\ntell this from the get in the above class declaration body.\n\n\nThere's nothing React-specific about getter methods, nor about this behaving in\nthis way! However, in React you will see this used in this way almost\nconstantly.\n\nReact components are complicated. Their syntax is complicated, and the reasoning\nbehind their syntax is especially complicated.\n\ncomponents can interact: a component can render another component.\n\nIn this lesson, you will learn another way that components can interact: a\ncomponent can pass information to another component.\n\nInformation that gets passed from one component to another is known as \"props.\"\n\nHere's how the naming convention works: first, think about what type of event\nyou are listening for. In our example, the event type was \"click.\"\n\nIf you are listening for a \"click\" event, then you name your event handler\nhandleClick. If you are listening for a \"keyPress\" event, then you name your\nevent handler handleKeyPress:\n\nprops is quite possibly the longest and most difficult lesson in all of our\nReact courses. Congratulations on getting this far!\n\nHere are some of the skills that you have learned:\n\nPassing a prop by giving an attribute to a component instance\nAccessing a passed-in prop via this.props.prop-name\nDisplaying a prop\nUsing a prop to make decisions about what to display\nDefining an event handler in a component class\nPassing an event handler as a prop\nReceiving a prop event handler and attaching it to an event listener\nNaming event handlers and event handler attributes according to convention\nthis.props.children\ngetDefaultProps\n\nstate\nUnlike props, a component's state is not passed in from the outside. A component\ndecides its own state.\n\nFor an in-depth explanation of this kind of binding trickery, begin with the\nReact docs. For the less curious, just know that in React, whenever you define\nan event handler that uses this, you need to add this.methodName =\nthis.methodName.bind(this) to your constructor function.\n\nAny time that you call this.setState(), this.setState() AUTOMATICALLY calls\n.render() as soon as the state has changed.\nThat is why you can't call this.setState() from inside of the .render() method!\nthis.setState() automatically calls .render(). If .render() calls\nthis.setState(), then an infinite loop is created.\n\nSource: Codeadamemy\n\nconstructors\n * super() is necessary for constructors to assign methods and values like props \n   to the component.\n * this. is uninitialised without a call to super()\n * super(props)is necessary if you want to access this.props inside the\n   constructor\n\nTODO - google 'types of methods in js' e.g. getter method","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2018-02-05 19:48:11","created_by":"1","updated_at":"2018-02-07 16:01:24","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd10","uuid":"3e7dfa57-82c4-49af-b8ff-c0e891ebc7a1","title":"Data Science IDE for Python","slug":"data-science-ide-for-python","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"- want something like Rstudio\\n\\n- Rodeo - abandonware, which is a shame.\\n\\n- Spyder - doesn't feel right, but not bad.\\n\\n- jupyter lab\\n\\nif you want to work with virtualenvs using jupyter lab... then do this..\\nhttp://anbasile.github.io/programming/2017/06/25/jupyter-venv/\"}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]]}","html":"<!--kg-card-begin: markdown--><ul>\n<li>\n<p>want something like Rstudio</p>\n</li>\n<li>\n<p>Rodeo - abandonware, which is a shame.</p>\n</li>\n<li>\n<p>Spyder - doesn't feel right, but not bad.</p>\n</li>\n<li>\n<p>jupyter lab</p>\n</li>\n</ul>\n<p>if you want to work with virtualenvs using jupyter lab... then do this..<br>\n<a href=\"http://anbasile.github.io/programming/2017/06/25/jupyter-venv/\">http://anbasile.github.io/programming/2017/06/25/jupyter-venv/</a></p>\n<!--kg-card-end: markdown-->","comment_id":"56","plaintext":" * want something like Rstudio\n   \n   \n * Rodeo - abandonware, which is a shame.\n   \n   \n * Spyder - doesn't feel right, but not bad.\n   \n   \n * jupyter lab\n   \n   \n\nif you want to work with virtualenvs using jupyter lab... then do this..\nhttp://anbasile.github.io/programming/2017/06/25/jupyter-venv/","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2018-03-04 18:06:47","created_by":"1","updated_at":"2020-03-19 20:24:21","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd11","uuid":"05a6930f-190c-41b6-a863-6854fb805806","title":"Goodbye Facebook. Hello Signal.","slug":"what-communication-platforms-should-you-use-instead-of-facebooks","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"## Why I left Facebook?\\n\\n## Why Signal?\\n\\n\\nSo you want to get out of the Faceboook Instagram Whatsapp...\\n\\nSignal\\n\\nTelegram\\n\\nWired\\n\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><h2 id=\"whyileftfacebook\">Why I left Facebook?</h2>\n<h2 id=\"whysignal\">Why Signal?</h2>\n<p>So you want to get out of the Faceboook Instagram Whatsapp...</p>\n<p>Signal</p>\n<p>Telegram</p>\n<p>Wired</p>\n<!--kg-card-end: markdown-->","comment_id":"57","plaintext":"Why I left Facebook?\nWhy Signal?\nSo you want to get out of the Faceboook Instagram Whatsapp...\n\nSignal\n\nTelegram\n\nWired","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2018-04-15 19:18:51","created_by":"1","updated_at":"2018-05-05 07:53:09","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd12","uuid":"d12de445-f7c1-48ba-a798-34db69c5c3ae","title":"How to setup pyspark (with JupyterLab)","slug":"lkasdjflksajdf","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"## Prerequsite\\nEnsure you have Java 8 or greater:\\n```\\n$ java -version\\njava version \\\"1.8.0_171\\\"\\nJava(TM) SE Runtime Environment (build 1.8.0_171-b11)\\nJava HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)\\n```\\n\\n## Step 1 - Download the latest Apache Spark\\n[Download the latest Apache Spark](https://spark.apache.org/downloads.html) pre-built for the latest version of Apache Hadoop.\\n\\n```bash\\n# Unzip\\n$ tar -xzf spark-1.2.0-bin-hadoop2.4.tgz\\n# Where you move it to is up to you\\n$ mv spark-2.3.1-bin-hadoop2.7 ~/spark\\n```\\n\\n## Step 2 - Export the path for Bash\\nIn other words, tell your terminal where to find spark.\\n\\n```bash\\nexport SPARK_HOME=~/spark\\nexport PATH=$SPARK_HOME/bin:$PATH\\n```\\n\\nAt this point you should be able to launch pyspark form your terminal.\\n```bash\\n$ pyspark\\nPython 2.7.15 (default, Jul 23 2018, 21:27:06)\\n[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)] on darwin\\nType \\\"help\\\", \\\"copyright\\\", \\\"credits\\\" or \\\"license\\\" for more information.\\n2018-08-12 16:14:53 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\\nSetting default log level to \\\"WARN\\\".\\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\\nWelcome to\\n      ____              __\\n     / __/__  ___ _____/ /__\\n    _\\\\ \\\\/ _ \\\\/ _ `/ __/  '_/\\n   /__ / .__/\\\\_,_/_/ /_/\\\\_\\\\   version 2.3.1\\n      /_/\\n\\nUsing Python version 2.7.15 (default, Jul 23 2018 21:27:06)\\nSparkSession available as 'spark'.\\n```\\n\\nIf like me pyspark launch with python2.7 fix it by running the following command:\\n```bash\\nexport PYSPARK_PYTHON=python3\\n```\\n\\n## Step 3 - Launch pyspark with JupyterLab\\n[JupyterLab](http://jupyterlab.readthedocs.io/en/stable/) is my preferred Python IDE when it comes to working with data. To install JupyterLab see [these instructions](https://github.com/jupyterlab/jupyterlab#installation). There are two options to launch with JupyterLab.\\n\\nYou can test whether you have successfully launched pyspark by using the Pi Estimation code from [Apache Spark's docs](https://spark.apache.org/examples.html) (small adjustments made for python3):\\n```\\n# python 2\\nimport random\\nNUM_SAMPLES = 1000000\\n\\ndef inside(p):\\n    x, y = random.random(), random.random()\\n    return x*x + y*y < 1\\n\\ncount = sc.parallelize(xrange(0, NUM_SAMPLES)) \\\\\\n             .filter(inside).count()\\nprint \\\"Pi is roughly %f\\\" % (4.0 * count / NUM_SAMPLES)\\n```\\n\\n\\n\\n### Option 1 - FindSpark package (recommended)\\nFindspark adds pyspark to sys.path at runtime.\\nInstall `findspark`:\\n\\n```\\n$ pip3 install findspark\\n```\\n\\nThis option is recommended as a limitation of JupyterLab is you can't link a text file to an existing Console per Option 2. This approach starts with an editor (e.g. cool_spark_code.py) and when you right-click you can select the option *Create Console for Editor*. \\n\\n### Option 2 - Export the Pyspark Driver for Python\\nThis tells your terminal what to launch with pyspark.\\n```\\nexport PYSPARK_DRIVER_PYTHON=\\\"jupyter\\\"\\n`\\n```\\n\\n\\n\\n## Resources\\n- [Jupyter Lab Github Issue](https://github.com/jupyterlab/jupyterlab/issues/3334)\\n- [Get Started with PySpark and Jupyter Notebook in 3 Minutes](https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f)\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><h2 id=\"prerequsite\">Prerequsite</h2>\n<p>Ensure you have Java 8 or greater:</p>\n<pre><code>$ java -version\njava version &quot;1.8.0_171&quot;\nJava(TM) SE Runtime Environment (build 1.8.0_171-b11)\nJava HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)\n</code></pre>\n<h2 id=\"step1downloadthelatestapachespark\">Step 1 - Download the latest Apache Spark</h2>\n<p><a href=\"https://spark.apache.org/downloads.html\">Download the latest Apache Spark</a> pre-built for the latest version of Apache Hadoop.</p>\n<pre><code class=\"language-bash\"># Unzip\n$ tar -xzf spark-1.2.0-bin-hadoop2.4.tgz\n# Where you move it to is up to you\n$ mv spark-2.3.1-bin-hadoop2.7 ~/spark\n</code></pre>\n<h2 id=\"step2exportthepathforbash\">Step 2 - Export the path for Bash</h2>\n<p>In other words, tell your terminal where to find spark.</p>\n<pre><code class=\"language-bash\">export SPARK_HOME=~/spark\nexport PATH=$SPARK_HOME/bin:$PATH\n</code></pre>\n<p>At this point you should be able to launch pyspark form your terminal.</p>\n<pre><code class=\"language-bash\">$ pyspark\nPython 2.7.15 (default, Jul 23 2018, 21:27:06)\n[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)] on darwin\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n2018-08-12 16:14:53 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to &quot;WARN&quot;.\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n      /_/\n\nUsing Python version 2.7.15 (default, Jul 23 2018 21:27:06)\nSparkSession available as 'spark'.\n</code></pre>\n<p>If like me pyspark launch with python2.7 fix it by running the following command:</p>\n<pre><code class=\"language-bash\">export PYSPARK_PYTHON=python3\n</code></pre>\n<h2 id=\"step3launchpysparkwithjupyterlab\">Step 3 - Launch pyspark with JupyterLab</h2>\n<p><a href=\"http://jupyterlab.readthedocs.io/en/stable/\">JupyterLab</a> is my preferred Python IDE when it comes to working with data. To install JupyterLab see <a href=\"https://github.com/jupyterlab/jupyterlab#installation\">these instructions</a>. There are two options to launch with JupyterLab.</p>\n<p>You can test whether you have successfully launched pyspark by using the Pi Estimation code from <a href=\"https://spark.apache.org/examples.html\">Apache Spark's docs</a> (small adjustments made for python3):</p>\n<pre><code># python 2\nimport random\nNUM_SAMPLES = 1000000\n\ndef inside(p):\n    x, y = random.random(), random.random()\n    return x*x + y*y &lt; 1\n\ncount = sc.parallelize(xrange(0, NUM_SAMPLES)) \\\n             .filter(inside).count()\nprint &quot;Pi is roughly %f&quot; % (4.0 * count / NUM_SAMPLES)\n</code></pre>\n<h3 id=\"option1findsparkpackagerecommended\">Option 1 - FindSpark package (recommended)</h3>\n<p>Findspark adds pyspark to sys.path at runtime.<br>\nInstall <code>findspark</code>:</p>\n<pre><code>$ pip3 install findspark\n</code></pre>\n<p>This option is recommended as a limitation of JupyterLab is you can't link a text file to an existing Console per Option 2. This approach starts with an editor (e.g. cool_spark_code.py) and when you right-click you can select the option <em>Create Console for Editor</em>.</p>\n<h3 id=\"option2exportthepysparkdriverforpython\">Option 2 - Export the Pyspark Driver for Python</h3>\n<p>This tells your terminal what to launch with pyspark.</p>\n<pre><code>export PYSPARK_DRIVER_PYTHON=&quot;jupyter&quot;\n`\n</code></pre>\n<h2 id=\"resources\">Resources</h2>\n<ul>\n<li><a href=\"https://github.com/jupyterlab/jupyterlab/issues/3334\">Jupyter Lab Github Issue</a></li>\n<li><a href=\"https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f\">Get Started with PySpark and Jupyter Notebook in 3 Minutes</a></li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"58","plaintext":"Prerequsite\nEnsure you have Java 8 or greater:\n\n$ java -version\njava version \"1.8.0_171\"\nJava(TM) SE Runtime Environment (build 1.8.0_171-b11)\nJava HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode)\n\n\nStep 1 - Download the latest Apache Spark\nDownload the latest Apache Spark [https://spark.apache.org/downloads.html] \npre-built for the latest version of Apache Hadoop.\n\n# Unzip\n$ tar -xzf spark-1.2.0-bin-hadoop2.4.tgz\n# Where you move it to is up to you\n$ mv spark-2.3.1-bin-hadoop2.7 ~/spark\n\n\nStep 2 - Export the path for Bash\nIn other words, tell your terminal where to find spark.\n\nexport SPARK_HOME=~/spark\nexport PATH=$SPARK_HOME/bin:$PATH\n\n\nAt this point you should be able to launch pyspark form your terminal.\n\n$ pyspark\nPython 2.7.15 (default, Jul 23 2018, 21:27:06)\n[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n2018-08-12 16:14:53 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n      /_/\n\nUsing Python version 2.7.15 (default, Jul 23 2018 21:27:06)\nSparkSession available as 'spark'.\n\n\nIf like me pyspark launch with python2.7 fix it by running the following\ncommand:\n\nexport PYSPARK_PYTHON=python3\n\n\nStep 3 - Launch pyspark with JupyterLab\nJupyterLab [http://jupyterlab.readthedocs.io/en/stable/] is my preferred Python\nIDE when it comes to working with data. To install JupyterLab see these\ninstructions [https://github.com/jupyterlab/jupyterlab#installation]. There are\ntwo options to launch with JupyterLab.\n\nYou can test whether you have successfully launched pyspark by using the Pi\nEstimation code from Apache Spark's docs\n[https://spark.apache.org/examples.html] (small adjustments made for python3):\n\n# python 2\nimport random\nNUM_SAMPLES = 1000000\n\ndef inside(p):\n    x, y = random.random(), random.random()\n    return x*x + y*y < 1\n\ncount = sc.parallelize(xrange(0, NUM_SAMPLES)) \\\n             .filter(inside).count()\nprint \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n\n\nOption 1 - FindSpark package (recommended)\nFindspark adds pyspark to sys.path at runtime.\nInstall findspark:\n\n$ pip3 install findspark\n\n\nThis option is recommended as a limitation of JupyterLab is you can't link a\ntext file to an existing Console per Option 2. This approach starts with an\neditor (e.g. cool_spark_code.py) and when you right-click you can select the\noption Create Console for Editor.\n\nOption 2 - Export the Pyspark Driver for Python\nThis tells your terminal what to launch with pyspark.\n\nexport PYSPARK_DRIVER_PYTHON=\"jupyter\"\n`\n\n\nResources\n * Jupyter Lab Github Issue\n   [https://github.com/jupyterlab/jupyterlab/issues/3334]\n * Get Started with PySpark and Jupyter Notebook in 3 Minutes\n   [https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f]","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2018-05-17 05:26:40","created_by":"1","updated_at":"2019-03-08 14:01:28","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd13","uuid":"fbdb5fa6-bad8-488e-a100-8b230b324e2c","title":"Notes on Maintaining a Blog","slug":"notes-on-maintaining-a-blog","mobiledoc":"{\"version\":\"0.3.2\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"\\n## Chrome Page Audit\\nUse Chrome's Page Audit tool to get a sense of where to get the biggest impact to optimise your website.\\n\\n## Image compression\\n`jpegoptim image.jpg` will losslessly compress an image and replace the file.\\n\\nReach a target kilabyte size using the size parameter `jpegoptim image.jpg -S 128`.\\n\\n[Further usage](http://ask.xmodulo.com/compress-jpeg-images-command-line-linux.html).\\n\\n## Transferring local files to server\\n`scp image.jpg user@103.276.25.13:/var/www/ghost/content/images/2017/07/`\\n\\n\\n## notes on running in a container\\nhttps://blog.dteuchert.com/create-your-blog-with-ghost/\\nhttps://blog.alexellis.io/keeping-shipping-your-blog/\\n\\n\\n## Upgrade from V0 to V3 notes\\n    upgrade 0.5.10 to v1.25.5 [releases]https://github.com/TryGhost/Ghost/releases?after=1.24.2:\\n    https://www.ghostforbeginners.com/how-to-upgrade-from-ghost-0-11-x-to-ghost-1-0/\\n        test docker upgrade locally\\n        https://ghost.org/faq/upgrade-to-ghost-1-0/\\n            Backup content \\n                Export the blog settings and data (exports a json) from website: https://lukesingham.com/ghost/settings/labs/\\n                backup content folder  scp -r root@104.236.76.13:/var/www/ghost/content .\\n            build docker image https://hub.docker.com/_/ghost/\\n                `docker pull ghost:1.26.2`\\n                https://github.com/docker-library/ghost/blob/e28a0e3482e710ed353fc2eecd8a4cf435fea045/1/debian/Dockerfile\\n            run v1.25.5 locally in docker\\n            docker run -d -p 3001:2368 -v /Users/lsingham/projects/lukesingham.com/contentBAK2018-09-15/content/images:/var/lib/ghost/content/images ghost-1.25.5\\n                mount content/images \\n                    docker run -d -p 3001:2368 -v /Users/lsingham/projects/lukesingham.com/contentBAK2018-09-15/content/images:/var/lib/ghost/content/images ghost-1.25.5\\n            run import of json backup on local v1.25.5\\n                log into the new ghost interface by following the prompts\\n                n.b warnings on import are fine as auto fallback to user created\\n            delete default posts by deleting ghost user\\n            issue with churn R post - since embedded html in the post\\n                remove header div classes <div id=\\\"logistic\\\" class=\\\"section level3\\\">\\n                    And closing divs\\n                fix saved Users/lsingham/projects/lukesingham.com/R-churn-post-FIX.txt\\n                    not perfect, div width inheritance seems a little screwed:\\n                        check the closing divs?\\n                        start again\\n                            delete import from currently running docker? else re-run all steps up until here\\n            install docker on vps\\n                wget -qO- https://get.docker.com/ | sh # not sure if this gets the right version\\n                sudo usermod -aG docker $(whoami)\\n                sudo apt-get -y install python-pip\\n                docker pull ghost:1.26.2\\n                fix before runnign https://stackoverflow.com/questions/55746291/oci-runtimecontainer-linux-go348-error-in-docker\\n            save and transfer image to vps? \\n            deploy v1\\n            code highlighting not working\\n    is certbot\\n        backup in repo\\n            nginx config\\n            certbot script\\n            /etc/cron.d/certbot\\n    now ghost is in a container, upgrade server from ubuntu 12 -> 16\\n        nginx config\\n        setup certbot https://certbot.eff.org/lets-encrypt/ubuntuother-nginx\\n        install docker\\n        run ghost docker\\n        sync files\\n    blog is running with:\\n    docker run -d -e url=https://lukesingham.com -p 3001:2368 --restart=always -v /var/www/ghost/content/images:/var/lib/ghost/content/images -t ghost:3.2.0-alpine\\n    backup\\n        export current json\\n        test locally latest image\\n            docker pull ghost:3.13.1-alpine\\n            copy content folder from serverii\\n            scp -r root@104.236.76.13:/var/www/ghost/content .\\n            docker run -d -e url=https://lukesingham.com -p 3001:2368 --restart=always -v content/images:/var/lib/ghost/content -t 3.13.1-alpine\\n            run image\\n            docker run -d -p 3001:2368 --restart=always -v content/images:/var/lib/ghost/content -t ghost:3.13.1-alpine\\n        update on server\\n            docker pull ghost:3.13.1-alpine\\n            stop container\\n            clear docker crud\\n                docker ps\\n                docker system prune -a\\n            docker run -d -e url=https://lukesingham.com -p 3001:2368 --restart=always -v /var/www/ghost/content:/var/lib/ghost/content -t ghost:3.13.1-alpine\\n\"}]],\"markups\":[],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><h2 id=\"chromepageaudit\">Chrome Page Audit</h2>\n<p>Use Chrome's Page Audit tool to get a sense of where to get the biggest impact to optimise your website.</p>\n<h2 id=\"imagecompression\">Image compression</h2>\n<p><code>jpegoptim image.jpg</code> will losslessly compress an image and replace the file.</p>\n<p>Reach a target kilabyte size using the size parameter <code>jpegoptim image.jpg -S 128</code>.</p>\n<p><a href=\"http://ask.xmodulo.com/compress-jpeg-images-command-line-linux.html\">Further usage</a>.</p>\n<h2 id=\"transferringlocalfilestoserver\">Transferring local files to server</h2>\n<p><code>scp image.jpg user@103.276.25.13:/var/www/ghost/content/images/2017/07/</code></p>\n<h2 id=\"notesonrunninginacontainer\">notes on running in a container</h2>\n<p><a href=\"https://blog.dteuchert.com/create-your-blog-with-ghost/\">https://blog.dteuchert.com/create-your-blog-with-ghost/</a><br>\n<a href=\"https://blog.alexellis.io/keeping-shipping-your-blog/\">https://blog.alexellis.io/keeping-shipping-your-blog/</a></p>\n<h2 id=\"upgradefromv0tov3notes\">Upgrade from V0 to V3 notes</h2>\n<pre><code>upgrade 0.5.10 to v1.25.5 [releases]https://github.com/TryGhost/Ghost/releases?after=1.24.2:\nhttps://www.ghostforbeginners.com/how-to-upgrade-from-ghost-0-11-x-to-ghost-1-0/\n    test docker upgrade locally\n    https://ghost.org/faq/upgrade-to-ghost-1-0/\n        Backup content \n            Export the blog settings and data (exports a json) from website: https://lukesingham.com/ghost/settings/labs/\n            backup content folder  scp -r root@104.236.76.13:/var/www/ghost/content .\n        build docker image https://hub.docker.com/_/ghost/\n            `docker pull ghost:1.26.2`\n            https://github.com/docker-library/ghost/blob/e28a0e3482e710ed353fc2eecd8a4cf435fea045/1/debian/Dockerfile\n        run v1.25.5 locally in docker\n        docker run -d -p 3001:2368 -v /Users/lsingham/projects/lukesingham.com/contentBAK2018-09-15/content/images:/var/lib/ghost/content/images ghost-1.25.5\n            mount content/images \n                docker run -d -p 3001:2368 -v /Users/lsingham/projects/lukesingham.com/contentBAK2018-09-15/content/images:/var/lib/ghost/content/images ghost-1.25.5\n        run import of json backup on local v1.25.5\n            log into the new ghost interface by following the prompts\n            n.b warnings on import are fine as auto fallback to user created\n        delete default posts by deleting ghost user\n        issue with churn R post - since embedded html in the post\n            remove header div classes &lt;div id=&quot;logistic&quot; class=&quot;section level3&quot;&gt;\n                And closing divs\n            fix saved Users/lsingham/projects/lukesingham.com/R-churn-post-FIX.txt\n                not perfect, div width inheritance seems a little screwed:\n                    check the closing divs?\n                    start again\n                        delete import from currently running docker? else re-run all steps up until here\n        install docker on vps\n            wget -qO- https://get.docker.com/ | sh # not sure if this gets the right version\n            sudo usermod -aG docker $(whoami)\n            sudo apt-get -y install python-pip\n            docker pull ghost:1.26.2\n            fix before runnign https://stackoverflow.com/questions/55746291/oci-runtimecontainer-linux-go348-error-in-docker\n        save and transfer image to vps? \n        deploy v1\n        code highlighting not working\nis certbot\n    backup in repo\n        nginx config\n        certbot script\n        /etc/cron.d/certbot\nnow ghost is in a container, upgrade server from ubuntu 12 -&gt; 16\n    nginx config\n    setup certbot https://certbot.eff.org/lets-encrypt/ubuntuother-nginx\n    install docker\n    run ghost docker\n    sync files\nblog is running with:\ndocker run -d -e url=https://lukesingham.com -p 3001:2368 --restart=always -v /var/www/ghost/content/images:/var/lib/ghost/content/images -t ghost:3.2.0-alpine\nbackup\n    export current json\n    test locally latest image\n        docker pull ghost:3.13.1-alpine\n        copy content folder from serverii\n        scp -r root@104.236.76.13:/var/www/ghost/content .\n        docker run -d -e url=https://lukesingham.com -p 3001:2368 --restart=always -v content/images:/var/lib/ghost/content -t 3.13.1-alpine\n        run image\n        docker run -d -p 3001:2368 --restart=always -v content/images:/var/lib/ghost/content -t ghost:3.13.1-alpine\n    update on server\n        docker pull ghost:3.13.1-alpine\n        stop container\n        clear docker crud\n            docker ps\n            docker system prune -a\n        docker run -d -e url=https://lukesingham.com -p 3001:2368 --restart=always -v /var/www/ghost/content:/var/lib/ghost/content -t ghost:3.13.1-alpine\n</code></pre>\n<!--kg-card-end: markdown-->","comment_id":"59","plaintext":"Chrome Page Audit\nUse Chrome's Page Audit tool to get a sense of where to get the biggest impact\nto optimise your website.\n\nImage compression\njpegoptim image.jpg will losslessly compress an image and replace the file.\n\nReach a target kilabyte size using the size parameter jpegoptim image.jpg -S 128\n.\n\nFurther usage\n[http://ask.xmodulo.com/compress-jpeg-images-command-line-linux.html].\n\nTransferring local files to server\nscp image.jpg user@103.276.25.13:/var/www/ghost/content/images/2017/07/\n\nnotes on running in a container\nhttps://blog.dteuchert.com/create-your-blog-with-ghost/\nhttps://blog.alexellis.io/keeping-shipping-your-blog/\n\nUpgrade from V0 to V3 notes\nupgrade 0.5.10 to v1.25.5 [releases]https://github.com/TryGhost/Ghost/releases?after=1.24.2:\nhttps://www.ghostforbeginners.com/how-to-upgrade-from-ghost-0-11-x-to-ghost-1-0/\n    test docker upgrade locally\n    https://ghost.org/faq/upgrade-to-ghost-1-0/\n        Backup content \n            Export the blog settings and data (exports a json) from website: https://lukesingham.com/ghost/settings/labs/\n            backup content folder  scp -r root@104.236.76.13:/var/www/ghost/content .\n        build docker image https://hub.docker.com/_/ghost/\n            `docker pull ghost:1.26.2`\n            https://github.com/docker-library/ghost/blob/e28a0e3482e710ed353fc2eecd8a4cf435fea045/1/debian/Dockerfile\n        run v1.25.5 locally in docker\n        docker run -d -p 3001:2368 -v /Users/lsingham/projects/lukesingham.com/contentBAK2018-09-15/content/images:/var/lib/ghost/content/images ghost-1.25.5\n            mount content/images \n                docker run -d -p 3001:2368 -v /Users/lsingham/projects/lukesingham.com/contentBAK2018-09-15/content/images:/var/lib/ghost/content/images ghost-1.25.5\n        run import of json backup on local v1.25.5\n            log into the new ghost interface by following the prompts\n            n.b warnings on import are fine as auto fallback to user created\n        delete default posts by deleting ghost user\n        issue with churn R post - since embedded html in the post\n            remove header div classes <div id=\"logistic\" class=\"section level3\">\n                And closing divs\n            fix saved Users/lsingham/projects/lukesingham.com/R-churn-post-FIX.txt\n                not perfect, div width inheritance seems a little screwed:\n                    check the closing divs?\n                    start again\n                        delete import from currently running docker? else re-run all steps up until here\n        install docker on vps\n            wget -qO- https://get.docker.com/ | sh # not sure if this gets the right version\n            sudo usermod -aG docker $(whoami)\n            sudo apt-get -y install python-pip\n            docker pull ghost:1.26.2\n            fix before runnign https://stackoverflow.com/questions/55746291/oci-runtimecontainer-linux-go348-error-in-docker\n        save and transfer image to vps? \n        deploy v1\n        code highlighting not working\nis certbot\n    backup in repo\n        nginx config\n        certbot script\n        /etc/cron.d/certbot\nnow ghost is in a container, upgrade server from ubuntu 12 -> 16\n    nginx config\n    setup certbot https://certbot.eff.org/lets-encrypt/ubuntuother-nginx\n    install docker\n    run ghost docker\n    sync files\nblog is running with:\ndocker run -d -e url=https://lukesingham.com -p 3001:2368 --restart=always -v /var/www/ghost/content/images:/var/lib/ghost/content/images -t ghost:3.2.0-alpine\nbackup\n    export current json\n    test locally latest image\n        docker pull ghost:3.13.1-alpine\n        copy content folder from serverii\n        scp -r root@104.236.76.13:/var/www/ghost/content .\n        docker run -d -e url=https://lukesingham.com -p 3001:2368 --restart=always -v content/images:/var/lib/ghost/content -t 3.13.1-alpine\n        run image\n        docker run -d -p 3001:2368 --restart=always -v content/images:/var/lib/ghost/content -t ghost:3.13.1-alpine\n    update on server\n        docker pull ghost:3.13.1-alpine\n        stop container\n        clear docker crud\n            docker ps\n            docker system prune -a\n        docker run -d -e url=https://lukesingham.com -p 3001:2368 --restart=always -v /var/www/ghost/content:/var/lib/ghost/content -t ghost:3.13.1-alpine","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2019-07-27 21:11:37","created_by":"1","updated_at":"2020-04-13 10:11:56","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd14","uuid":"254829ea-ffaa-4654-89bb-a0376641128b","title":"Open Source Data Analytics","slug":"open-source-data-analytics","mobiledoc":"{\"version\":\"0.3.2\",\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"https://github.com/dremio/\\n\\n### ETL\\nHas open source marketplace of sources (taps) and datawarehouse (targets)  `tap | target`\\nhttps://github.com/singer-io\\n\\nGitlab's development in the space\\nhttps://gitlab.com/meltano/meltano\\n\\nhttps://ananasanalytics.com/\\n\\nhttps://www.getdbt.com/product/\\n\\n\\n### Data pipeline tools\\n\\n#### Job Orchestration\\nhttps://github.com/apache/airflow\\n\\n#### DAG creation / testing / documentation\\nhttps://www.getdbt.com/\\npossible alternative to dbt https://www.liquibase.org/\\nhttps://github.com/dataform-co/dataform\\n\\n\\n### Data Versioning / Data Science Pipelining\\nhttps://github.com/pachyderm/pachyderm\\n\\n\\n### Metadata\\n\\n\\n### Data Visualisation / Exploration [Self service]\\n\\nhttps://github.com/getredash/redash\\n\\nBI viz/dashboards for SQL lovers\\nhttps://github.com/shzlw/poli\\n\\nAirbnb's superset, now under the Apache Foundation\\nhttps://github.com/apache/incubator-superset\\n\\nLike Apache Superset\\nhttps://www.metabase.com/\\n\\nA client side (browser) app for users to dump spreadsheet data in and usea gui to create D3 viz\\nhttps://github.com/rawgraphs/raw\\n\\n### Custom dashboards\\n[Python - Dash](https://github.com/ucg8j/awesome-dash)\\n[R - Shiny](https://github.com/grabear/awesome-rshiny)\\n\\n### Containerised Middleware Application\\nShinyproxy\\n\\n\"}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]]}","html":"<!--kg-card-begin: markdown--><p><a href=\"https://github.com/dremio/\">https://github.com/dremio/</a></p>\n<h3 id=\"etl\">ETL</h3>\n<p>Has open source marketplace of sources (taps) and datawarehouse (targets)  <code>tap | target</code><br>\n<a href=\"https://github.com/singer-io\">https://github.com/singer-io</a></p>\n<p>Gitlab's development in the space<br>\n<a href=\"https://gitlab.com/meltano/meltano\">https://gitlab.com/meltano/meltano</a></p>\n<p><a href=\"https://ananasanalytics.com/\">https://ananasanalytics.com/</a></p>\n<p><a href=\"https://www.getdbt.com/product/\">https://www.getdbt.com/product/</a></p>\n<h3 id=\"datapipelinetools\">Data pipeline tools</h3>\n<h4 id=\"joborchestration\">Job Orchestration</h4>\n<p><a href=\"https://github.com/apache/airflow\">https://github.com/apache/airflow</a></p>\n<h4 id=\"dagcreationtestingdocumentation\">DAG creation / testing / documentation</h4>\n<p><a href=\"https://www.getdbt.com/\">https://www.getdbt.com/</a><br>\npossible alternative to dbt <a href=\"https://www.liquibase.org/\">https://www.liquibase.org/</a><br>\n<a href=\"https://github.com/dataform-co/dataform\">https://github.com/dataform-co/dataform</a></p>\n<h3 id=\"dataversioningdatasciencepipelining\">Data Versioning / Data Science Pipelining</h3>\n<p><a href=\"https://github.com/pachyderm/pachyderm\">https://github.com/pachyderm/pachyderm</a></p>\n<h3 id=\"metadata\">Metadata</h3>\n<h3 id=\"datavisualisationexplorationselfservice\">Data Visualisation / Exploration [Self service]</h3>\n<p><a href=\"https://github.com/getredash/redash\">https://github.com/getredash/redash</a></p>\n<p>BI viz/dashboards for SQL lovers<br>\n<a href=\"https://github.com/shzlw/poli\">https://github.com/shzlw/poli</a></p>\n<p>Airbnb's superset, now under the Apache Foundation<br>\n<a href=\"https://github.com/apache/incubator-superset\">https://github.com/apache/incubator-superset</a></p>\n<p>Like Apache Superset<br>\n<a href=\"https://www.metabase.com/\">https://www.metabase.com/</a></p>\n<p>A client side (browser) app for users to dump spreadsheet data in and usea gui to create D3 viz<br>\n<a href=\"https://github.com/rawgraphs/raw\">https://github.com/rawgraphs/raw</a></p>\n<h3 id=\"customdashboards\">Custom dashboards</h3>\n<p><a href=\"https://github.com/ucg8j/awesome-dash\">Python - Dash</a><br>\n<a href=\"https://github.com/grabear/awesome-rshiny\">R - Shiny</a></p>\n<h3 id=\"containerisedmiddlewareapplication\">Containerised Middleware Application</h3>\n<p>Shinyproxy</p>\n<!--kg-card-end: markdown-->","comment_id":"60","plaintext":"https://github.com/dremio/\n\nETL\nHas open source marketplace of sources (taps) and datawarehouse (targets) tap |\ntarget\nhttps://github.com/singer-io\n\nGitlab's development in the space\nhttps://gitlab.com/meltano/meltano\n\nhttps://ananasanalytics.com/\n\nhttps://www.getdbt.com/product/\n\nData pipeline tools\nJob Orchestration\nhttps://github.com/apache/airflow\n\nDAG creation / testing / documentation\nhttps://www.getdbt.com/\npossible alternative to dbt https://www.liquibase.org/\nhttps://github.com/dataform-co/dataform\n\nData Versioning / Data Science Pipelining\nhttps://github.com/pachyderm/pachyderm\n\nMetadata\nData Visualisation / Exploration [Self service]\nhttps://github.com/getredash/redash\n\nBI viz/dashboards for SQL lovers\nhttps://github.com/shzlw/poli\n\nAirbnb's superset, now under the Apache Foundation\nhttps://github.com/apache/incubator-superset\n\nLike Apache Superset\nhttps://www.metabase.com/\n\nA client side (browser) app for users to dump spreadsheet data in and usea gui\nto create D3 viz\nhttps://github.com/rawgraphs/raw\n\nCustom dashboards\nPython - Dash [https://github.com/ucg8j/awesome-dash]\nR - Shiny [https://github.com/grabear/awesome-rshiny]\n\nContainerised Middleware Application\nShinyproxy","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2019-07-27 21:45:35","created_by":"1","updated_at":"2020-09-02 17:26:28","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd15","uuid":"b46d7208-a37d-4516-bd75-40bd64e1637b","title":"How to get your flight data","slug":"how-to-get-your-flight-data","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Assumption:\\n- you use gmail\\n\\nOne advantage of having the Google mothership scanning all of your emails is to take advantage of getting what they know about you back. A little known piece of data google gives you access to is all Reservations, Flights and Purchases.\\n\\nOnce downloaded, how do I filter the jsons to flights only? Unfortunately Google doesn't make this easy by naming flight files with, I don't know, 'flight_xxxx.json'. Instead, looking at the jsons, the keys inside help e.g. it's very likely I am getting only flights if I search for orders with the key 'flightReservation'.\\n\\n```bash\\n# create a destination folder for flights\\nmkdir only_flights\\n\\n# filter jsons, ensure unique and copy to only_flights\\ngrep -il 'flightReservation' * | \\\\\\n  sort | \\\\\\n  uniq | \\\\\\n  xargs -I{} cp {} only_flights/\\n  \\n# how many flights is that?\\nls | wc -l\\n#195\\n```\\n\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Assumption:</p>\n<ul>\n<li>you use gmail</li>\n</ul>\n<p>One advantage of having the Google mothership scanning all of your emails is to take advantage of getting what they know about you back. A little known piece of data google gives you access to is all Reservations, Flights and Purchases.</p>\n<p>Once downloaded, how do I filter the jsons to flights only? Unfortunately Google doesn't make this easy by naming flight files with, I don't know, 'flight_xxxx.json'. Instead, looking at the jsons, the keys inside help e.g. it's very likely I am getting only flights if I search for orders with the key 'flightReservation'.</p>\n<pre><code class=\"language-bash\"># create a destination folder for flights\nmkdir only_flights\n\n# filter jsons, ensure unique and copy to only_flights\ngrep -il 'flightReservation' * | \\\n  sort | \\\n  uniq | \\\n  xargs -I{} cp {} only_flights/\n  \n# how many flights is that?\nls | wc -l\n#195\n</code></pre>\n<!--kg-card-end: markdown-->","comment_id":"61","plaintext":"Assumption:\n\n * you use gmail\n\nOne advantage of having the Google mothership scanning all of your emails is to\ntake advantage of getting what they know about you back. A little known piece of\ndata google gives you access to is all Reservations, Flights and Purchases.\n\nOnce downloaded, how do I filter the jsons to flights only? Unfortunately Google\ndoesn't make this easy by naming flight files with, I don't know,\n'flight_xxxx.json'. Instead, looking at the jsons, the keys inside help e.g.\nit's very likely I am getting only flights if I search for orders with the key\n'flightReservation'.\n\n# create a destination folder for flights\nmkdir only_flights\n\n# filter jsons, ensure unique and copy to only_flights\ngrep -il 'flightReservation' * | \\\n  sort | \\\n  uniq | \\\n  xargs -I{} cp {} only_flights/\n  \n# how many flights is that?\nls | wc -l\n#195","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2019-10-10 02:24:15","created_by":"1","updated_at":"2019-10-10 02:36:11","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd16","uuid":"a0fa06f2-06f6-4162-96d5-fc988e9fd8f6","title":"Indoor Environmental Monitoring with Raspberry Pi and Enviro+","slug":"indoor-environmental-monitoring-with-raspberry-pi-and-enviro","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Aim: Setup an indoor environmental monitoring, to capture data on volume, gas, pollutants, noice etc\\n\\nGetting started with a Raspberry Pi Zero W v1.1 - this version includes wireless and bluetooth. \\nI currently have just the board and power adaptor. There's no way to connect a keyboard or SSH into the RPi at this point. What I need is some essential components:\\n\\n- Storage: SD Card 64gb\\n- Hardware Connectivity: [40-pin male GPIO header](https://shop.pimoroni.com/products/male-40-pin-2x20-hat-header?variant=10476117383)\\n- [2 x 2Pack Standoffs](https://shop.pimoroni.com/products/brass-m2-5-standoffs-for-pi-hats-black-plated-pack-of-2)\\n- [ethical solder](https://shop.pimoroni.com/products/fair-loetet?variant=34730900938)\\n\\n- soldering iron https://www.amazon.com/Tabiger-Soldering-110V-Adjustable-Temperature-Welding/dp/B01H1IFT54?SubscriptionId=AKIAJQUMBT74S4ZKCBTA&tag=makeradvisor-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=B01H1IFT54\\n\\nSensors\\nhttps://shop.pimoroni.com/products/bme680-breakout\\nhttps://github.com/pimoroni/bme680-python\\n\\nhttps://shop.pimoroni.com/products/enviro?variant=31155658457171\\nhttps://learn.pimoroni.com/tutorial/sandyj/getting-started-with-enviro-plus\\nhttps://github.com/pimoroni/enviroplus-python\\n\\n\\n\\n## References\\n[getting-started-with-the-raspberry-pi-zero-w-without-a-monitor](https://www.losant.com/blog/getting-started-with-the-raspberry-pi-zero-w-without-a-monitor)\\n\\n[Raspberry Pi System Stats](https://learn.pimoroni.com/tutorial/networked-pi/raspberry-pi-system-stats-python)\\n\\nhttps://github.com/nophead/EnviroPlusWeb/blob/master/app.py\\n\\nhttps://shop.pimoroni.com/products/enviro?variant=31155658457171\\n\\nhttps://forums.pimoroni.com/t/enviroplus-assistance/11595/2\\n\\nhttps://www.balena.io/blog/build-an-environment-and-air-quality-monitor-with-raspberry-pi/\\n\\nhttps://www.balena.io/blog/aggregate-data-from-a-fleet-of-sensors-with-balenasense-and-influxdb/\\n\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Aim: Setup an indoor environmental monitoring, to capture data on volume, gas, pollutants, noice etc</p>\n<p>Getting started with a Raspberry Pi Zero W v1.1 - this version includes wireless and bluetooth.<br>\nI currently have just the board and power adaptor. There's no way to connect a keyboard or SSH into the RPi at this point. What I need is some essential components:</p>\n<ul>\n<li>\n<p>Storage: SD Card 64gb</p>\n</li>\n<li>\n<p>Hardware Connectivity: <a href=\"https://shop.pimoroni.com/products/male-40-pin-2x20-hat-header?variant=10476117383\">40-pin male GPIO header</a></p>\n</li>\n<li>\n<p><a href=\"https://shop.pimoroni.com/products/brass-m2-5-standoffs-for-pi-hats-black-plated-pack-of-2\">2 x 2Pack Standoffs</a></p>\n</li>\n<li>\n<p><a href=\"https://shop.pimoroni.com/products/fair-loetet?variant=34730900938\">ethical solder</a></p>\n</li>\n<li>\n<p>soldering iron <a href=\"https://www.amazon.com/Tabiger-Soldering-110V-Adjustable-Temperature-Welding/dp/B01H1IFT54?SubscriptionId=AKIAJQUMBT74S4ZKCBTA&amp;tag=makeradvisor-20&amp;linkCode=xm2&amp;camp=2025&amp;creative=165953&amp;creativeASIN=B01H1IFT54\">https://www.amazon.com/Tabiger-Soldering-110V-Adjustable-Temperature-Welding/dp/B01H1IFT54?SubscriptionId=AKIAJQUMBT74S4ZKCBTA&amp;tag=makeradvisor-20&amp;linkCode=xm2&amp;camp=2025&amp;creative=165953&amp;creativeASIN=B01H1IFT54</a></p>\n</li>\n</ul>\n<p>Sensors<br>\n<a href=\"https://shop.pimoroni.com/products/bme680-breakout\">https://shop.pimoroni.com/products/bme680-breakout</a><br>\n<a href=\"https://github.com/pimoroni/bme680-python\">https://github.com/pimoroni/bme680-python</a></p>\n<p><a href=\"https://shop.pimoroni.com/products/enviro?variant=31155658457171\">https://shop.pimoroni.com/products/enviro?variant=31155658457171</a><br>\n<a href=\"https://learn.pimoroni.com/tutorial/sandyj/getting-started-with-enviro-plus\">https://learn.pimoroni.com/tutorial/sandyj/getting-started-with-enviro-plus</a><br>\n<a href=\"https://github.com/pimoroni/enviroplus-python\">https://github.com/pimoroni/enviroplus-python</a></p>\n<h2 id=\"references\">References</h2>\n<p><a href=\"https://www.losant.com/blog/getting-started-with-the-raspberry-pi-zero-w-without-a-monitor\">getting-started-with-the-raspberry-pi-zero-w-without-a-monitor</a></p>\n<p><a href=\"https://learn.pimoroni.com/tutorial/networked-pi/raspberry-pi-system-stats-python\">Raspberry Pi System Stats</a></p>\n<p><a href=\"https://github.com/nophead/EnviroPlusWeb/blob/master/app.py\">https://github.com/nophead/EnviroPlusWeb/blob/master/app.py</a></p>\n<p><a href=\"https://shop.pimoroni.com/products/enviro?variant=31155658457171\">https://shop.pimoroni.com/products/enviro?variant=31155658457171</a></p>\n<p><a href=\"https://forums.pimoroni.com/t/enviroplus-assistance/11595/2\">https://forums.pimoroni.com/t/enviroplus-assistance/11595/2</a></p>\n<p><a href=\"https://www.balena.io/blog/build-an-environment-and-air-quality-monitor-with-raspberry-pi/\">https://www.balena.io/blog/build-an-environment-and-air-quality-monitor-with-raspberry-pi/</a></p>\n<p><a href=\"https://www.balena.io/blog/aggregate-data-from-a-fleet-of-sensors-with-balenasense-and-influxdb/\">https://www.balena.io/blog/aggregate-data-from-a-fleet-of-sensors-with-balenasense-and-influxdb/</a></p>\n<!--kg-card-end: markdown-->","comment_id":"62","plaintext":"Aim: Setup an indoor environmental monitoring, to capture data on volume, gas,\npollutants, noice etc\n\nGetting started with a Raspberry Pi Zero W v1.1 - this version includes wireless\nand bluetooth.\nI currently have just the board and power adaptor. There's no way to connect a\nkeyboard or SSH into the RPi at this point. What I need is some essential\ncomponents:\n\n * Storage: SD Card 64gb\n   \n   \n * Hardware Connectivity: 40-pin male GPIO header\n   [https://shop.pimoroni.com/products/male-40-pin-2x20-hat-header?variant=10476117383]\n   \n   \n * 2 x 2Pack Standoffs\n   [https://shop.pimoroni.com/products/brass-m2-5-standoffs-for-pi-hats-black-plated-pack-of-2]\n   \n   \n * ethical solder\n   [https://shop.pimoroni.com/products/fair-loetet?variant=34730900938]\n   \n   \n * soldering iron \n   https://www.amazon.com/Tabiger-Soldering-110V-Adjustable-Temperature-Welding/dp/B01H1IFT54?SubscriptionId=AKIAJQUMBT74S4ZKCBTA&tag=makeradvisor-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=B01H1IFT54\n   [https://www.amazon.com/Tabiger-Soldering-110V-Adjustable-Temperature-Welding/dp/B01H1IFT54?SubscriptionId=AKIAJQUMBT74S4ZKCBTA&tag=makeradvisor-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=B01H1IFT54]\n   \n   \n\nSensors\nhttps://shop.pimoroni.com/products/bme680-breakout\nhttps://github.com/pimoroni/bme680-python\n\nhttps://shop.pimoroni.com/products/enviro?variant=31155658457171\nhttps://learn.pimoroni.com/tutorial/sandyj/getting-started-with-enviro-plus\nhttps://github.com/pimoroni/enviroplus-python\n\nReferences\ngetting-started-with-the-raspberry-pi-zero-w-without-a-monitor\n[https://www.losant.com/blog/getting-started-with-the-raspberry-pi-zero-w-without-a-monitor]\n\nRaspberry Pi System Stats\n[https://learn.pimoroni.com/tutorial/networked-pi/raspberry-pi-system-stats-python]\n\nhttps://github.com/nophead/EnviroPlusWeb/blob/master/app.py\n\nhttps://shop.pimoroni.com/products/enviro?variant=31155658457171\n\nhttps://forums.pimoroni.com/t/enviroplus-assistance/11595/2\n\nhttps://www.balena.io/blog/build-an-environment-and-air-quality-monitor-with-raspberry-pi/\n\nhttps://www.balena.io/blog/aggregate-data-from-a-fleet-of-sensors-with-balenasense-and-influxdb/","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2019-11-24 10:50:51","created_by":"1","updated_at":"2019-11-26 22:30:29","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd17","uuid":"d6744e0e-95c1-4d33-8578-2acf40773733","title":"learning notes on HDFS,  YARN, Spark","slug":"historisation-of-a-dataset","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"###HDFS\\n- *Name node* knows where things go on the *data nodes\\n- *There is a *replication factor*, so if a data node goes down data is still available. So when data goes to a data node, the name node sends intructions of which data nodes the recieving data node should replicate to.\\n- how does the Name Node know where everything is? Firstly it has a FS image where everything is. And an edit log of changes that have been made.\\n- if the name node went down you wouldn’t know where to put/get things from HDFS. So there are two name nodes an active and a backup/passive one.\\n\\n* on the name node, run hdfs balancer after pruning to try and recover nodes i.e. reudce them below 85% where they start to fail. Can also run this with different thresholds if necessary (see HDFS Runbook (https://rtfm.palantir.build/docs/hadoop-runbook/master/maintenance/pages/rebalance.html)) and this http://www.informit.com/articles/article.aspx?p=2755708&seqNum=5\\n        * you can increase bandwidth whilst (though why not before) hdfs balancer is running: ./hdfs dfsadmin -setBalancerBandwidth 104857760\\n        * Check the bandwidth available on your ethernet card first ethtool eth0 could be eth1\\n\\n##YARN\\n\\n* Yarn a resource manager.\\n* Containers/executors spin up on boxes. \\n* Resource mgmt policies, e.g. fair share. \\n* Preemption. \\n* Dynamic resource allocation.\\n\\n##SPARK\\n\\n* Each executor is a JVM\\n* shuffling happens when the data that is required for a computation lives on different executors\\n* to determine what data goes where, a executor will divvy up it’s data and send it out to other executors\\n* (speaking physically about the data) there are shuffle joins which are useful when you have two large datasets. And there are broadcast joins, where the entire B dataset is sent to the executors that have parts of the A dataset.\\n    E.g. table A is a TB, and table B is a MB. Doing a shuffle where the TB is sent out across the network will be take up a lot of network capacity. Whereas, sending table B to all the executors would be cheap.\\n* Joining on nulls is dangerous, so you have 5million nulls, spark will see that as the same key and ship them to one executor, if that’s say 10gig of data then you will get a OOM error.\\n* Non-splittable files are problematic, e.g. gzip files where spark will try and load it into memory, and if you have a big gzip this quickly becomes problematic. Better to added a prior step in your pipeline that breaks up the file prior to feeding it to spark.\\n* sparkContext is the connection to the driver\\n\\n\\n\\n\\nhttps://johnpaton.net/posts/forward-fill-spark/\\nColleague asked about solution on filling nulls in from last known good value. https://stackoverflow.com/a/45374955/3691003\\n\\n##MISC\\n\\n```\\nraise RuntimeError(output_df)\\n```\\n\\nhistoricisation of a dataset that has a changing schema\\n- serialise all cols into a dataset containing one col blob (e.g json) then deserialise in a third datset\\n\\n\\n\\n```python\\ndef get_all_fieldnames(a_list_of_dfs):\\n    '''Return two sets:\\n        1. columns - all column names\\n        2. structs - all structFields'''\\n    columns = set([])\\n    structs = set([])\\n    for df in a_list_of_dfs:\\n        for col in df.schema.fieldNames():\\n            columns.add(col)\\n        for struct in df.schema:\\n            structs.add(struct)\\n\\n    # when the length isn't the same, indicates more than one type used for same col name upstream\\n    assert len(columns) == len(structs)\\n    return columns, structs\\n\\n\\ndef get_super_dataset(df, columns, schema):\\n    '''Return a dataset with additional columns if additional columns are in provided schema'''\\n    missing_columns = [col for col in columns if col not in df.schema.fieldNames()]\\n    for col in missing_columns:\\n        for struct in schema:\\n            if col == struct.name:\\n                df.withColumn(col, F.lit(None).cast(struct.typeName))\\n    return df\\n\\n\\n# create test dfs\\ndf1 = spark_session.createDataFrame(\\n    data=['wibble', 'wobble'],\\n    schema=['foo', 'bar']\\n)\\ndf2 = spark_session.createDataFrame(\\n    data=['wibble', 'wobble', 'whoah'],\\n    schema=['foo', 'bar', 'baz']\\n)\\n\\n# create a list of dfs allowing for iteration\\nlist_of_dfs = [df1, df2]\\n\\ncolumns, structs = get_all_fieldnames(list_of_dfs)\\n\\nall_super_dfs = [get_super_dataset(df) for df in list_of_dfs] \\n```\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><h3 id=\"hdfs\">HDFS</h3>\n<ul>\n<li><em>Name node</em> knows where things go on the *data nodes</li>\n<li>*There is a <em>replication factor</em>, so if a data node goes down data is still available. So when data goes to a data node, the name node sends intructions of which data nodes the recieving data node should replicate to.</li>\n<li>how does the Name Node know where everything is? Firstly it has a FS image where everything is. And an edit log of changes that have been made.</li>\n<li>if the name node went down you wouldn’t know where to put/get things from HDFS. So there are two name nodes an active and a backup/passive one.</li>\n</ul>\n<ul>\n<li>on the name node, run hdfs balancer after pruning to try and recover nodes i.e. reudce them below 85% where they start to fail. Can also run this with different thresholds if necessary (see HDFS Runbook (<a href=\"https://rtfm.palantir.build/docs/hadoop-runbook/master/maintenance/pages/rebalance.html\">https://rtfm.palantir.build/docs/hadoop-runbook/master/maintenance/pages/rebalance.html</a>)) and this <a href=\"http://www.informit.com/articles/article.aspx?p=2755708&amp;seqNum=5\">http://www.informit.com/articles/article.aspx?p=2755708&amp;seqNum=5</a><br>\n* you can increase bandwidth whilst (though why not before) hdfs balancer is running: ./hdfs dfsadmin -setBalancerBandwidth 104857760<br>\n* Check the bandwidth available on your ethernet card first ethtool eth0 could be eth1</li>\n</ul>\n<h2 id=\"yarn\">YARN</h2>\n<ul>\n<li>Yarn a resource manager.</li>\n<li>Containers/executors spin up on boxes.</li>\n<li>Resource mgmt policies, e.g. fair share.</li>\n<li>Preemption.</li>\n<li>Dynamic resource allocation.</li>\n</ul>\n<h2 id=\"spark\">SPARK</h2>\n<ul>\n<li>Each executor is a JVM</li>\n<li>shuffling happens when the data that is required for a computation lives on different executors</li>\n<li>to determine what data goes where, a executor will divvy up it’s data and send it out to other executors</li>\n<li>(speaking physically about the data) there are shuffle joins which are useful when you have two large datasets. And there are broadcast joins, where the entire B dataset is sent to the executors that have parts of the A dataset.<br>\nE.g. table A is a TB, and table B is a MB. Doing a shuffle where the TB is sent out across the network will be take up a lot of network capacity. Whereas, sending table B to all the executors would be cheap.</li>\n<li>Joining on nulls is dangerous, so you have 5million nulls, spark will see that as the same key and ship them to one executor, if that’s say 10gig of data then you will get a OOM error.</li>\n<li>Non-splittable files are problematic, e.g. gzip files where spark will try and load it into memory, and if you have a big gzip this quickly becomes problematic. Better to added a prior step in your pipeline that breaks up the file prior to feeding it to spark.</li>\n<li>sparkContext is the connection to the driver</li>\n</ul>\n<p><a href=\"https://johnpaton.net/posts/forward-fill-spark/\">https://johnpaton.net/posts/forward-fill-spark/</a><br>\nColleague asked about solution on filling nulls in from last known good value. <a href=\"https://stackoverflow.com/a/45374955/3691003\">https://stackoverflow.com/a/45374955/3691003</a></p>\n<h2 id=\"misc\">MISC</h2>\n<pre><code>raise RuntimeError(output_df)\n</code></pre>\n<p>historicisation of a dataset that has a changing schema</p>\n<ul>\n<li>serialise all cols into a dataset containing one col blob (e.g json) then deserialise in a third datset</li>\n</ul>\n<pre><code class=\"language-python\">def get_all_fieldnames(a_list_of_dfs):\n    '''Return two sets:\n        1. columns - all column names\n        2. structs - all structFields'''\n    columns = set([])\n    structs = set([])\n    for df in a_list_of_dfs:\n        for col in df.schema.fieldNames():\n            columns.add(col)\n        for struct in df.schema:\n            structs.add(struct)\n\n    # when the length isn't the same, indicates more than one type used for same col name upstream\n    assert len(columns) == len(structs)\n    return columns, structs\n\n\ndef get_super_dataset(df, columns, schema):\n    '''Return a dataset with additional columns if additional columns are in provided schema'''\n    missing_columns = [col for col in columns if col not in df.schema.fieldNames()]\n    for col in missing_columns:\n        for struct in schema:\n            if col == struct.name:\n                df.withColumn(col, F.lit(None).cast(struct.typeName))\n    return df\n\n\n# create test dfs\ndf1 = spark_session.createDataFrame(\n    data=['wibble', 'wobble'],\n    schema=['foo', 'bar']\n)\ndf2 = spark_session.createDataFrame(\n    data=['wibble', 'wobble', 'whoah'],\n    schema=['foo', 'bar', 'baz']\n)\n\n# create a list of dfs allowing for iteration\nlist_of_dfs = [df1, df2]\n\ncolumns, structs = get_all_fieldnames(list_of_dfs)\n\nall_super_dfs = [get_super_dataset(df) for df in list_of_dfs] \n</code></pre>\n<!--kg-card-end: markdown-->","comment_id":"63","plaintext":"HDFS\n * Name node knows where things go on the *data nodes\n * *There is a replication factor, so if a data node goes down data is still\n   available. So when data goes to a data node, the name node sends intructions\n   of which data nodes the recieving data node should replicate to.\n * how does the Name Node know where everything is? Firstly it has a FS image\n   where everything is. And an edit log of changes that have been made.\n * if the name node went down you wouldn’t know where to put/get things from\n   HDFS. So there are two name nodes an active and a backup/passive one.\n\n * on the name node, run hdfs balancer after pruning to try and recover nodes\n   i.e. reudce them below 85% where they start to fail. Can also run this with\n   different thresholds if necessary (see HDFS Runbook (\n   https://rtfm.palantir.build/docs/hadoop-runbook/master/maintenance/pages/rebalance.html\n   )) and this http://www.informit.com/articles/article.aspx?p=2755708&seqNum=5\n   [http://www.informit.com/articles/article.aspx?p=2755708&seqNum=5]\n   * you can increase bandwidth whilst (though why not before) hdfs balancer is\n   running: ./hdfs dfsadmin -setBalancerBandwidth 104857760\n   * Check the bandwidth available on your ethernet card first ethtool eth0\n   could be eth1\n\nYARN\n * Yarn a resource manager.\n * Containers/executors spin up on boxes.\n * Resource mgmt policies, e.g. fair share.\n * Preemption.\n * Dynamic resource allocation.\n\nSPARK\n * Each executor is a JVM\n * shuffling happens when the data that is required for a computation lives on\n   different executors\n * to determine what data goes where, a executor will divvy up it’s data and\n   send it out to other executors\n * (speaking physically about the data) there are shuffle joins which are useful\n   when you have two large datasets. And there are broadcast joins, where the\n   entire B dataset is sent to the executors that have parts of the A dataset.\n   E.g. table A is a TB, and table B is a MB. Doing a shuffle where the TB is\n   sent out across the network will be take up a lot of network capacity.\n   Whereas, sending table B to all the executors would be cheap.\n * Joining on nulls is dangerous, so you have 5million nulls, spark will see\n   that as the same key and ship them to one executor, if that’s say 10gig of\n   data then you will get a OOM error.\n * Non-splittable files are problematic, e.g. gzip files where spark will try\n   and load it into memory, and if you have a big gzip this quickly becomes\n   problematic. Better to added a prior step in your pipeline that breaks up the\n   file prior to feeding it to spark.\n * sparkContext is the connection to the driver\n\nhttps://johnpaton.net/posts/forward-fill-spark/\nColleague asked about solution on filling nulls in from last known good value. \nhttps://stackoverflow.com/a/45374955/3691003\n\nMISC\nraise RuntimeError(output_df)\n\n\nhistoricisation of a dataset that has a changing schema\n\n * serialise all cols into a dataset containing one col blob (e.g json) then\n   deserialise in a third datset\n\ndef get_all_fieldnames(a_list_of_dfs):\n    '''Return two sets:\n        1. columns - all column names\n        2. structs - all structFields'''\n    columns = set([])\n    structs = set([])\n    for df in a_list_of_dfs:\n        for col in df.schema.fieldNames():\n            columns.add(col)\n        for struct in df.schema:\n            structs.add(struct)\n\n    # when the length isn't the same, indicates more than one type used for same col name upstream\n    assert len(columns) == len(structs)\n    return columns, structs\n\n\ndef get_super_dataset(df, columns, schema):\n    '''Return a dataset with additional columns if additional columns are in provided schema'''\n    missing_columns = [col for col in columns if col not in df.schema.fieldNames()]\n    for col in missing_columns:\n        for struct in schema:\n            if col == struct.name:\n                df.withColumn(col, F.lit(None).cast(struct.typeName))\n    return df\n\n\n# create test dfs\ndf1 = spark_session.createDataFrame(\n    data=['wibble', 'wobble'],\n    schema=['foo', 'bar']\n)\ndf2 = spark_session.createDataFrame(\n    data=['wibble', 'wobble', 'whoah'],\n    schema=['foo', 'bar', 'baz']\n)\n\n# create a list of dfs allowing for iteration\nlist_of_dfs = [df1, df2]\n\ncolumns, structs = get_all_fieldnames(list_of_dfs)\n\nall_super_dfs = [get_super_dataset(df) for df in list_of_dfs]","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2019-12-06 19:29:43","created_by":"1","updated_at":"2020-03-17 10:06:33","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd18","uuid":"cb919867-1784-439a-ad6f-054d7868d6bc","title":"(Untitled)","slug":"untitled","mobiledoc":"{\"version\":\"0.3.2\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"https://www.thedarkweblinks.com/darknet-market-list/\\n\\nhttps://darknetlive.com/darknet-market-list/\\n\\n\"}],[\"hr\",{}],[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://news.bitcoin.com/beginners-guide-buying-goods-darknet/\"]],[\"a\",[\"href\",\"https://mastodon.social/@darkdotfail\"]],[\"a\",[\"href\",\"https://nitter.dark.fail/darkdotfail\"]],[\"strong\"],[\"a\",[\"href\",\"https://dark.fail/canary.txt\"]],[\"a\",[\"href\",\"https://dark.fail/pgp\"]],[\"a\",[\"href\",\"https://www.torproject.org\"]],[\"code\"],[\"a\",[\"href\",\"https://dark.fail/#philosophy\"]],[\"a\",[\"href\",\"https://dark.fail/#donations\"]],[\"a\",[\"href\",\"https://dark.fail/darknetlive\"]],[\"a\",[\"href\",\"https://dark.fail/dna\"]],[\"a\",[\"href\",\"https://dark.fail/dread\"]],[\"a\",[\"href\",\"https://dark.fail/recon\"]],[\"a\",[\"href\",\"https://dark.fail/envoy\"]],[\"a\",[\"href\",\"https://dark.fail/thehub\"]],[\"a\",[\"href\",\"https://dark.fail/tmg\"]],[\"a\",[\"href\",\"https://dark.fail/torum\"]],[\"a\",[\"href\",\"https://dark.fail/kilos\"]],[\"a\",[\"href\",\"https://dark.fail/empire\"]],[\"a\",[\"href\",\"https://dark.fail/empire-forum\"]],[\"a\",[\"href\",\"https://dark.fail/monopoly\"]],[\"a\",[\"href\",\"https://dark.fail/whitehouse\"]],[\"a\",[\"href\",\"https://dark.fail/darkmarket\"]],[\"a\",[\"href\",\"https://dark.fail/tormarket\"]],[\"a\",[\"href\",\"https://dark.fail/imperiya\"]],[\"a\",[\"href\",\"https://dark.fail/verses\"]],[\"a\",[\"href\",\"https://dark.fail/cannahome\"]],[\"a\",[\"href\",\"https://dark.fail/cannazon\"]],[\"a\",[\"href\",\"https://dark.fail/deepsea\"]],[\"a\",[\"href\",\"https://dark.fail/square\"]],[\"a\",[\"href\",\"https://dark.fail/televend\"]],[\"a\",[\"href\",\"https://dark.fail/torrez\"]],[\"a\",[\"href\",\"https://dark.fail/hydra\"]],[\"a\",[\"href\",\"https://dark.fail/exploit-in\"]],[\"a\",[\"href\",\"https://dark.fail/tochka-git\"]],[\"a\",[\"href\",\"https://dark.fail/tochka\"]],[\"a\",[\"href\",\"https://dark.fail/verified\"]],[\"a\",[\"href\",\"https://dark.fail/canada-hq\"]],[\"a\",[\"href\",\"https://dark.fail/deutschland-im-deep-web\"]],[\"a\",[\"href\",\"https://dark.fail/uas-service-rdp\"]],[\"a\",[\"href\",\"https://dark.fail/wannabuy\"]],[\"a\",[\"href\",\"https://dark.fail/yellow-brick\"]],[\"a\",[\"href\",\"https://dark.fail/wayaway\"]],[\"a\",[\"href\",\"https://dark.fail/bbc\"]],[\"a\",[\"href\",\"https://dark.fail/bm\"]],[\"a\",[\"href\",\"https://dark.fail/deutsche-welle\"]],[\"a\",[\"href\",\"https://dark.fail/pornhub\"]],[\"a\",[\"href\",\"https://dark.fail/propublica\"]],[\"a\",[\"href\",\"https://dark.fail/proton\"]],[\"a\",[\"href\",\"https://dark.fail/riseup\"]],[\"a\",[\"href\",\"https://dark.fail/calyx\"]],[\"a\",[\"href\",\"https://dark.fail/nyt\"]],[\"a\",[\"href\",\"https://dark.fail/torbox\"]],[\"a\",[\"href\",\"https://dark.fail/agora-desk\"]],[\"a\",[\"href\",\"https://dark.fail/local-monero\"]],[\"a\",[\"href\",\"https://dark.fail/tor\"]],[\"a\",[\"href\",\"https://dark.fail/xmrguide\"]],[\"a\",[\"href\",\"https://dark.fail/cryptostorm\"]],[\"a\",[\"href\",\"https://dark.fail/ipredator\"]],[\"a\",[\"href\",\"https://dark.fail/mullvad\"]],[\"a\",[\"href\",\"https://dark.fail/njalla\"]],[\"a\",[\"href\",\"https://dark.fail/archive-is\"]],[\"a\",[\"href\",\"https://dark.fail/duckduckgo\"]],[\"a\",[\"href\",\"https://dark.fail/facebook\"]],[\"a\",[\"href\",\"https://dark.fail/keybase\"]],[\"a\",[\"href\",\"https://dark.fail/psychonaut\"]],[\"a\",[\"href\",\"https://dark.fail/8chan\"]],[\"a\",[\"href\",\"https://dark.fail/8kun\"]],[\"a\",[\"href\",\"https://dark.fail/endchan\"]],[\"a\",[\"href\",\"https://dark.fail/ddosecrets\"]],[\"a\",[\"href\",\"https://dark.fail/elude\"]],[\"a\",[\"href\",\"https://dark.fail/secmail\"]],[\"a\",[\"href\",\"https://dark.fail/coinpayments\"]],[\"a\",[\"href\",\"https://dark.fail/sks\"]],[\"a\",[\"href\",\"https://dark.fail/the-pirate-bay\"]],[\"a\",[\"href\",\"https://dark.fail/torch\"]],[\"a\",[\"href\",\"https://dark.fail/wikileaks\"]],[\"a\",[\"href\",\"https://dark.fail/chronos\"]],[\"a\",[\"href\",\"https://dark.fail/tape\"]],[\"a\",[\"href\",\"https://dark.fail/netherlands-police\"]],[\"a\",[\"href\",\"https://dark.fail/cia\"]],[\"a\",[\"href\",\"https://dark.fail/fbi-ncide\"]],[\"a\",[\"href\",\"https://dark.fail/dream-partner\"]],[\"a\",[\"href\",\"https://dark.fail/df\"]],[\"a\",[\"href\",\"https://supporters.eff.org/\"]],[\"a\",[\"href\",\"https://dark.fail/pgp-sigs.txt\"]],[\"a\",[\"href\",\"https://dark.fail/spec/omg.txt\"]]],\"sections\":[[10,0],[10,1],[1,\"p\",[[0,[0],1,\"https://news.bitcoin.com/beginners-guide-buying-goods-darknet/\"]]],[10,2],[1,\"h3\",[[0,[],0,\"dark.fail: Is a darknet site online?Updated Sat, 08 Aug 2020 15:27:25 UTC\"],[1,[],0,0],[0,[1],1,\"Mastodon\"],[0,[],0,\" | \"],[0,[2],1,\"Twitter\"]]],[1,\"p\",[[0,[3],1,\"We apologize for the recent downtime.\"],[0,[],0,\" This was caused by a technical mistake that was entirely our fault. This single point of failure is now being redesigned to prevent this from happening in the future. \"],[0,[4],1,\"Updated canary.\"]]],[1,\"p\",[[0,[3],1,\"NEW:\"],[0,[],0,\" Verify signatures with Dark.fail's \"],[0,[5],1,\"new PGP Tool\"],[0,[],0,\". Don't get phished. Always PGP verify .onion and Bitcoin addresses before interacting with them.\"]]],[1,\"p\",[[0,[],0,\"Tor is the uncensored internet. Install \"],[0,[6],1,\"Tor Browser\"],[0,[],0,\" to explore it. Set \"],[0,[7],1,\"darkfailllnkf4vf.onion\"],[0,[],0,\" as your home page to save time. Links are PGP verified and unclickable for your safety. dark.fail's \"],[0,[8],1,\"philosophy\"],[0,[],0,\" and \"],[0,[9],1,\"finances\"]]],[1,\"p\",[[0,[3],1,\"This resource is intended for researchers only.\"],[0,[],0,\" I do not vouch for any sites.\"]]],[1,\"h4\",[[0,[10],1,\"Darknet Live\"]]],[3,\"ul\",[[[0,[7],1,\"http://darkzzx4avcsuofgfez5zq75cqc4mprjvfqywo45dfcaxrwqg6qrlfid.onion\"]]]],[1,\"h4\",[[0,[11],1,\"Darknet Avengers Forum\"]]],[3,\"ul\",[[[0,[7],1,\"http://avengersdutyk3xf.onion\"]]]],[1,\"h4\",[[0,[12],1,\"Dread\"]]],[3,\"ul\",[[[0,[7],1,\"http://dreadditevelidot.onion\"]],[[0,[7],1,\"http://dreadytofatroptsdj6io7l3xptbet6onoyno2yv7jicoxknyazubrad.onion\"]]]],[1,\"h4\",[[0,[13],1,\"Recon\"]]],[3,\"ul\",[[[0,[7],1,\"http://reconponydonugup.onion\"]]]],[1,\"h4\",[[0,[14],1,\"Envoy Forum\"]]],[3,\"ul\",[[[0,[7],1,\"http://envoys5appps3bin.onion\"]],[[0,[7],1,\"http://envoyvvcccvepc5i.onion\"]],[[0,[7],1,\"http://envoyzlbxpgniels.onion\"],[0,[14],1,\"3 more...\"]]]],[1,\"h4\",[[0,[15],1,\"The Hub Forum\"]]],[3,\"ul\",[[[0,[7],1,\"http://thehub7xbw4dc5r2.onion\"]]]],[1,\"h4\",[[0,[16],1,\"The Majestic Garden\"]]],[3,\"ul\",[[[0,[7],1,\"http://talismanrestz7mr.onion\"]],[[0,[7],1,\"http://nzlbyrcvvqtrkxiu.onion\"]],[[0,[7],1,\"http://2oywvwmtzdelmiei.onion\"],[0,[16],1,\"2 more...\"]]]],[1,\"h4\",[[0,[17],1,\"Torum\"]]],[3,\"ul\",[[[0,[7],1,\"http://torum43tajnrxritn4iumy75giwb5yfw6cjq2czjikhtcac67tfif2yd.onion\"]]]],[1,\"h4\",[[0,[18],1,\"Kilos\"]]],[3,\"ul\",[[[0,[7],1,\"http://dnmugu4755642434.onion\"]]]],[1,\"h4\",[[0,[19],1,\"Empire Market\"]]],[3,\"ul\",[[[0,[7],1,\"http://erj7kwqkdkl73ewsuq6stztehx2tehk2aidxlex3btrfnjqax3ucvgyd.onion\"],[0,[19],1,\"5 more...\"]]]],[1,\"h4\",[[0,[20],1,\"Empire Market Forum\"]]],[3,\"ul\",[[[0,[7],1,\"http://empire23d4mne2afpn4lnemfahydxp6dv3c7s6tekii3g7vel3xs7rad.onion\"]]]],[1,\"h4\",[[0,[21],1,\"Monopoly Market\"]]],[3,\"ul\",[[[0,[7],1,\"http://monopolyberbucxu.onion\"]]]],[1,\"h4\",[[0,[22],1,\"White House Market\"]]],[3,\"ul\",[[[0,[7],1,\"http://auzbdiguv5qtp37xoma3n4xfch62duxtdiu4cfrrwbxgckipd4aktxid.onion\"]],[[0,[7],1,\"http://7yipwxdv5cfdjfpjztiz7sv2jlzzjuepmxy4mtlvuaojejwhg3zhliqd.onion\"]],[[0,[7],1,\"http://cieprrpdgp7moka2ktlwy54ooymtgsre23enrf4dfzssap74zz45f6id.onion\"],[0,[22],1,\"5 more...\"]]]],[1,\"h4\",[[0,[23],1,\"Dark Market\"]]],[3,\"ul\",[[[0,[7],1,\"http://darkmarketsomqvzqfjudpd6t5eabgvvpplrbtzq6prervyogenlrlqd.onion\"]],[[0,[7],1,\"http://darknet3ysxpbjtvbtyqorzuq3bp4pk3vt4fuhqxz6h2bzj2kyjbisqd.onion\"]],[[0,[7],1,\"http://darknet4rbfizlg53dwc5lt5hj4mewcgltubcpvrfpvjavm64inaf3ad.onion\"],[0,[23],1,\"1 more...\"]]]],[1,\"h4\",[[0,[23],1,\"DarkMarket\"]]],[3,\"ul\",[[[0,[7],1,\"http://darkevuygggqkqhq.onion\"]],[[0,[7],1,\"http://darkmarkl7f5hz5f.onion\"]],[[0,[7],1,\"http://darkmarkhbh2xyel.onion\"],[0,[23],1,\"1 more...\"]]]],[1,\"h4\",[[0,[24],1,\"Tor Market\"]]],[3,\"ul\",[[[0,[7],1,\"http://tt2mopgckifmberr.onion\"]]]],[1,\"h4\",[[0,[25],1,\"Imperiya Vendor Shops\"]]],[3,\"ul\",[[[0,[7],1,\"http://imperiyakggyacaf.onion\"]]]],[1,\"h4\",[[0,[26],1,\"The Versus Project\"]]],[3,\"ul\",[[[0,[7],1,\"http://pqqmr3p3tppwqvvapi6fa7jowrehgd36ct6lzr26qqormaqvh6gt4jyd.onion\"]],[[0,[7],1,\"http://q2f7swt5yvbhciqqbbsidufu2vtkv6ivwy6g5i5ukejjlb2jeghd2had.onion\"]]]],[1,\"h4\",[[0,[27],1,\"Cannahome\"]]],[3,\"ul\",[[[0,[7],1,\"http://cannahome3ke3366.onion\"]],[[0,[7],1,\"http://cannahomekql6hhg.onion\"]]]],[1,\"h4\",[[0,[28],1,\"Cannazon Market\"]]],[3,\"ul\",[[[0,[7],1,\"http://cannazon4gbjluus.onion\"]],[[0,[7],1,\"http://sxwjdzct7jnoef7o.onion\"]],[[0,[7],1,\"http://cannazonceujdye3.onion\"],[0,[28],1,\"3 more...\"]]]],[1,\"h4\",[[0,[29],1,\"DeepSea Market\"]]],[3,\"ul\",[[[0,[7],1,\"http://dsmktyc4q6i4ycvjibd6lf732l66vcvjmhsce5x22bomtrrwdobzbfad.onion\"]],[[0,[7],1,\"http://dsmktipxelmq3666psdthvz6obow5thplftw7gww4hi3vmendm6tsaad.onion\"]],[[0,[7],1,\"http://dsmktvpejm3sj5c5ua7az5hnx5mo36ewwixaj3rfbbwudfoyxaz7xxid.onion\"]]]],[1,\"h4\",[[0,[30],1,\"Square Market\"]]],[3,\"ul\",[[[0,[7],1,\"http://c77yo2fe3f4e3g7tll5qrzzoneymihws2tpjceg6wop2ky6pkcaqczyd.onion\"]]]],[1,\"h4\",[[0,[31],1,\"Televend\"]]],[3,\"ul\",[[[0,[7],1,\"http://televendmnsfbznh.onion\"]]]],[1,\"h4\",[[0,[32],1,\"ToRReZ Market\"]]],[3,\"ul\",[[[0,[7],1,\"http://yxuy5oau7nugw4kpb4lclrqdbixp3wvc4iuiad23ebyp2q3gx7rtrgqd.onion\"]],[[0,[7],1,\"http://lstkx6p3gzsgfwsqpntlv7tv4tsjzziwp76gvkaxx2mqe3whvlp243id.onion\"]],[[0,[7],1,\"http://f3dxwzcmojrlphehqwcecwe3amg6rkzgilcn4xqxmaiezoedijf6rtid.onion\"],[0,[32],1,\"1 more...\"]]]],[1,\"h4\",[[0,[33],1,\"Hydra\"]]],[3,\"ul\",[[[0,[7],1,\"http://hydraruzxpnew4af.onion\"]]]],[1,\"h4\",[[0,[34],1,\"Exploit.in Forum\"]]],[3,\"ul\",[[[0,[7],1,\"http://exploitinqx4sjro.onion\"]]]],[1,\"h4\",[[0,[35],1,\"Tochka Git Repository\"]]],[1,\"h5\",[[0,[],0,\"Offline\"]]],[3,\"ul\",[[[0,[7],1,\"http://qxklmrhx7qkzais6.onion\"]]]],[1,\"h4\",[[0,[36],1,\"Tochka Market\"]]],[1,\"h5\",[[0,[],0,\"Offline\"]]],[3,\"ul\",[[[0,[7],1,\"http://34n4am6fxu4dnv66.onion\"]],[[0,[7],1,\"http://pointgg344ghbo2s.onion\"]]]],[1,\"h4\",[[0,[37],1,\"Verified Forum\"]]],[3,\"ul\",[[[0,[7],1,\"http://verified2ebdpvms.onion\"]]]],[1,\"h4\",[[0,[38],1,\"CanadaHQ\"]]],[3,\"ul\",[[[0,[7],1,\"http://canadahq2lo3logs.onion\"]],[[0,[7],1,\"http://canadahqx53lcurj.onion\"]]]],[1,\"h4\",[[0,[39],1,\"Deutschland im Deep Web Forum\"]]],[3,\"ul\",[[[0,[7],1,\"http://germanyruvvy2tcw.onion\"]]]],[1,\"h4\",[[0,[40],1,\"UAS Service RDP\"]]],[3,\"ul\",[[[0,[7],1,\"http://2x4tmsirlqvqmwdz.onion\"]]]],[1,\"h4\",[[0,[41],1,\"Wannabuy RDP\"]]],[3,\"ul\",[[[0,[7],1,\"http://wannabuyaynozvmz.onion\"]]]],[1,\"h4\",[[0,[42],1,\"Yellow Brick Market\"]]],[3,\"ul\",[[[0,[7],1,\"http://ck73ugjvx5a4wkhsmrfvwhlrq7evceovbsb7tvaxilpahybdokbyqcqd.onion\"]]]],[1,\"h4\",[[0,[43],1,\"Wayaway\"]]],[1,\"h5\",[[0,[],0,\"Offline\"]]],[3,\"ul\",[[[0,[7],1,\"http://wayawaytcl3k66fl.onion\"]]]],[1,\"h4\",[[0,[44],1,\"BBC\"]]],[3,\"ul\",[[[0,[7],1,\"http://bbcnewsv2vjtpsuy.onion\"]]]],[1,\"h4\",[[0,[45],1,\"Bitmessage Gateway\"]]],[3,\"ul\",[[[0,[7],1,\"http://bitmailendavkbec.onion\"]]]],[1,\"h4\",[[0,[46],1,\"Deutsche Welle\"]]],[3,\"ul\",[[[0,[7],1,\"http://dwnewsvdyyiamwnp.onion\"]]]],[1,\"h4\",[[0,[47],1,\"Pornhub\"]]],[3,\"ul\",[[[0,[7],1,\"http://pornhubthbh7ap3u.onion\"]]]],[1,\"h4\",[[0,[48],1,\"ProPublica\"]]],[3,\"ul\",[[[0,[7],1,\"http://propub3r6espa33w.onion\"]]]],[1,\"h4\",[[0,[49],1,\"Protonmail\"]]],[3,\"ul\",[[[0,[7],1,\"https://protonirockerxow.onion\"]]]],[1,\"h4\",[[0,[50],1,\"Riseup\"]]],[3,\"ul\",[[[0,[7],1,\"http://nzh3fv6jc6jskki3.onion\"]],[[0,[7],1,\"http://zsolxunfmbfuq7wf.onion\"]],[[0,[7],1,\"http://j6uhdvbhz74oefxf.onion\"],[0,[50],1,\"1 more...\"]]]],[1,\"h4\",[[0,[51],1,\"The Calyx Institute (Jabber)\"]]],[3,\"ul\",[[[0,[7],1,\"http://ijeeynrc6x2uy5ob.onion\"]]]],[1,\"h4\",[[0,[52],1,\"The New York Times\"]]],[3,\"ul\",[[[0,[7],1,\"https://nytimes3xbfgragh.onion\"]]]],[1,\"h4\",[[0,[53],1,\"TorBox Email\"]]],[3,\"ul\",[[[0,[7],1,\"http://torbox3uiot6wchz.onion\"]]]],[1,\"h4\",[[0,[54],1,\"AgoraDesk\"]]],[3,\"ul\",[[[0,[7],1,\"http://agoradeska6jfxpf.onion\"]]]],[1,\"h4\",[[0,[55],1,\"LocalMonero.co\"]]],[3,\"ul\",[[[0,[7],1,\"http://localmonerogt7be.onion\"]]]],[1,\"h4\",[[0,[56],1,\"The Tor Project\"]]],[3,\"ul\",[[[0,[7],1,\"http://expyuzz4wqqyqhjn.onion\"]]]],[1,\"h4\",[[0,[57],1,\"xmrguide\"]]],[3,\"ul\",[[[0,[7],1,\"http://xmrguide42y34onq.onion\"]]]],[1,\"h4\",[[0,[58],1,\"Cryptostorm VPN\"]]],[3,\"ul\",[[[0,[7],1,\"http://stormgm7blbk7odd.onion\"]]]],[1,\"h4\",[[0,[59],1,\"IPredator VPN\"]]],[3,\"ul\",[[[0,[7],1,\"https://fwtnuwekoeayj3s7.onion\"]]]],[1,\"h4\",[[0,[60],1,\"Mullvad VPN\"]]],[3,\"ul\",[[[0,[7],1,\"http://xcln5hkbriyklr6n.onion\"]]]],[1,\"h4\",[[0,[61],1,\"Njal.la\"]]],[3,\"ul\",[[[0,[7],1,\"http://njalladnspotetti.onion\"]]]],[1,\"h4\",[[0,[62],1,\"Archive.is\"]]],[3,\"ul\",[[[0,[7],1,\"http://archivecaslytosk.onion\"]]]],[1,\"h4\",[[0,[63],1,\"DuckDuckGo\"]]],[3,\"ul\",[[[0,[7],1,\"https://3g2upl4pq6kufc4m.onion\"]]]],[1,\"h4\",[[0,[64],1,\"Facebook\"]]],[3,\"ul\",[[[0,[7],1,\"https://facebookcorewwwi.onion\"]]]],[1,\"h4\",[[0,[65],1,\"Keybase\"]]],[3,\"ul\",[[[0,[7],1,\"http://fncuwbiisyh6ak3i.onion\"]]]],[1,\"h4\",[[0,[66],1,\"Psychonaut Wiki\"]]],[3,\"ul\",[[[0,[7],1,\"http://psychonaut3z5aoz.onion\"]]]],[1,\"h4\",[[0,[67],1,\"8chan\"]]],[1,\"h5\",[[0,[],0,\"Offline\"]]],[3,\"ul\",[[[0,[7],1,\"http://oxwugzccvk3dk6tj.onion\"]]]],[1,\"h4\",[[0,[68],1,\"8kun (formerly 8chan)\"]]],[3,\"ul\",[[[0,[7],1,\"http://jthnx5wyvjvzsxtu.onion\"]]]],[1,\"h4\",[[0,[69],1,\"Endchan\"]]],[3,\"ul\",[[[0,[7],1,\"http://enxx3byspwsdo446jujc52ucy2pf5urdbhqw3kbsfhlfjwmbpj5smdad.onion\"]],[[0,[7],1,\"http://endchan5doxvprs5.onion\"]],[[0,[7],1,\"http://s6424n4x4bsmqs27.onion\"]]]],[1,\"h4\",[[0,[70],1,\"Distributed Denial of Secrets\"]]],[3,\"ul\",[[[0,[7],1,\"http://ddosecretspzwfy7.onion\"]]]],[1,\"h4\",[[0,[71],1,\"Elude.in\"]]],[3,\"ul\",[[[0,[7],1,\"http://eludemaillhqfkh5.onion\"]]]],[1,\"h4\",[[0,[72],1,\"Secmail\"]]],[3,\"ul\",[[[0,[7],1,\"http://secmailw453j7piv.onion\"]]]],[1,\"h4\",[[0,[73],1,\"CoinPayments\"]]],[1,\"h5\",[[0,[],0,\"Offline\"]]],[3,\"ul\",[[[0,[7],1,\"https://coinpaymtstgtibr.onion\"]]]],[1,\"h4\",[[0,[74],1,\"SKS PGP Keyserver Pool\"]]],[3,\"ul\",[[[0,[7],1,\"http://jirk5u4osbsr34t5.onion\"]]]],[1,\"h4\",[[0,[75],1,\"The Pirate Bay\"]]],[3,\"ul\",[[[0,[7],1,\"http://suprbayoubiexnmp.onion\"]]]],[1,\"h4\",[[0,[76],1,\"Torch Search\"]]],[3,\"ul\",[[[0,[7],1,\"http://xmh57jrzrnw6insl.onion\"]]]],[1,\"h4\",[[0,[77],1,\"WikiLeaks\"]]],[3,\"ul\",[[[0,[7],1,\"http://wlchatc3pjwpli5r.onion\"]]]],[1,\"h4\",[[0,[78],1,\"Chronos\"]]],[1,\"h5\",[[0,[],0,\"Offline\"]]],[3,\"ul\",[[[0,[7],1,\"http://chronosu3ulk3o3a.onion\"]]]],[1,\"h4\",[[0,[79],1,\"Tape\"]]],[3,\"ul\",[[[0,[7],1,\"http://tapeucwutvne7l5o.onion\"]]]],[1,\"h4\",[[0,[80],1,\"National Police of the Netherlands\"]]],[3,\"ul\",[[[0,[7],1,\"http://politiepcvh42eav.onion\"]]]],[1,\"h4\",[[0,[81],1,\"The CIA's Official Onion Site\"]]],[3,\"ul\",[[[0,[7],1,\"http://ciadotgov4sjwlzihbbgxnqg3xiyrg7so2r2o3lt5wz5ypk4sxyjstad.onion\"]]]],[1,\"h4\",[[0,[82],1,\"The FBI's NCIDE Task Force\"]]],[3,\"ul\",[[[0,[7],1,\"http://ncidetf3j26mdtvf.onion\"]]]],[1,\"h4\",[[0,[83],1,\"Dream Market's Partner (announced during shutdown)\"]]],[1,\"h5\",[[0,[],0,\"Offline\"]]],[3,\"ul\",[[[0,[7],1,\"http://weroidjkazxqds2l.onion\"],[0,[],0,\"\"]]]],[1,\"h4\",[[0,[84],1,\"dark.fail\"]]],[3,\"ul\",[[[0,[7],1,\"http://darkfailllnkf4vf.onion\"]]]],[1,\"h3\",[[0,[],0,\"Philosophy\"]]],[1,\"p\",[[0,[],0,\"Always fast, one server request. No tracking ever, no javascript ever.\"]]],[1,\"p\",[[0,[],0,\"Accurate URLs verified by PGP. No direct linking in order to protect against DNS leaks from accidental clicking in a clearnet browser.\"]]],[1,\"p\",[[0,[],0,\"Knowledge of darknet site uptime is important to many cybersecurity researchers. This site is provided for information only. No endorsements are made or implied regarding any sites or organizations mentioned here.\"]]],[1,\"p\",[[0,[],0,\"Want a link listed or removed here? Email hello -a-t- dark.fail\"]]],[1,\"p\",[[0,[],0,\"Jabber: darkdotfail at jabber.calyxinstitute.org\"]]],[1,\"p\",[[0,[],0,\"Defend your right to privacy: \"],[0,[85],1,\"donate to the EFF.\"]]],[1,\"h5\",[[0,[],0,\"Donations are Appreciated\"]]],[1,\"p\",[[0,[],0,\"dark.fail is supported by our users. No sites pay for placement or advertisements, no affiliate links have or will ever be used. If this resource has helped you please consider sending a contribution.\"]]],[1,\"p\",[[0,[],0,\"Bitcoin: 1EWXzHdvHFBYgr8fHyiNfFuW9A9etyNVqC\"]]],[1,\"p\",[[0,[],0,\"Monero: 88qw83dNPkGdnsxN5FWHvg2D7QmkjbybYLwhi7EyNkTfhz8tFWP5L2w9XmHXuY2WAKRMmwz7Ro6PcZ6gDPYR6Xb99FaKRca\"]]],[1,\"h5\",[[0,[],0,\"PGP Key-----BEGIN PGP PUBLIC KEY BLOCK----- mQINBF1PnPgBEAC0vRFW12SXkmY7NAA0sDO1XFCtfartMtwCT+RJc/SYv/RZdoaE YaLC7n6UCOs6mlBgwd8+iZI7LOL7nOt11z7AiPtrZ/ANqpaEcl2HqBP6q8uBnd3P P5x/LNWHUcHGSPV/pbNK49tT4/awU9QZ2g1ZaDzOHbGU9/BgXWnCs7nmb0jvTkz8 cipenoNGTldiqEJDWx8m92geh9NMNjrYSL6aj/MoscO8CKo0BQnyP5zwi/jhn2sw 379qr7aCAXrEmqCf12oXXGzXWnHXhz9HpC5jPxoVQZL9SdyulvKkMk8jrqStmi9S EpmG+KhYr+sNWpFBkL6P51qV+NQ9oRA8ASSIZCe7IejEnGRIEjq0d4CJzh7BkcVS mvwGy4beHdWgt4o14MU1XuvDn9YO33hAkAxMnhBs3I8hGeghb35zyORZS7nlyx4X AY+X9ObMTpWc5sMvEgkP6ythGI/QzBc80wTUAGeAnGiyjmV9TwkIZIJTqUujascB mzbCMTWnX04VZtggQGcGb+vEgvUJ1iRGrRyrWCeC2eRU/ArfAN8ZNfk6LGBcdTCq Y9gjWEIEr4WQUHLyWgOfpV3Wpd72YzJdHy0zMaQJqWfiyDUI23AZNrhqg2jz+1E1 +w6pPMrjlV+BByAIdIGBOdSkh0MJjmk2ExjLc6LHwnBy4+Qnjfq2I3oW8QARAQAB tB1EYXJrRG90RmFpbCA8aGVsbG9AZGFyay5mYWlsPokCVAQTAQoAPhYhBG39rmbU P8Huzrh+fOLkadx9jD2iBQJdT5z4AhsDBQkB/prIBQsJCAcDBRUKCQgLBRYCAwEA Ah4BAheAAAoJEOLkadx9jD2iZXoQAKUD5bpsTUvP62b5eOV3h1EZhrwQo4NSxi64 banmyTK6hKZuhY77x2t/bLWcWkNfPkgpqVYS0SRdbduqYPXzCX1ssQDl3ypLakqH z+UUpKxBkEZgBRQU2N283b0g+AD4Rr4RzHYsBBbfB5BtWBnZ3qpMFGB6ymUnT+vh CPEVkdkxpQqXfbq3Viwo2XOGj2Q6MISw2UJGFA2d3AAMM73hqsSTMXHKPQ74KlnM bk4DWB3Ig5m9aVpyMV8pB/6rZKG3wOHhSmsOYHBj26Ao+XAyk71qOpp/IZEDDBYr EZ7OZxxA4nB4B4SpGDv5515aa1G+4ar6F86ywz2yrtOQcMlMtJoB9QpL0aOraqRT mDMVqSeGG7FnW7FxE580dTm36qAlzfdW5WqBN4+WAmERPwckR0x88rmaVN8mG8eY soiBAYZU2RbecvbZotCWLPkeqBH6m5a0xmDcKf7SOKZbZKGz6JZcJeGKuFQDrs3h akpxHE4PlJAfk5InW8JZxY/D59+douzzz0sJDBLyoCY3cf8E3q2rJz/KFK6vr23j c63g/AVFbAP1XvfKyumT/nR/J4VD1ExImtnP7i2vl/faoa1gkMbkR5zAuulFyPkQ UGJjnBKniwfO/axppXSwtXJZ/SEJkd/DhGBs2n9+i8qsp1bW8UeSx8h6PjMIio4S pXXHipdcuQINBF1PnPgBEAC3Wks0hoFvdU6qw1wsmsK0U8ukcO3PsaxI2go10TKP jmPKBJ4qmW3nSqdEENwx0RakWVlIFgHtJX2Rfk75zhGc3DQy6tpoD3IyEniXzkUS xIApc0uvWM+CXN5I0Iplp+RC2qlqZ/ratRStkF2mhXS+IxKTgB0puXkq7VhbXTre QnyuacEuubhtisUhx9a8btbbK3zKTWC2ySBl0ctYemkJ1+22zFnmqdwUo1cp1YwN Siv88xT+iZZMcvcoctod+UuQLLXyiZsCE0ELnVSQKGlTS+zef/LnV/WFqP7AKN2A r8MxKXKd1M5hszYAY4ovvDR/C6pIsuPI1aTQLB1/97qIW6MFtHkl4PqDvEJxJZuA 3COYXLrpRQtTH6A81Bcath+7GPNYxIHyEjtRdB9aWUQIkN3inEYG0grWlWBX/Fms jNfcds2cl+PKeXYKXbGjXex1d36VawIm8TqeQIueNe3BypapL8tF9mth5l+JoUr2 Uy61AJSP6VJa7/Z3iW//Pq7xNH73/yHqHGMpM38UArFPO7goI3V/AWWNikxD3rUe PPDSkjUouYEmt80dloSkmRXErWEP2M8l1sHiOcBZmu9imtlneNtrGpfrG8+Box59 Ia8ICd2KAUyiFDVqsOsjacGiTJqXcbfogp5fdGkONyYEfJnbLc80OuGi5dyntzKR qQARAQABiQI8BBgBCgAmFiEEbf2uZtQ/we7OuH584uRp3H2MPaIFAl1PnPgCGwwF CQH+msgACgkQ4uRp3H2MPaJFCg//Rz+EXjTUXCjPkSbZwbgSHF7eExvivt6s12t9 rfcAPGzLP0cb+/G7AjZwqlHj9Ly50oy8DFXlx/XGcyVUB9RFH6uI+Dqyeh8FGOmp wtSReAMEdTppV5XPYgrNEIzK2Z6HUdFLcYepANQVPf03c63/uJdUlLnWSb3h3TKb nE+JkHeglsHSrLXzdw2fZ5jhZCp6oaRax6GFLvNhYY5MHPMr9tBP8Qkze83Dq54Q S2+5XIzjtNSIBfK5F1urX4ZpOT4qKuGeGz8p7wiveOxU6q2Hu+ltotkJSYD/G0dy hXbLuyDmJtrp3Q2CR6mwgoHVAAUQFCB7JeUsA3dOAAqg4B/btZ4ht6bKTvx2qGnS aI+Dmq6pm8ZNoSCWXcNVGhzSigfU310KUtQ7LzYGl/ZRPSROmd0nw9XgFNJFi5k1 YbITFbfOg2BE+rYOdZB4LyyyD9dzZWtvawLSGjjKkvj1HOlGwvWHSPIhT6h5Z614 cXt7S2kqTp7Vn+YzDSPLZt1eofV5+jWTn8Jw3C/KQezWfrQeUQlYm1WxK79IPZkE yKPX/9D8Ty2qjAEf5R480Pr0yAR8jDx4+RKQJOH/41H77hJwsOfmJ7wPREmigui/ EUXcEuJOagUd4H05mi2iGvzKengudehsIJw3IVs3nV6/79YT4DqdJMMGyjPFQwjm qpBfe6I= =J6e1 -----END PGP PUBLIC KEY BLOCK-----\"]]],[1,\"p\",[[0,[86],1,\"Key rotated on 2019-08-11\"]]],[1,\"p\",[[0,[3],1,\"ADMINS:\"],[0,[],0,\" Introducing the Onion Mirror Guidelines. \"],[0,[87],1,\"Implement the \\\"OMG\\\"\"],[0,[],0,\" to remain listed on dark.fail.\"]]]]}","html":"<!--kg-card-begin: markdown--><p><a href=\"https://www.thedarkweblinks.com/darknet-market-list/\">https://www.thedarkweblinks.com/darknet-market-list/</a></p>\n<p><a href=\"https://darknetlive.com/darknet-market-list/\">https://darknetlive.com/darknet-market-list/</a></p>\n<!--kg-card-end: markdown--><hr><p><a href=\"https://news.bitcoin.com/beginners-guide-buying-goods-darknet/\">https://news.bitcoin.com/beginners-guide-buying-goods-darknet/</a></p><hr><h3 id=\"dark-fail-is-a-darknet-site-onlineupdated-sat-08-aug-2020-15-27-25-utcmastodon-twitter\">dark.fail: Is a darknet site online?Updated Sat, 08 Aug 2020 15:27:25 UTC<br><a href=\"https://mastodon.social/@darkdotfail\">Mastodon</a> | <a href=\"https://nitter.dark.fail/darkdotfail\">Twitter</a></h3><p><strong>We apologize for the recent downtime.</strong> This was caused by a technical mistake that was entirely our fault. This single point of failure is now being redesigned to prevent this from happening in the future. <a href=\"https://dark.fail/canary.txt\">Updated canary.</a></p><p><strong>NEW:</strong> Verify signatures with Dark.fail's <a href=\"https://dark.fail/pgp\">new PGP Tool</a>. Don't get phished. Always PGP verify .onion and Bitcoin addresses before interacting with them.</p><p>Tor is the uncensored internet. Install <a href=\"https://www.torproject.org\">Tor Browser</a> to explore it. Set <code>darkfailllnkf4vf.onion</code> as your home page to save time. Links are PGP verified and unclickable for your safety. dark.fail's <a href=\"https://dark.fail/#philosophy\">philosophy</a> and <a href=\"https://dark.fail/#donations\">finances</a></p><p><strong>This resource is intended for researchers only.</strong> I do not vouch for any sites.</p><h4 id=\"darknet-live\"><a href=\"https://dark.fail/darknetlive\">Darknet Live</a></h4><ul><li><code>http://darkzzx4avcsuofgfez5zq75cqc4mprjvfqywo45dfcaxrwqg6qrlfid.onion</code></li></ul><h4 id=\"darknet-avengers-forum\"><a href=\"https://dark.fail/dna\">Darknet Avengers Forum</a></h4><ul><li><code>http://avengersdutyk3xf.onion</code></li></ul><h4 id=\"dread\"><a href=\"https://dark.fail/dread\">Dread</a></h4><ul><li><code>http://dreadditevelidot.onion</code></li><li><code>http://dreadytofatroptsdj6io7l3xptbet6onoyno2yv7jicoxknyazubrad.onion</code></li></ul><h4 id=\"recon\"><a href=\"https://dark.fail/recon\">Recon</a></h4><ul><li><code>http://reconponydonugup.onion</code></li></ul><h4 id=\"envoy-forum\"><a href=\"https://dark.fail/envoy\">Envoy Forum</a></h4><ul><li><code>http://envoys5appps3bin.onion</code></li><li><code>http://envoyvvcccvepc5i.onion</code></li><li><code>http://envoyzlbxpgniels.onion</code><a href=\"https://dark.fail/envoy\">3 more...</a></li></ul><h4 id=\"the-hub-forum\"><a href=\"https://dark.fail/thehub\">The Hub Forum</a></h4><ul><li><code>http://thehub7xbw4dc5r2.onion</code></li></ul><h4 id=\"the-majestic-garden\"><a href=\"https://dark.fail/tmg\">The Majestic Garden</a></h4><ul><li><code>http://talismanrestz7mr.onion</code></li><li><code>http://nzlbyrcvvqtrkxiu.onion</code></li><li><code>http://2oywvwmtzdelmiei.onion</code><a href=\"https://dark.fail/tmg\">2 more...</a></li></ul><h4 id=\"torum\"><a href=\"https://dark.fail/torum\">Torum</a></h4><ul><li><code>http://torum43tajnrxritn4iumy75giwb5yfw6cjq2czjikhtcac67tfif2yd.onion</code></li></ul><h4 id=\"kilos\"><a href=\"https://dark.fail/kilos\">Kilos</a></h4><ul><li><code>http://dnmugu4755642434.onion</code></li></ul><h4 id=\"empire-market\"><a href=\"https://dark.fail/empire\">Empire Market</a></h4><ul><li><code>http://erj7kwqkdkl73ewsuq6stztehx2tehk2aidxlex3btrfnjqax3ucvgyd.onion</code><a href=\"https://dark.fail/empire\">5 more...</a></li></ul><h4 id=\"empire-market-forum\"><a href=\"https://dark.fail/empire-forum\">Empire Market Forum</a></h4><ul><li><code>http://empire23d4mne2afpn4lnemfahydxp6dv3c7s6tekii3g7vel3xs7rad.onion</code></li></ul><h4 id=\"monopoly-market\"><a href=\"https://dark.fail/monopoly\">Monopoly Market</a></h4><ul><li><code>http://monopolyberbucxu.onion</code></li></ul><h4 id=\"white-house-market\"><a href=\"https://dark.fail/whitehouse\">White House Market</a></h4><ul><li><code>http://auzbdiguv5qtp37xoma3n4xfch62duxtdiu4cfrrwbxgckipd4aktxid.onion</code></li><li><code>http://7yipwxdv5cfdjfpjztiz7sv2jlzzjuepmxy4mtlvuaojejwhg3zhliqd.onion</code></li><li><code>http://cieprrpdgp7moka2ktlwy54ooymtgsre23enrf4dfzssap74zz45f6id.onion</code><a href=\"https://dark.fail/whitehouse\">5 more...</a></li></ul><h4 id=\"dark-market\"><a href=\"https://dark.fail/darkmarket\">Dark Market</a></h4><ul><li><code>http://darkmarketsomqvzqfjudpd6t5eabgvvpplrbtzq6prervyogenlrlqd.onion</code></li><li><code>http://darknet3ysxpbjtvbtyqorzuq3bp4pk3vt4fuhqxz6h2bzj2kyjbisqd.onion</code></li><li><code>http://darknet4rbfizlg53dwc5lt5hj4mewcgltubcpvrfpvjavm64inaf3ad.onion</code><a href=\"https://dark.fail/darkmarket\">1 more...</a></li></ul><h4 id=\"darkmarket\"><a href=\"https://dark.fail/darkmarket\">DarkMarket</a></h4><ul><li><code>http://darkevuygggqkqhq.onion</code></li><li><code>http://darkmarkl7f5hz5f.onion</code></li><li><code>http://darkmarkhbh2xyel.onion</code><a href=\"https://dark.fail/darkmarket\">1 more...</a></li></ul><h4 id=\"tor-market\"><a href=\"https://dark.fail/tormarket\">Tor Market</a></h4><ul><li><code>http://tt2mopgckifmberr.onion</code></li></ul><h4 id=\"imperiya-vendor-shops\"><a href=\"https://dark.fail/imperiya\">Imperiya Vendor Shops</a></h4><ul><li><code>http://imperiyakggyacaf.onion</code></li></ul><h4 id=\"the-versus-project\"><a href=\"https://dark.fail/verses\">The Versus Project</a></h4><ul><li><code>http://pqqmr3p3tppwqvvapi6fa7jowrehgd36ct6lzr26qqormaqvh6gt4jyd.onion</code></li><li><code>http://q2f7swt5yvbhciqqbbsidufu2vtkv6ivwy6g5i5ukejjlb2jeghd2had.onion</code></li></ul><h4 id=\"cannahome\"><a href=\"https://dark.fail/cannahome\">Cannahome</a></h4><ul><li><code>http://cannahome3ke3366.onion</code></li><li><code>http://cannahomekql6hhg.onion</code></li></ul><h4 id=\"cannazon-market\"><a href=\"https://dark.fail/cannazon\">Cannazon Market</a></h4><ul><li><code>http://cannazon4gbjluus.onion</code></li><li><code>http://sxwjdzct7jnoef7o.onion</code></li><li><code>http://cannazonceujdye3.onion</code><a href=\"https://dark.fail/cannazon\">3 more...</a></li></ul><h4 id=\"deepsea-market\"><a href=\"https://dark.fail/deepsea\">DeepSea Market</a></h4><ul><li><code>http://dsmktyc4q6i4ycvjibd6lf732l66vcvjmhsce5x22bomtrrwdobzbfad.onion</code></li><li><code>http://dsmktipxelmq3666psdthvz6obow5thplftw7gww4hi3vmendm6tsaad.onion</code></li><li><code>http://dsmktvpejm3sj5c5ua7az5hnx5mo36ewwixaj3rfbbwudfoyxaz7xxid.onion</code></li></ul><h4 id=\"square-market\"><a href=\"https://dark.fail/square\">Square Market</a></h4><ul><li><code>http://c77yo2fe3f4e3g7tll5qrzzoneymihws2tpjceg6wop2ky6pkcaqczyd.onion</code></li></ul><h4 id=\"televend\"><a href=\"https://dark.fail/televend\">Televend</a></h4><ul><li><code>http://televendmnsfbznh.onion</code></li></ul><h4 id=\"torrez-market\"><a href=\"https://dark.fail/torrez\">ToRReZ Market</a></h4><ul><li><code>http://yxuy5oau7nugw4kpb4lclrqdbixp3wvc4iuiad23ebyp2q3gx7rtrgqd.onion</code></li><li><code>http://lstkx6p3gzsgfwsqpntlv7tv4tsjzziwp76gvkaxx2mqe3whvlp243id.onion</code></li><li><code>http://f3dxwzcmojrlphehqwcecwe3amg6rkzgilcn4xqxmaiezoedijf6rtid.onion</code><a href=\"https://dark.fail/torrez\">1 more...</a></li></ul><h4 id=\"hydra\"><a href=\"https://dark.fail/hydra\">Hydra</a></h4><ul><li><code>http://hydraruzxpnew4af.onion</code></li></ul><h4 id=\"exploit-in-forum\"><a href=\"https://dark.fail/exploit-in\">Exploit.in Forum</a></h4><ul><li><code>http://exploitinqx4sjro.onion</code></li></ul><h4 id=\"tochka-git-repository\"><a href=\"https://dark.fail/tochka-git\">Tochka Git Repository</a></h4><h5 id=\"offline\">Offline</h5><ul><li><code>http://qxklmrhx7qkzais6.onion</code></li></ul><h4 id=\"tochka-market\"><a href=\"https://dark.fail/tochka\">Tochka Market</a></h4><h5 id=\"offline-1\">Offline</h5><ul><li><code>http://34n4am6fxu4dnv66.onion</code></li><li><code>http://pointgg344ghbo2s.onion</code></li></ul><h4 id=\"verified-forum\"><a href=\"https://dark.fail/verified\">Verified Forum</a></h4><ul><li><code>http://verified2ebdpvms.onion</code></li></ul><h4 id=\"canadahq\"><a href=\"https://dark.fail/canada-hq\">CanadaHQ</a></h4><ul><li><code>http://canadahq2lo3logs.onion</code></li><li><code>http://canadahqx53lcurj.onion</code></li></ul><h4 id=\"deutschland-im-deep-web-forum\"><a href=\"https://dark.fail/deutschland-im-deep-web\">Deutschland im Deep Web Forum</a></h4><ul><li><code>http://germanyruvvy2tcw.onion</code></li></ul><h4 id=\"uas-service-rdp\"><a href=\"https://dark.fail/uas-service-rdp\">UAS Service RDP</a></h4><ul><li><code>http://2x4tmsirlqvqmwdz.onion</code></li></ul><h4 id=\"wannabuy-rdp\"><a href=\"https://dark.fail/wannabuy\">Wannabuy RDP</a></h4><ul><li><code>http://wannabuyaynozvmz.onion</code></li></ul><h4 id=\"yellow-brick-market\"><a href=\"https://dark.fail/yellow-brick\">Yellow Brick Market</a></h4><ul><li><code>http://ck73ugjvx5a4wkhsmrfvwhlrq7evceovbsb7tvaxilpahybdokbyqcqd.onion</code></li></ul><h4 id=\"wayaway\"><a href=\"https://dark.fail/wayaway\">Wayaway</a></h4><h5 id=\"offline-2\">Offline</h5><ul><li><code>http://wayawaytcl3k66fl.onion</code></li></ul><h4 id=\"bbc\"><a href=\"https://dark.fail/bbc\">BBC</a></h4><ul><li><code>http://bbcnewsv2vjtpsuy.onion</code></li></ul><h4 id=\"bitmessage-gateway\"><a href=\"https://dark.fail/bm\">Bitmessage Gateway</a></h4><ul><li><code>http://bitmailendavkbec.onion</code></li></ul><h4 id=\"deutsche-welle\"><a href=\"https://dark.fail/deutsche-welle\">Deutsche Welle</a></h4><ul><li><code>http://dwnewsvdyyiamwnp.onion</code></li></ul><h4 id=\"pornhub\"><a href=\"https://dark.fail/pornhub\">Pornhub</a></h4><ul><li><code>http://pornhubthbh7ap3u.onion</code></li></ul><h4 id=\"propublica\"><a href=\"https://dark.fail/propublica\">ProPublica</a></h4><ul><li><code>http://propub3r6espa33w.onion</code></li></ul><h4 id=\"protonmail\"><a href=\"https://dark.fail/proton\">Protonmail</a></h4><ul><li><code>https://protonirockerxow.onion</code></li></ul><h4 id=\"riseup\"><a href=\"https://dark.fail/riseup\">Riseup</a></h4><ul><li><code>http://nzh3fv6jc6jskki3.onion</code></li><li><code>http://zsolxunfmbfuq7wf.onion</code></li><li><code>http://j6uhdvbhz74oefxf.onion</code><a href=\"https://dark.fail/riseup\">1 more...</a></li></ul><h4 id=\"the-calyx-institute-jabber-\"><a href=\"https://dark.fail/calyx\">The Calyx Institute (Jabber)</a></h4><ul><li><code>http://ijeeynrc6x2uy5ob.onion</code></li></ul><h4 id=\"the-new-york-times\"><a href=\"https://dark.fail/nyt\">The New York Times</a></h4><ul><li><code>https://nytimes3xbfgragh.onion</code></li></ul><h4 id=\"torbox-email\"><a href=\"https://dark.fail/torbox\">TorBox Email</a></h4><ul><li><code>http://torbox3uiot6wchz.onion</code></li></ul><h4 id=\"agoradesk\"><a href=\"https://dark.fail/agora-desk\">AgoraDesk</a></h4><ul><li><code>http://agoradeska6jfxpf.onion</code></li></ul><h4 id=\"localmonero-co\"><a href=\"https://dark.fail/local-monero\">LocalMonero.co</a></h4><ul><li><code>http://localmonerogt7be.onion</code></li></ul><h4 id=\"the-tor-project\"><a href=\"https://dark.fail/tor\">The Tor Project</a></h4><ul><li><code>http://expyuzz4wqqyqhjn.onion</code></li></ul><h4 id=\"xmrguide\"><a href=\"https://dark.fail/xmrguide\">xmrguide</a></h4><ul><li><code>http://xmrguide42y34onq.onion</code></li></ul><h4 id=\"cryptostorm-vpn\"><a href=\"https://dark.fail/cryptostorm\">Cryptostorm VPN</a></h4><ul><li><code>http://stormgm7blbk7odd.onion</code></li></ul><h4 id=\"ipredator-vpn\"><a href=\"https://dark.fail/ipredator\">IPredator VPN</a></h4><ul><li><code>https://fwtnuwekoeayj3s7.onion</code></li></ul><h4 id=\"mullvad-vpn\"><a href=\"https://dark.fail/mullvad\">Mullvad VPN</a></h4><ul><li><code>http://xcln5hkbriyklr6n.onion</code></li></ul><h4 id=\"njal-la\"><a href=\"https://dark.fail/njalla\">Njal.la</a></h4><ul><li><code>http://njalladnspotetti.onion</code></li></ul><h4 id=\"archive-is\"><a href=\"https://dark.fail/archive-is\">Archive.is</a></h4><ul><li><code>http://archivecaslytosk.onion</code></li></ul><h4 id=\"duckduckgo\"><a href=\"https://dark.fail/duckduckgo\">DuckDuckGo</a></h4><ul><li><code>https://3g2upl4pq6kufc4m.onion</code></li></ul><h4 id=\"facebook\"><a href=\"https://dark.fail/facebook\">Facebook</a></h4><ul><li><code>https://facebookcorewwwi.onion</code></li></ul><h4 id=\"keybase\"><a href=\"https://dark.fail/keybase\">Keybase</a></h4><ul><li><code>http://fncuwbiisyh6ak3i.onion</code></li></ul><h4 id=\"psychonaut-wiki\"><a href=\"https://dark.fail/psychonaut\">Psychonaut Wiki</a></h4><ul><li><code>http://psychonaut3z5aoz.onion</code></li></ul><h4 id=\"8chan\"><a href=\"https://dark.fail/8chan\">8chan</a></h4><h5 id=\"offline-3\">Offline</h5><ul><li><code>http://oxwugzccvk3dk6tj.onion</code></li></ul><h4 id=\"8kun-formerly-8chan-\"><a href=\"https://dark.fail/8kun\">8kun (formerly 8chan)</a></h4><ul><li><code>http://jthnx5wyvjvzsxtu.onion</code></li></ul><h4 id=\"endchan\"><a href=\"https://dark.fail/endchan\">Endchan</a></h4><ul><li><code>http://enxx3byspwsdo446jujc52ucy2pf5urdbhqw3kbsfhlfjwmbpj5smdad.onion</code></li><li><code>http://endchan5doxvprs5.onion</code></li><li><code>http://s6424n4x4bsmqs27.onion</code></li></ul><h4 id=\"distributed-denial-of-secrets\"><a href=\"https://dark.fail/ddosecrets\">Distributed Denial of Secrets</a></h4><ul><li><code>http://ddosecretspzwfy7.onion</code></li></ul><h4 id=\"elude-in\"><a href=\"https://dark.fail/elude\">Elude.in</a></h4><ul><li><code>http://eludemaillhqfkh5.onion</code></li></ul><h4 id=\"secmail\"><a href=\"https://dark.fail/secmail\">Secmail</a></h4><ul><li><code>http://secmailw453j7piv.onion</code></li></ul><h4 id=\"coinpayments\"><a href=\"https://dark.fail/coinpayments\">CoinPayments</a></h4><h5 id=\"offline-4\">Offline</h5><ul><li><code>https://coinpaymtstgtibr.onion</code></li></ul><h4 id=\"sks-pgp-keyserver-pool\"><a href=\"https://dark.fail/sks\">SKS PGP Keyserver Pool</a></h4><ul><li><code>http://jirk5u4osbsr34t5.onion</code></li></ul><h4 id=\"the-pirate-bay\"><a href=\"https://dark.fail/the-pirate-bay\">The Pirate Bay</a></h4><ul><li><code>http://suprbayoubiexnmp.onion</code></li></ul><h4 id=\"torch-search\"><a href=\"https://dark.fail/torch\">Torch Search</a></h4><ul><li><code>http://xmh57jrzrnw6insl.onion</code></li></ul><h4 id=\"wikileaks\"><a href=\"https://dark.fail/wikileaks\">WikiLeaks</a></h4><ul><li><code>http://wlchatc3pjwpli5r.onion</code></li></ul><h4 id=\"chronos\"><a href=\"https://dark.fail/chronos\">Chronos</a></h4><h5 id=\"offline-5\">Offline</h5><ul><li><code>http://chronosu3ulk3o3a.onion</code></li></ul><h4 id=\"tape\"><a href=\"https://dark.fail/tape\">Tape</a></h4><ul><li><code>http://tapeucwutvne7l5o.onion</code></li></ul><h4 id=\"national-police-of-the-netherlands\"><a href=\"https://dark.fail/netherlands-police\">National Police of the Netherlands</a></h4><ul><li><code>http://politiepcvh42eav.onion</code></li></ul><h4 id=\"the-cia-s-official-onion-site\"><a href=\"https://dark.fail/cia\">The CIA's Official Onion Site</a></h4><ul><li><code>http://ciadotgov4sjwlzihbbgxnqg3xiyrg7so2r2o3lt5wz5ypk4sxyjstad.onion</code></li></ul><h4 id=\"the-fbi-s-ncide-task-force\"><a href=\"https://dark.fail/fbi-ncide\">The FBI's NCIDE Task Force</a></h4><ul><li><code>http://ncidetf3j26mdtvf.onion</code></li></ul><h4 id=\"dream-market-s-partner-announced-during-shutdown-\"><a href=\"https://dark.fail/dream-partner\">Dream Market's Partner (announced during shutdown)</a></h4><h5 id=\"offline-6\">Offline</h5><ul><li><code>http://weroidjkazxqds2l.onion</code></li></ul><h4 id=\"dark-fail\"><a href=\"https://dark.fail/df\">dark.fail</a></h4><ul><li><code>http://darkfailllnkf4vf.onion</code></li></ul><h3 id=\"philosophy\">Philosophy</h3><p>Always fast, one server request. No tracking ever, no javascript ever.</p><p>Accurate URLs verified by PGP. No direct linking in order to protect against DNS leaks from accidental clicking in a clearnet browser.</p><p>Knowledge of darknet site uptime is important to many cybersecurity researchers. This site is provided for information only. No endorsements are made or implied regarding any sites or organizations mentioned here.</p><p>Want a link listed or removed here? Email hello -a-t- dark.fail</p><p>Jabber: darkdotfail at jabber.calyxinstitute.org</p><p>Defend your right to privacy: <a href=\"https://supporters.eff.org/\">donate to the EFF.</a></p><h5 id=\"donations-are-appreciated\">Donations are Appreciated</h5><p>dark.fail is supported by our users. No sites pay for placement or advertisements, no affiliate links have or will ever be used. If this resource has helped you please consider sending a contribution.</p><p>Bitcoin: 1EWXzHdvHFBYgr8fHyiNfFuW9A9etyNVqC</p><p>Monero: 88qw83dNPkGdnsxN5FWHvg2D7QmkjbybYLwhi7EyNkTfhz8tFWP5L2w9XmHXuY2WAKRMmwz7Ro6PcZ6gDPYR6Xb99FaKRca</p><h5 id=\"pgp-key-begin-pgp-public-key-block-mqinbf1pnpgbeac0vrfw12sxkmy7naa0sdo1xfctfartmtwct-rjc-syv-rzdoae-yalc7n6ucos6mlbgwd8-izi7lol7not11z7aiptrz-anqpaecl2hqbp6q8ubnd3p-p5x-lnwhuchgspv-pbnk49tt4-awu9qz2g1zadzohbgu9-bgxwncs7nmb0jvtkz8-cipenongtldiqejdwx8m92geh9nmnjrysl6aj-mosco8cko0bqnyp5zwi-jhn2sw-379qr7acaxremqcf12oxxgzxwnhxhz9hpc5jpxovqzl9sdyulvkkmk8jrqstmi9s-epmg-khyr-snwpfbkl6p51qv-nq9ora8assizce7iejengriejq0d4cjzh7bkcvs-mvwgy4behdwgt4o14mu1xuvdn9yo33hakaxmnhbs3i8hgeghb35zyorzs7nlyx4x-ay-x9obmtpwc5smvegkp6ythgi-qzbc80wtuageangiyjmv9twkizijtquujascb-mzbcmtwnx04vztggqgcgb-vegvuj1irgrryrwcec2eru-arfan8znfk6lgbcdtcq-y9gjweier4wquhlywgofpv3wpd72yzjdhy0zmaqjqwfiydui23aznrhqg2jz-1e1-w6ppmrjlv-bbyaidigbodskh0mjjmk2exjlc6lhwnby4-qnjfq2i3ow8qaraqab-tb1eyxjrrg90rmfpbca8agvsbg9azgfyay5mywlspokcvaqtaqoaphyhbg39rmbu-p8huzrh-folkadx9jd2ibqjdt5z4ahsdbqkb-pribqsjcacdbrukcqglbrycawea-ah4baheaaaojeolkadx9jd2izxoqakud5bpstuvp62b5eov3h1ezhrwqo4nsxi64-banmytk6hkzuhy77x2t-blwcwknfpkgpqvys0srdbduqypxzcx1ssqdl3yplakqh-z-uupkxbkezgbrqu2n283b0g-ad4rr4rzhysbbbfb5btwbnz3qpmfgb6ymunt-vh-cpevkdkxpqqxfbq3viwo2xogj2q6misw2ujgfa2d3aamm73hqsstmxhkpq74klnm-bk4dwb3ig5m9avpymv8pb-6rzkg3wohhsmsoyhbj26ao-xayk71qopp-izeddbyr-ez7ozxxa4nb4b4spgdv5515aa1g-4ar6f86ywz2yrtoqcmlmtjob9qpl0aoraqrt-mdmvqsegg7fnw7fxe580dtm36qalzfdw5wqbn4-wamerpwckr0x88rmavn8mg8ey-soibayzu2rbecvbzotcwlpkeqbh6m5a0xmdckf7sokzbzkgz6jzcjegkufqdrs3h-akpxhe4pljafk5inw8jzxy-d59-douzzz0sjdblyocy3cf8e3q2rjz-kfk6vr23j-c63g-avfbap1xvfkyumt-nr-j4vd1eximtnp7i2vl-faoa1gkmbkr5zauulfypkq-ugjjnbkniwfo-axppxswtxjz-sejkd-dhgbs2n9-i8qsp1bw8uesx8h6pjmiio4s-pxxhipdcuqinbf1pnpgbeac3wks0hofvdu6qw1wsmsk0u8ukco3psaxi2go10tkp-jmpkbj4qmw3nsqdeenwx0rakwvlifghtjx2rfk75zhgc3dqy6tpod3iyenixzkus-xiapc0uvwm-cxn5i0iplp-rc2qlqz-ratrstkf2mhxs-ixktgb0puxkq7vhbxtre-qnyuaceuubhtisuhx9a8btbbk3zktwc2ysbl0ctyemkj1-22zfnmqdwuo1cp1ywn-siv88xt-izzmcvcoctod-uuqllxyizsce0elnvsqkglts-zef-lnv-wfqp7akn2a-r8mxkxkd1m5hszyay4ovvdr-c6pisupi1atqlb1-97qiw6mfthkl4pqdvejxjzua-3coyxlrprqtth6a81bcath-7gpnyxihyejtrdb9awuqikn3ineyg0grwlwbx-fms-jnfcds2cl-pkexykxbgjxex1d36vawim8tqeqiuene3bypapl8tf9mth5l-jour2-uy61ajsp6vja7-z3iw-pq7xnh73-yhqhgmpm38uarfpo7goi3v-awwnikxd3rue-ppdskjuouyemt80dloskmrxerwep2m8l1shiocbzmu9imtlnentrgpfrg8-box59-ia8icd2kauyifdvqsosjacgitjqxcbfogp5fdgkonyyefjnblc80ougi5dyntzkr-qqaraqabiqi8bbgbcgamfieebf2uztq-we7ouh584urp3h2mpaifal1pnpgcgwwf-cqh-msgacgkq4urp3h2mpajfcg-rz-exjtuxcjpksbzwbgshf7eexvivt6s12t9-rfcapgzlp0cb-g7ajzwqlhj9ly50oy8dfxlx-xgcyvub9rfh6ui-dqyeh8fgomp-wtsreamedtppv5xpygrneizk2z6hudflcyepanqvpf03c63-ujdullnwsb3h3tkb-ne-jkheglshsrlxzdw2fz5jhzcp6oarax6gflvnhyy5mhpmr9tbp8qkze83dq54q-s2-5xizjtnsibfk5f1urx4zpot4qkugegz8p7wiveoxu6q2hu-ltotkjsyd-g0dy-hxbluydmjtrp3q2cr6mwgohvaauqfcb7jeusa3doaaqg4b-btz4ht6bktvx2qgns-ai-dmq6pm8znoscwxcnvghzsigfu310kutq7lzygl-zrpsromd0nw9xgfnjfi5k1-ybitfbfog2be-ryodzb4lyyyd9dzzwtvawlsgjjkkvj1holgwvwhspiht6h5z614-cxt7s2kqtp7vn-yzdsplzt1eofv5-jwtn8jw3c-kqezwfrqeuqlym1wxk79ipzke-ykpx-9d8ty2qjaef5r480pr0yar8jdx4-rkqjoh-41h77hjwsofmj7wpremigui-euxceujoagud4h05mi2igvzkengudehsijw3ivs3nv6-79yt4dqdjmmgyjpfqwjm-qpbfe6i-j6e1-end-pgp-public-key-block-\">PGP Key-----BEGIN PGP PUBLIC KEY BLOCK----- mQINBF1PnPgBEAC0vRFW12SXkmY7NAA0sDO1XFCtfartMtwCT+RJc/SYv/RZdoaE YaLC7n6UCOs6mlBgwd8+iZI7LOL7nOt11z7AiPtrZ/ANqpaEcl2HqBP6q8uBnd3P P5x/LNWHUcHGSPV/pbNK49tT4/awU9QZ2g1ZaDzOHbGU9/BgXWnCs7nmb0jvTkz8 cipenoNGTldiqEJDWx8m92geh9NMNjrYSL6aj/MoscO8CKo0BQnyP5zwi/jhn2sw 379qr7aCAXrEmqCf12oXXGzXWnHXhz9HpC5jPxoVQZL9SdyulvKkMk8jrqStmi9S EpmG+KhYr+sNWpFBkL6P51qV+NQ9oRA8ASSIZCe7IejEnGRIEjq0d4CJzh7BkcVS mvwGy4beHdWgt4o14MU1XuvDn9YO33hAkAxMnhBs3I8hGeghb35zyORZS7nlyx4X AY+X9ObMTpWc5sMvEgkP6ythGI/QzBc80wTUAGeAnGiyjmV9TwkIZIJTqUujascB mzbCMTWnX04VZtggQGcGb+vEgvUJ1iRGrRyrWCeC2eRU/ArfAN8ZNfk6LGBcdTCq Y9gjWEIEr4WQUHLyWgOfpV3Wpd72YzJdHy0zMaQJqWfiyDUI23AZNrhqg2jz+1E1 +w6pPMrjlV+BByAIdIGBOdSkh0MJjmk2ExjLc6LHwnBy4+Qnjfq2I3oW8QARAQAB tB1EYXJrRG90RmFpbCA8aGVsbG9AZGFyay5mYWlsPokCVAQTAQoAPhYhBG39rmbU P8Huzrh+fOLkadx9jD2iBQJdT5z4AhsDBQkB/prIBQsJCAcDBRUKCQgLBRYCAwEA Ah4BAheAAAoJEOLkadx9jD2iZXoQAKUD5bpsTUvP62b5eOV3h1EZhrwQo4NSxi64 banmyTK6hKZuhY77x2t/bLWcWkNfPkgpqVYS0SRdbduqYPXzCX1ssQDl3ypLakqH z+UUpKxBkEZgBRQU2N283b0g+AD4Rr4RzHYsBBbfB5BtWBnZ3qpMFGB6ymUnT+vh CPEVkdkxpQqXfbq3Viwo2XOGj2Q6MISw2UJGFA2d3AAMM73hqsSTMXHKPQ74KlnM bk4DWB3Ig5m9aVpyMV8pB/6rZKG3wOHhSmsOYHBj26Ao+XAyk71qOpp/IZEDDBYr EZ7OZxxA4nB4B4SpGDv5515aa1G+4ar6F86ywz2yrtOQcMlMtJoB9QpL0aOraqRT mDMVqSeGG7FnW7FxE580dTm36qAlzfdW5WqBN4+WAmERPwckR0x88rmaVN8mG8eY soiBAYZU2RbecvbZotCWLPkeqBH6m5a0xmDcKf7SOKZbZKGz6JZcJeGKuFQDrs3h akpxHE4PlJAfk5InW8JZxY/D59+douzzz0sJDBLyoCY3cf8E3q2rJz/KFK6vr23j c63g/AVFbAP1XvfKyumT/nR/J4VD1ExImtnP7i2vl/faoa1gkMbkR5zAuulFyPkQ UGJjnBKniwfO/axppXSwtXJZ/SEJkd/DhGBs2n9+i8qsp1bW8UeSx8h6PjMIio4S pXXHipdcuQINBF1PnPgBEAC3Wks0hoFvdU6qw1wsmsK0U8ukcO3PsaxI2go10TKP jmPKBJ4qmW3nSqdEENwx0RakWVlIFgHtJX2Rfk75zhGc3DQy6tpoD3IyEniXzkUS xIApc0uvWM+CXN5I0Iplp+RC2qlqZ/ratRStkF2mhXS+IxKTgB0puXkq7VhbXTre QnyuacEuubhtisUhx9a8btbbK3zKTWC2ySBl0ctYemkJ1+22zFnmqdwUo1cp1YwN Siv88xT+iZZMcvcoctod+UuQLLXyiZsCE0ELnVSQKGlTS+zef/LnV/WFqP7AKN2A r8MxKXKd1M5hszYAY4ovvDR/C6pIsuPI1aTQLB1/97qIW6MFtHkl4PqDvEJxJZuA 3COYXLrpRQtTH6A81Bcath+7GPNYxIHyEjtRdB9aWUQIkN3inEYG0grWlWBX/Fms jNfcds2cl+PKeXYKXbGjXex1d36VawIm8TqeQIueNe3BypapL8tF9mth5l+JoUr2 Uy61AJSP6VJa7/Z3iW//Pq7xNH73/yHqHGMpM38UArFPO7goI3V/AWWNikxD3rUe PPDSkjUouYEmt80dloSkmRXErWEP2M8l1sHiOcBZmu9imtlneNtrGpfrG8+Box59 Ia8ICd2KAUyiFDVqsOsjacGiTJqXcbfogp5fdGkONyYEfJnbLc80OuGi5dyntzKR qQARAQABiQI8BBgBCgAmFiEEbf2uZtQ/we7OuH584uRp3H2MPaIFAl1PnPgCGwwF CQH+msgACgkQ4uRp3H2MPaJFCg//Rz+EXjTUXCjPkSbZwbgSHF7eExvivt6s12t9 rfcAPGzLP0cb+/G7AjZwqlHj9Ly50oy8DFXlx/XGcyVUB9RFH6uI+Dqyeh8FGOmp wtSReAMEdTppV5XPYgrNEIzK2Z6HUdFLcYepANQVPf03c63/uJdUlLnWSb3h3TKb nE+JkHeglsHSrLXzdw2fZ5jhZCp6oaRax6GFLvNhYY5MHPMr9tBP8Qkze83Dq54Q S2+5XIzjtNSIBfK5F1urX4ZpOT4qKuGeGz8p7wiveOxU6q2Hu+ltotkJSYD/G0dy hXbLuyDmJtrp3Q2CR6mwgoHVAAUQFCB7JeUsA3dOAAqg4B/btZ4ht6bKTvx2qGnS aI+Dmq6pm8ZNoSCWXcNVGhzSigfU310KUtQ7LzYGl/ZRPSROmd0nw9XgFNJFi5k1 YbITFbfOg2BE+rYOdZB4LyyyD9dzZWtvawLSGjjKkvj1HOlGwvWHSPIhT6h5Z614 cXt7S2kqTp7Vn+YzDSPLZt1eofV5+jWTn8Jw3C/KQezWfrQeUQlYm1WxK79IPZkE yKPX/9D8Ty2qjAEf5R480Pr0yAR8jDx4+RKQJOH/41H77hJwsOfmJ7wPREmigui/ EUXcEuJOagUd4H05mi2iGvzKengudehsIJw3IVs3nV6/79YT4DqdJMMGyjPFQwjm qpBfe6I= =J6e1 -----END PGP PUBLIC KEY BLOCK-----</h5><p><a href=\"https://dark.fail/pgp-sigs.txt\">Key rotated on 2019-08-11</a></p><p><strong>ADMINS:</strong> Introducing the Onion Mirror Guidelines. <a href=\"https://dark.fail/spec/omg.txt\">Implement the \"OMG\"</a> to remain listed on dark.fail.</p>","comment_id":"64","plaintext":"https://www.thedarkweblinks.com/darknet-market-list/\n\nhttps://darknetlive.com/darknet-market-list/\n\n\n--------------------------------------------------------------------------------\n\nhttps://news.bitcoin.com/beginners-guide-buying-goods-darknet/\n\n\n--------------------------------------------------------------------------------\n\ndark.fail: Is a darknet site online?Updated Sat, 08 Aug 2020 15:27:25 UTC\nMastodon [https://mastodon.social/@darkdotfail] | Twitter\n[https://nitter.dark.fail/darkdotfail]\nWe apologize for the recent downtime. This was caused by a technical mistake\nthat was entirely our fault. This single point of failure is now being\nredesigned to prevent this from happening in the future. Updated canary.\n[https://dark.fail/canary.txt]\n\nNEW: Verify signatures with Dark.fail's new PGP Tool [https://dark.fail/pgp].\nDon't get phished. Always PGP verify .onion and Bitcoin addresses before\ninteracting with them.\n\nTor is the uncensored internet. Install Tor Browser [https://www.torproject.org] \nto explore it. Set darkfailllnkf4vf.onion as your home page to save time. Links\nare PGP verified and unclickable for your safety. dark.fail's philosophy\n[https://dark.fail/#philosophy] and finances [https://dark.fail/#donations]\n\nThis resource is intended for researchers only. I do not vouch for any sites.\n\nDarknet Live [https://dark.fail/darknetlive]\n * http://darkzzx4avcsuofgfez5zq75cqc4mprjvfqywo45dfcaxrwqg6qrlfid.onion\n\nDarknet Avengers Forum [https://dark.fail/dna]\n * http://avengersdutyk3xf.onion\n\nDread [https://dark.fail/dread]\n * http://dreadditevelidot.onion\n * http://dreadytofatroptsdj6io7l3xptbet6onoyno2yv7jicoxknyazubrad.onion\n\nRecon [https://dark.fail/recon]\n * http://reconponydonugup.onion\n\nEnvoy Forum [https://dark.fail/envoy]\n * http://envoys5appps3bin.onion\n * http://envoyvvcccvepc5i.onion\n * http://envoyzlbxpgniels.onion3 more... [https://dark.fail/envoy]\n\nThe Hub Forum [https://dark.fail/thehub]\n * http://thehub7xbw4dc5r2.onion\n\nThe Majestic Garden [https://dark.fail/tmg]\n * http://talismanrestz7mr.onion\n * http://nzlbyrcvvqtrkxiu.onion\n * http://2oywvwmtzdelmiei.onion2 more... [https://dark.fail/tmg]\n\nTorum [https://dark.fail/torum]\n * http://torum43tajnrxritn4iumy75giwb5yfw6cjq2czjikhtcac67tfif2yd.onion\n\nKilos [https://dark.fail/kilos]\n * http://dnmugu4755642434.onion\n\nEmpire Market [https://dark.fail/empire]\n * http://erj7kwqkdkl73ewsuq6stztehx2tehk2aidxlex3btrfnjqax3ucvgyd.onion5\n   more... [https://dark.fail/empire]\n\nEmpire Market Forum [https://dark.fail/empire-forum]\n * http://empire23d4mne2afpn4lnemfahydxp6dv3c7s6tekii3g7vel3xs7rad.onion\n\nMonopoly Market [https://dark.fail/monopoly]\n * http://monopolyberbucxu.onion\n\nWhite House Market [https://dark.fail/whitehouse]\n * http://auzbdiguv5qtp37xoma3n4xfch62duxtdiu4cfrrwbxgckipd4aktxid.onion\n * http://7yipwxdv5cfdjfpjztiz7sv2jlzzjuepmxy4mtlvuaojejwhg3zhliqd.onion\n * http://cieprrpdgp7moka2ktlwy54ooymtgsre23enrf4dfzssap74zz45f6id.onion5\n   more... [https://dark.fail/whitehouse]\n\nDark Market [https://dark.fail/darkmarket]\n * http://darkmarketsomqvzqfjudpd6t5eabgvvpplrbtzq6prervyogenlrlqd.onion\n * http://darknet3ysxpbjtvbtyqorzuq3bp4pk3vt4fuhqxz6h2bzj2kyjbisqd.onion\n * http://darknet4rbfizlg53dwc5lt5hj4mewcgltubcpvrfpvjavm64inaf3ad.onion1\n   more... [https://dark.fail/darkmarket]\n\nDarkMarket [https://dark.fail/darkmarket]\n * http://darkevuygggqkqhq.onion\n * http://darkmarkl7f5hz5f.onion\n * http://darkmarkhbh2xyel.onion1 more... [https://dark.fail/darkmarket]\n\nTor Market [https://dark.fail/tormarket]\n * http://tt2mopgckifmberr.onion\n\nImperiya Vendor Shops [https://dark.fail/imperiya]\n * http://imperiyakggyacaf.onion\n\nThe Versus Project [https://dark.fail/verses]\n * http://pqqmr3p3tppwqvvapi6fa7jowrehgd36ct6lzr26qqormaqvh6gt4jyd.onion\n * http://q2f7swt5yvbhciqqbbsidufu2vtkv6ivwy6g5i5ukejjlb2jeghd2had.onion\n\nCannahome [https://dark.fail/cannahome]\n * http://cannahome3ke3366.onion\n * http://cannahomekql6hhg.onion\n\nCannazon Market [https://dark.fail/cannazon]\n * http://cannazon4gbjluus.onion\n * http://sxwjdzct7jnoef7o.onion\n * http://cannazonceujdye3.onion3 more... [https://dark.fail/cannazon]\n\nDeepSea Market [https://dark.fail/deepsea]\n * http://dsmktyc4q6i4ycvjibd6lf732l66vcvjmhsce5x22bomtrrwdobzbfad.onion\n * http://dsmktipxelmq3666psdthvz6obow5thplftw7gww4hi3vmendm6tsaad.onion\n * http://dsmktvpejm3sj5c5ua7az5hnx5mo36ewwixaj3rfbbwudfoyxaz7xxid.onion\n\nSquare Market [https://dark.fail/square]\n * http://c77yo2fe3f4e3g7tll5qrzzoneymihws2tpjceg6wop2ky6pkcaqczyd.onion\n\nTelevend [https://dark.fail/televend]\n * http://televendmnsfbznh.onion\n\nToRReZ Market [https://dark.fail/torrez]\n * http://yxuy5oau7nugw4kpb4lclrqdbixp3wvc4iuiad23ebyp2q3gx7rtrgqd.onion\n * http://lstkx6p3gzsgfwsqpntlv7tv4tsjzziwp76gvkaxx2mqe3whvlp243id.onion\n * http://f3dxwzcmojrlphehqwcecwe3amg6rkzgilcn4xqxmaiezoedijf6rtid.onion1\n   more... [https://dark.fail/torrez]\n\nHydra [https://dark.fail/hydra]\n * http://hydraruzxpnew4af.onion\n\nExploit.in Forum [https://dark.fail/exploit-in]\n * http://exploitinqx4sjro.onion\n\nTochka Git Repository [https://dark.fail/tochka-git]\nOffline\n * http://qxklmrhx7qkzais6.onion\n\nTochka Market [https://dark.fail/tochka]\nOffline\n * http://34n4am6fxu4dnv66.onion\n * http://pointgg344ghbo2s.onion\n\nVerified Forum [https://dark.fail/verified]\n * http://verified2ebdpvms.onion\n\nCanadaHQ [https://dark.fail/canada-hq]\n * http://canadahq2lo3logs.onion\n * http://canadahqx53lcurj.onion\n\nDeutschland im Deep Web Forum [https://dark.fail/deutschland-im-deep-web]\n * http://germanyruvvy2tcw.onion\n\nUAS Service RDP [https://dark.fail/uas-service-rdp]\n * http://2x4tmsirlqvqmwdz.onion\n\nWannabuy RDP [https://dark.fail/wannabuy]\n * http://wannabuyaynozvmz.onion\n\nYellow Brick Market [https://dark.fail/yellow-brick]\n * http://ck73ugjvx5a4wkhsmrfvwhlrq7evceovbsb7tvaxilpahybdokbyqcqd.onion\n\nWayaway [https://dark.fail/wayaway]\nOffline\n * http://wayawaytcl3k66fl.onion\n\nBBC [https://dark.fail/bbc]\n * http://bbcnewsv2vjtpsuy.onion\n\nBitmessage Gateway [https://dark.fail/bm]\n * http://bitmailendavkbec.onion\n\nDeutsche Welle [https://dark.fail/deutsche-welle]\n * http://dwnewsvdyyiamwnp.onion\n\nPornhub [https://dark.fail/pornhub]\n * http://pornhubthbh7ap3u.onion\n\nProPublica [https://dark.fail/propublica]\n * http://propub3r6espa33w.onion\n\nProtonmail [https://dark.fail/proton]\n * https://protonirockerxow.onion\n\nRiseup [https://dark.fail/riseup]\n * http://nzh3fv6jc6jskki3.onion\n * http://zsolxunfmbfuq7wf.onion\n * http://j6uhdvbhz74oefxf.onion1 more... [https://dark.fail/riseup]\n\nThe Calyx Institute (Jabber) [https://dark.fail/calyx]\n * http://ijeeynrc6x2uy5ob.onion\n\nThe New York Times [https://dark.fail/nyt]\n * https://nytimes3xbfgragh.onion\n\nTorBox Email [https://dark.fail/torbox]\n * http://torbox3uiot6wchz.onion\n\nAgoraDesk [https://dark.fail/agora-desk]\n * http://agoradeska6jfxpf.onion\n\nLocalMonero.co [https://dark.fail/local-monero]\n * http://localmonerogt7be.onion\n\nThe Tor Project [https://dark.fail/tor]\n * http://expyuzz4wqqyqhjn.onion\n\nxmrguide [https://dark.fail/xmrguide]\n * http://xmrguide42y34onq.onion\n\nCryptostorm VPN [https://dark.fail/cryptostorm]\n * http://stormgm7blbk7odd.onion\n\nIPredator VPN [https://dark.fail/ipredator]\n * https://fwtnuwekoeayj3s7.onion\n\nMullvad VPN [https://dark.fail/mullvad]\n * http://xcln5hkbriyklr6n.onion\n\nNjal.la [https://dark.fail/njalla]\n * http://njalladnspotetti.onion\n\nArchive.is [https://dark.fail/archive-is]\n * http://archivecaslytosk.onion\n\nDuckDuckGo [https://dark.fail/duckduckgo]\n * https://3g2upl4pq6kufc4m.onion\n\nFacebook [https://dark.fail/facebook]\n * https://facebookcorewwwi.onion\n\nKeybase [https://dark.fail/keybase]\n * http://fncuwbiisyh6ak3i.onion\n\nPsychonaut Wiki [https://dark.fail/psychonaut]\n * http://psychonaut3z5aoz.onion\n\n8chan [https://dark.fail/8chan]\nOffline\n * http://oxwugzccvk3dk6tj.onion\n\n8kun (formerly 8chan) [https://dark.fail/8kun]\n * http://jthnx5wyvjvzsxtu.onion\n\nEndchan [https://dark.fail/endchan]\n * http://enxx3byspwsdo446jujc52ucy2pf5urdbhqw3kbsfhlfjwmbpj5smdad.onion\n * http://endchan5doxvprs5.onion\n * http://s6424n4x4bsmqs27.onion\n\nDistributed Denial of Secrets [https://dark.fail/ddosecrets]\n * http://ddosecretspzwfy7.onion\n\nElude.in [https://dark.fail/elude]\n * http://eludemaillhqfkh5.onion\n\nSecmail [https://dark.fail/secmail]\n * http://secmailw453j7piv.onion\n\nCoinPayments [https://dark.fail/coinpayments]\nOffline\n * https://coinpaymtstgtibr.onion\n\nSKS PGP Keyserver Pool [https://dark.fail/sks]\n * http://jirk5u4osbsr34t5.onion\n\nThe Pirate Bay [https://dark.fail/the-pirate-bay]\n * http://suprbayoubiexnmp.onion\n\nTorch Search [https://dark.fail/torch]\n * http://xmh57jrzrnw6insl.onion\n\nWikiLeaks [https://dark.fail/wikileaks]\n * http://wlchatc3pjwpli5r.onion\n\nChronos [https://dark.fail/chronos]\nOffline\n * http://chronosu3ulk3o3a.onion\n\nTape [https://dark.fail/tape]\n * http://tapeucwutvne7l5o.onion\n\nNational Police of the Netherlands [https://dark.fail/netherlands-police]\n * http://politiepcvh42eav.onion\n\nThe CIA's Official Onion Site [https://dark.fail/cia]\n * http://ciadotgov4sjwlzihbbgxnqg3xiyrg7so2r2o3lt5wz5ypk4sxyjstad.onion\n\nThe FBI's NCIDE Task Force [https://dark.fail/fbi-ncide]\n * http://ncidetf3j26mdtvf.onion\n\nDream Market's Partner (announced during shutdown)\n[https://dark.fail/dream-partner]\nOffline\n * http://weroidjkazxqds2l.onion\n\ndark.fail [https://dark.fail/df]\n * http://darkfailllnkf4vf.onion\n\nPhilosophy\nAlways fast, one server request. No tracking ever, no javascript ever.\n\nAccurate URLs verified by PGP. No direct linking in order to protect against DNS\nleaks from accidental clicking in a clearnet browser.\n\nKnowledge of darknet site uptime is important to many cybersecurity researchers.\nThis site is provided for information only. No endorsements are made or implied\nregarding any sites or organizations mentioned here.\n\nWant a link listed or removed here? Email hello -a-t- dark.fail\n\nJabber: darkdotfail at jabber.calyxinstitute.org\n\nDefend your right to privacy: donate to the EFF. [https://supporters.eff.org/]\n\nDonations are Appreciated\ndark.fail is supported by our users. No sites pay for placement or\nadvertisements, no affiliate links have or will ever be used. If this resource\nhas helped you please consider sending a contribution.\n\nBitcoin: 1EWXzHdvHFBYgr8fHyiNfFuW9A9etyNVqC\n\nMonero:\n88qw83dNPkGdnsxN5FWHvg2D7QmkjbybYLwhi7EyNkTfhz8tFWP5L2w9XmHXuY2WAKRMmwz7Ro6PcZ6gDPYR6Xb99FaKRca\n\nPGP Key-----BEGIN PGP PUBLIC KEY BLOCK-----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 qpBfe6I= =J6e1\n-----END PGP PUBLIC KEY BLOCK-----\nKey rotated on 2019-08-11 [https://dark.fail/pgp-sigs.txt]\n\nADMINS: Introducing the Onion Mirror Guidelines. Implement the \"OMG\"\n[https://dark.fail/spec/omg.txt] to remain listed on dark.fail.","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2019-12-19 22:31:12","created_by":"1","updated_at":"2020-08-08 16:40:20","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd19","uuid":"5b318301-21e3-4b5b-8adf-81107553cb16","title":"Post War but not Post Confict","slug":"post-war","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"em\"],[\"a\",[\"href\",\"http://www.bbc.co.uk/news/world-asia-21873551\"]],[\"a\",[\"href\",\"http://www.un.org/News/dh/infocus/Sri_Lanka/POE_Report_Full.pdf\"]],[\"a\",[\"href\",\"http://www.un.org/News/dh/infocus/Sri_Lanka/The_Internal_Review_Panel_report_on_Sri_Lanka.pdf\"]],[\"strong\"],[\"a\",[\"href\",\"https://www.change.org/en-AU/petitions/prime-minister-reconsider-chogm-2013-in-sri-lanka\"]],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=r3yPzyM0KMU&bpctr=1582841718\"]],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=EWzlQeVKcUg\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"\\\"\"],[0,[0],1,\"What happened in Sri Lanka was a major Rwanda-like atrocity, in a different scale, where the West didn't care. There was plenty of early warning. This [conflict] has been going on for years and decades. Plenty of things could have been done [to prevent it]. But there was not enough interest\"],[0,[],0,\".\\\" -N. Chomsky\"]]],[1,\"p\",[[0,[],0,\"A bloody civil war that gripped the island nation of Sri Lanka for over a quarter of a century came to a horrific end in 2009. The Liberation Tigers of Tamil Eelam (LTTE) were overwhelmingly defeated by the state led Sinhalese majority government forces. The atrocities committed by the state to reach such a conclusion have been vastly underreported. Estimates of civilian deaths in the final five months of the war range between 9,000 and 75,000 (\"],[0,[1],1,\"BBC, 2013\"],[0,[],0,\"). One United Nations report puts the number at 40,000; most recently they have reported that up to 70,000 civilians could have died in the final 5 months (\"],[0,[2],1,\"UN, 2011\"],[0,[],0,\", & \"],[0,[3],1,\"UN, 2012\"],[0,[],0,\"). If this is true, as Frances Harrison of the Guardian states, the magnitude of the estimated civilian deaths is on ‘the scale of Syria but condensed in speed into 5-6 months whereas Syria has been going on for 18 months [to] two years.’ Former Norwegian diplomat Erik Solheim said ‘There were massive war crimes in the last phase which were probably the biggest bloodshed of the twenty first century.’\"]]],[1,\"p\",[[0,[],0,\"A UN panel of experts reported that the government of Sri Lanka has engaged in:\"]]],[3,\"ul\",[[[0,[],0,\"Killing of civilians through widespread shelling\"]],[[0,[],0,\"Shelling of hospitals and humanitarian objects\"]],[[0,[],0,\"Denial of humanitarian assistance\"]],[[0,[],0,\"Human rights violations suffered by victims and survivors of the conflict, including both Internally Displaced People (IDP) and suspected LTTE cadres\"]],[[0,[],0,\"Human rights violations outside the conflict zone, including against the media and other critics of the Government\"]]]],[1,\"p\",[[0,[],0,\"An insidious and well-planned strategy by the government has meant very little independent reporting, foreign aid bodies being prevented from going into the warzone and to date, the resistance of any post-war independent investigation.\"]]],[1,\"p\",[[0,[],0,\"The host of CHOGM (Commonwealth Heads of Government Meeting) is the same government, the same people who are ultimately responsible. The question of accountability will not go away, particularly for the Tamil people, but also for those courageous journalists now in exile who dared to question the government.\"]]],[1,\"p\",[[0,[],0,\"Sadly, the end of the war has not meant the end of human rights abuses perpetrated by the Sri Lankan government. The once Tamil controlled northern areas are now under military occupation. Many displaced Tamils are unable to return home since the government have seized their property for ‘military purposes’ or for ‘agricultural purposes’. Ongoing disappearances, rape, sexual abuse, land grabbing and beatings are stories that are still coming out of Sri Lanka leading some critics to call the post-war, but not post-conflict scenario a ‘structural genocide’. The Sri Lankan government is unsurprisingly against transparency and judicial independence, as demonstrated by the unconstitutional impeachment and removal of the Chief Justice earlier this year, further showing the abuse of power wielded by the Executive.\"]]],[1,\"p\",[[0,[],0,\"For many progressives, the recent Labor asylum policy announcement borne out of political pragmatism rather than compassion and morality has come as an utter disappointment. However, it raises the challenge of changing the hearts and minds of the electorate. This will occur through learning about the plight that refugees flee. Each country’s refugee has its unique and often complex story. This motion aims to shed light on the Sri Lankan Tamils’ story while also advocating for diplomatic pressure to be placed on the Sri Lankan Government.\"]]],[1,\"p\",[[0,[4],1,\"What can we do about it?\"]]],[1,\"p\",[[0,[],0,\"International pressure has been shown to be an effective tool in pressuring governments to do the right thing. The Sri Lankan government may not care about the Tamils but they do care about the Commonwealth. A boycott of CHOGM would make a good start. Apartheid is a good example of an international sporting boycott that raised awareness and pressured a nation towards corrective action. A boycott of Sri Lankan cricket, I believe, would mount enough pressure both internationally and domestically on the government to accept the calls for an independent investigation into the final stages of the war.\"]]],[1,\"p\",[[0,[],0,\"If you would like to find out there are few links below:\"]]],[1,\"p\",[[0,[4,5],2,\"Boycott CHOGM Petition\"]]],[1,\"p\",[[0,[4],1,\"Documentaries\"]]],[1,\"p\",[[0,[],0,\"1. \"],[0,[6],1,\"Killing Fields of Sri Lanka\"]]],[1,\"p\",[[0,[],0,\"2. \"],[0,[7],1,\"Killing Fields of Sri Lanka follow-up documentary\"]]]]}","html":"<p>\"<em>What happened in Sri Lanka was a major Rwanda-like atrocity, in a different scale, where the West didn't care. There was plenty of early warning. This [conflict] has been going on for years and decades. Plenty of things could have been done [to prevent it]. But there was not enough interest</em>.\" -N. Chomsky</p><p>A bloody civil war that gripped the island nation of Sri Lanka for over a quarter of a century came to a horrific end in 2009. The Liberation Tigers of Tamil Eelam (LTTE) were overwhelmingly defeated by the state led Sinhalese majority government forces. The atrocities committed by the state to reach such a conclusion have been vastly underreported. Estimates of civilian deaths in the final five months of the war range between 9,000 and 75,000 (<a href=\"http://www.bbc.co.uk/news/world-asia-21873551\">BBC, 2013</a>). One United Nations report puts the number at 40,000; most recently they have reported that up to 70,000 civilians could have died in the final 5 months (<a href=\"http://www.un.org/News/dh/infocus/Sri_Lanka/POE_Report_Full.pdf\">UN, 2011</a>, &amp; <a href=\"http://www.un.org/News/dh/infocus/Sri_Lanka/The_Internal_Review_Panel_report_on_Sri_Lanka.pdf\">UN, 2012</a>). If this is true, as Frances Harrison of the Guardian states, the magnitude of the estimated civilian deaths is on ‘the scale of Syria but condensed in speed into 5-6 months whereas Syria has been going on for 18 months [to] two years.’ Former Norwegian diplomat Erik Solheim said ‘There were massive war crimes in the last phase which were probably the biggest bloodshed of the twenty first century.’</p><p>A UN panel of experts reported that the government of Sri Lanka has engaged in:</p><ul><li>Killing of civilians through widespread shelling</li><li>Shelling of hospitals and humanitarian objects</li><li>Denial of humanitarian assistance</li><li>Human rights violations suffered by victims and survivors of the conflict, including both Internally Displaced People (IDP) and suspected LTTE cadres</li><li>Human rights violations outside the conflict zone, including against the media and other critics of the Government</li></ul><p>An insidious and well-planned strategy by the government has meant very little independent reporting, foreign aid bodies being prevented from going into the warzone and to date, the resistance of any post-war independent investigation.</p><p>The host of CHOGM (Commonwealth Heads of Government Meeting) is the same government, the same people who are ultimately responsible. The question of accountability will not go away, particularly for the Tamil people, but also for those courageous journalists now in exile who dared to question the government.</p><p>Sadly, the end of the war has not meant the end of human rights abuses perpetrated by the Sri Lankan government. The once Tamil controlled northern areas are now under military occupation. Many displaced Tamils are unable to return home since the government have seized their property for ‘military purposes’ or for ‘agricultural purposes’. Ongoing disappearances, rape, sexual abuse, land grabbing and beatings are stories that are still coming out of Sri Lanka leading some critics to call the post-war, but not post-conflict scenario a ‘structural genocide’. The Sri Lankan government is unsurprisingly against transparency and judicial independence, as demonstrated by the unconstitutional impeachment and removal of the Chief Justice earlier this year, further showing the abuse of power wielded by the Executive.</p><p>For many progressives, the recent Labor asylum policy announcement borne out of political pragmatism rather than compassion and morality has come as an utter disappointment. However, it raises the challenge of changing the hearts and minds of the electorate. This will occur through learning about the plight that refugees flee. Each country’s refugee has its unique and often complex story. This motion aims to shed light on the Sri Lankan Tamils’ story while also advocating for diplomatic pressure to be placed on the Sri Lankan Government.</p><p><strong>What can we do about it?</strong></p><p>International pressure has been shown to be an effective tool in pressuring governments to do the right thing. The Sri Lankan government may not care about the Tamils but they do care about the Commonwealth. A boycott of CHOGM would make a good start. Apartheid is a good example of an international sporting boycott that raised awareness and pressured a nation towards corrective action. A boycott of Sri Lankan cricket, I believe, would mount enough pressure both internationally and domestically on the government to accept the calls for an independent investigation into the final stages of the war.</p><p>If you would like to find out there are few links below:</p><p><strong><a href=\"https://www.change.org/en-AU/petitions/prime-minister-reconsider-chogm-2013-in-sri-lanka\">Boycott CHOGM Petition</a></strong></p><p><strong>Documentaries</strong></p><p>1. <a href=\"https://www.youtube.com/watch?v=r3yPzyM0KMU&amp;bpctr=1582841718\">Killing Fields of Sri Lanka</a></p><p>2. <a href=\"https://www.youtube.com/watch?v=EWzlQeVKcUg\">Killing Fields of Sri Lanka follow-up documentary</a></p>","comment_id":"5e583a2dda57b50001213889","plaintext":"\"What happened in Sri Lanka was a major Rwanda-like atrocity, in a different\nscale, where the West didn't care. There was plenty of early warning. This\n[conflict] has been going on for years and decades. Plenty of things could have\nbeen done [to prevent it]. But there was not enough interest.\" -N. Chomsky\n\nA bloody civil war that gripped the island nation of Sri Lanka for over a\nquarter of a century came to a horrific end in 2009. The Liberation Tigers of\nTamil Eelam (LTTE) were overwhelmingly defeated by the state led Sinhalese\nmajority government forces. The atrocities committed by the state to reach such\na conclusion have been vastly underreported. Estimates of civilian deaths in the\nfinal five months of the war range between 9,000 and 75,000 (BBC, 2013\n[http://www.bbc.co.uk/news/world-asia-21873551]). One United Nations report puts\nthe number at 40,000; most recently they have reported that up to 70,000\ncivilians could have died in the final 5 months (UN, 2011\n[http://www.un.org/News/dh/infocus/Sri_Lanka/POE_Report_Full.pdf], & UN, 2012\n[http://www.un.org/News/dh/infocus/Sri_Lanka/The_Internal_Review_Panel_report_on_Sri_Lanka.pdf]\n). If this is true, as Frances Harrison of the Guardian states, the magnitude of\nthe estimated civilian deaths is on ‘the scale of Syria but condensed in speed\ninto 5-6 months whereas Syria has been going on for 18 months [to] two years.’\nFormer Norwegian diplomat Erik Solheim said ‘There were massive war crimes in\nthe last phase which were probably the biggest bloodshed of the twenty first\ncentury.’\n\nA UN panel of experts reported that the government of Sri Lanka has engaged in:\n\n * Killing of civilians through widespread shelling\n * Shelling of hospitals and humanitarian objects\n * Denial of humanitarian assistance\n * Human rights violations suffered by victims and survivors of the conflict,\n   including both Internally Displaced People (IDP) and suspected LTTE cadres\n * Human rights violations outside the conflict zone, including against the\n   media and other critics of the Government\n\nAn insidious and well-planned strategy by the government has meant very little\nindependent reporting, foreign aid bodies being prevented from going into the\nwarzone and to date, the resistance of any post-war independent investigation.\n\nThe host of CHOGM (Commonwealth Heads of Government Meeting) is the same\ngovernment, the same people who are ultimately responsible. The question of\naccountability will not go away, particularly for the Tamil people, but also for\nthose courageous journalists now in exile who dared to question the government.\n\nSadly, the end of the war has not meant the end of human rights abuses\nperpetrated by the Sri Lankan government. The once Tamil controlled northern\nareas are now under military occupation. Many displaced Tamils are unable to\nreturn home since the government have seized their property for ‘military\npurposes’ or for ‘agricultural purposes’. Ongoing disappearances, rape, sexual\nabuse, land grabbing and beatings are stories that are still coming out of Sri\nLanka leading some critics to call the post-war, but not post-conflict scenario\na ‘structural genocide’. The Sri Lankan government is unsurprisingly against\ntransparency and judicial independence, as demonstrated by the unconstitutional\nimpeachment and removal of the Chief Justice earlier this year, further showing\nthe abuse of power wielded by the Executive.\n\nFor many progressives, the recent Labor asylum policy announcement borne out of\npolitical pragmatism rather than compassion and morality has come as an utter\ndisappointment. However, it raises the challenge of changing the hearts and\nminds of the electorate. This will occur through learning about the plight that\nrefugees flee. Each country’s refugee has its unique and often complex story.\nThis motion aims to shed light on the Sri Lankan Tamils’ story while also\nadvocating for diplomatic pressure to be placed on the Sri Lankan Government.\n\nWhat can we do about it?\n\nInternational pressure has been shown to be an effective tool in pressuring\ngovernments to do the right thing. The Sri Lankan government may not care about\nthe Tamils but they do care about the Commonwealth. A boycott of CHOGM would\nmake a good start. Apartheid is a good example of an international sporting\nboycott that raised awareness and pressured a nation towards corrective action.\nA boycott of Sri Lankan cricket, I believe, would mount enough pressure both\ninternationally and domestically on the government to accept the calls for an\nindependent investigation into the final stages of the war.\n\nIf you would like to find out there are few links below:\n\nBoycott CHOGM Petition\n[https://www.change.org/en-AU/petitions/prime-minister-reconsider-chogm-2013-in-sri-lanka]\n\nDocumentaries\n\n1. Killing Fields of Sri Lanka\n[https://www.youtube.com/watch?v=r3yPzyM0KMU&bpctr=1582841718]\n\n2. Killing Fields of Sri Lanka follow-up documentary\n[https://www.youtube.com/watch?v=EWzlQeVKcUg]","feature_image":null,"featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2020-02-27 21:52:45","created_by":"1","updated_at":"2020-02-27 22:08:48","updated_by":null,"published_at":"2013-08-21 21:52:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd1a","uuid":"2ca833f0-9e40-49e6-8211-025c4003cf8d","title":"Replacing a Shower Extractor Fan UK","slug":"replacing-a-shower-extractor-fan-uk","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"\\n    3PHASE                   - FAN\\nlive  : brown                - black\\nN     : black sleeved(blue)  - blue\\nsLive  : grey(sleeved brown) - grey\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"Mapping of wires to fan.\"]]],[10,0],[1,\"p\",[]]]}","html":"<p>Mapping of wires to fan.</p><pre><code>\n    3PHASE                   - FAN\nlive  : brown                - black\nN     : black sleeved(blue)  - blue\nsLive  : grey(sleeved brown) - grey</code></pre>","comment_id":"5e661d2eda57b500012138c4","plaintext":"Mapping of wires to fan.\n\n\n    3PHASE                   - FAN\nlive  : brown                - black\nN     : black sleeved(blue)  - blue\nsLive  : grey(sleeved brown) - grey","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2020-03-09 10:40:46","created_by":"1","updated_at":"2020-03-09 10:41:35","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd1b","uuid":"91ec9649-14bd-4391-b5ba-6b570b86a983","title":"Querying Bigquery using Jupyter Lab","slug":"querying-bigquery-using-jupyter-lab","mobiledoc":"{\"version\":\"0.3.2\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"# Terminal commands prior to running\\n#   $ gcloud auth application-default login\\n#   $ pip3 install --user --upgrade google-api-python-client\\n# ref: https://googleapis.dev/python/bigquery/latest/magics.html\\n# \\nfrom google.cloud import bigquery\\n%load_ext google.cloud.bigquery\\n# Set your default project here\\nfrom google.cloud.bigquery import magics\\nfrom google.oauth2 import service_account\\nmagics.context.project = 'monzo-analytics'\\n\\n%%bigquery\\nSELECT distinct staff_user_id\\nFROM `monzo-analytics.prod.stg_staff_events_no_pii`\\nwhere end_date < start_date\",\"language\":\"python\"}]],\"markups\":[[\"a\",[\"href\",\"https://towardsdatascience.com/using-jupyter-notebook-to-manage-your-bigquery-analytics-c4dc7b2a4113\"]]],\"sections\":[[10,0],[1,\"h2\",[[0,[],0,\"Reference\"]]],[3,\"ul\",[[[0,[0],1,\"This one got me started - but I got quite a few errors, so documented what worked for me\"]]]]]}","html":"<pre><code class=\"language-python\"># Terminal commands prior to running\n#   $ gcloud auth application-default login\n#   $ pip3 install --user --upgrade google-api-python-client\n# ref: https://googleapis.dev/python/bigquery/latest/magics.html\n# \nfrom google.cloud import bigquery\n%load_ext google.cloud.bigquery\n# Set your default project here\nfrom google.cloud.bigquery import magics\nfrom google.oauth2 import service_account\nmagics.context.project = 'monzo-analytics'\n\n%%bigquery\nSELECT distinct staff_user_id\nFROM `monzo-analytics.prod.stg_staff_events_no_pii`\nwhere end_date &lt; start_date</code></pre><h2 id=\"reference\">Reference</h2><ul><li><a href=\"https://towardsdatascience.com/using-jupyter-notebook-to-manage-your-bigquery-analytics-c4dc7b2a4113\">This one got me started - but I got quite a few errors, so documented what worked for me</a></li></ul>","comment_id":"5e99c092ee6c8e0001bd9f88","plaintext":"# Terminal commands prior to running\n#   $ gcloud auth application-default login\n#   $ pip3 install --user --upgrade google-api-python-client\n# ref: https://googleapis.dev/python/bigquery/latest/magics.html\n# \nfrom google.cloud import bigquery\n%load_ext google.cloud.bigquery\n# Set your default project here\nfrom google.cloud.bigquery import magics\nfrom google.oauth2 import service_account\nmagics.context.project = 'monzo-analytics'\n\n%%bigquery\nSELECT distinct staff_user_id\nFROM `monzo-analytics.prod.stg_staff_events_no_pii`\nwhere end_date < start_date\n\nReference\n * This one got me started - but I got quite a few errors, so documented what\n   worked for me\n   [https://towardsdatascience.com/using-jupyter-notebook-to-manage-your-bigquery-analytics-c4dc7b2a4113]","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2020-04-17 14:43:30","created_by":"1","updated_at":"2020-04-17 14:46:38","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd1c","uuid":"20193b73-8242-476d-af62-6b54cdfb3e84","title":"Setting up Pi-Hole on a Raspberry Pi Zero W using SSH","slug":"pi-hole","mobiledoc":"{\"version\":\"0.3.2\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"/content/images/2020/04/raspberry-pi-zero-w-annotated-2.jpg\"}],[\"code\",{\"code\":\"country=GB #your country's 2digit isocode\\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\\nupdate_config=1\\n\\nnetwork={\\n    ssid=\\\"yourWiFiNetworkName\\\"\\n    psk=\\\"yourWiFiPassword\\\"\\n}\",\"language\":\"bash\"}],[\"code\",{\"code\":\"$ touch ssh\",\"language\":\"BASH\"}],[\"code\",{\"code\":\"$ ssh pi@raspberrypi.local\",\"language\":\"BASH\"}],[\"code\",{\"code\":\"$ sudo apt-get update && sudo apt-get upgrade -y \\n\"}],[\"code\",{\"code\":\"$ passwd\",\"language\":\"BASH\"}],[\"code\",{\"code\":\"$ go get github.com/mwiora/NAMEinator\\n$ go get github.com/miekg/dns\\n$ cd ~/go/src/github.com/mwiora/NAMEinator/\\n$ go build\\n$ ./NAMEinator\",\"language\":\"BASH\"}],[\"code\",{\"code\":\"LETS GO - each dot is a completed domain request against all nameservers\\n....................................................................................................\\nfinished - presenting results:\\n\\n208.67.222.222:\\nAvg. [88.618011ms], Min. [10ms], Max. [606.061151ms]\\n\\n156.154.71.1:\\nAvg. [101.25658ms], Min. [10ms], Max. [2.004470267s]\\n\\n216.146.35.35:\\nAvg. [117.992645ms], Min. [10ms], Max. [1.402119851s]\\n\\n1.0.0.1:\\nAvg. [50.555672ms], Min. [10ms], Max. [615.732289ms]\\n\\n1.1.1.1:\\nAvg. [28.486788ms], Min. [10ms], Max. [329.542844ms]\\n\\n8.8.8.8:\\nAvg. [79.401147ms], Min. [10ms], Max. [2.003982489s]\\n\\n8.8.4.4:\\nAvg. [65.633078ms], Min. [10ms], Max. [491.458848ms]\\n\\nAu revoir!\",\"language\":\"BASH\"}],[\"code\",{\"code\":\"$ curl -sSL https://install.pi-hole.net | bash\"}],[\"image\",{\"src\":\"/content/images/2020/04/Screen-Shot-2020-04-27-at-09.39.50.png\"}],[\"image\",{\"src\":\"/content/images/2020/04/Screen-Shot-2020-04-26-at-22.18.25-1.png\"}],[\"image\",{\"src\":\"/content/images/2020/04/Screen-Shot-2020-04-26-at-22.36.07.png\"}],[\"image\",{\"src\":\"/content/images/2020/04/Screen-Shot-2020-04-26-at-22.38.08-4.png\"}],[\"image\",{\"src\":\"/content/images/2020/04/osx-enter-custom-dns.png\"}],[\"image\",{\"src\":\"/content/images/2020/04/Screen-Shot-2020-04-27-at-00.00.33.png\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://money.cnn.com/2015/11/27/technology/raspberry-pi-zero/index.html\"]],[\"a\",[\"href\",\"https://shop.pimoroni.com/products/raspberry-pi-zero-w?src=raspberrypi#show-accessories\"]],[\"a\",[\"href\",\"https://shop.pimoroni.com/products/raspberry-pi-universal-power-supply\"]],[\"a\",[\"href\",\"https://shop.pimoroni.com/products/noobs-32gb-microsd-card-3-1\"]],[\"a\",[\"href\",\"https://www.raspbian.org/\"]],[\"a\",[\"href\",\"https://pi-hole.net/\"]],[\"code\"],[\"em\"],[\"a\",[\"href\",\"https://github.com/mwiora/NAMEinator\"]],[\"a\",[\"href\",\"https://www.cloudflare.com/en-gb/\"]],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=S43CFcpOZSI\"]],[\"a\",[\"href\",\"https://discourse.pi-hole.net/t/how-do-i-configure-my-devices-to-use-pi-hole-as-their-dns-server/245\"]],[\"a\",[\"href\",\"https://learn.pimoroni.com/tutorial/sandyj/setting-up-a-headless-pi\"]],[\"a\",[\"href\",\"https://www.losant.com/blog/getting-started-with-the-raspberry-pi-zero-w-without-a-monitor?q=%20&hPP=20&idx=production_BLOG&p=0&is_v=1\"]],[\"a\",[\"href\",\"https://www.reddit.com/r/pihole/comments/61an37/pihole_refusing_to_work_over_wifi/\"]]],\"sections\":[[1,\"p\",[[0,[0],1,\"Aim: \"],[0,[],0,\"🛑 Block all advertisements for all devices on my home WiFi network\"]]],[1,\"p\",[[0,[0],1,\"Estimated time to complete:\"],[0,[],0,\" Best part of a morning ⏰ depending on how well things go for you i.e. 1.5-3hrs  \"],[1,[],0,0]]],[1,\"p\",[[0,[],0,\"When the Raspberry Pi Zero came out it hit front pages as the \"],[0,[1],1,\"$5 computer and sold out instantly\"],[0,[],0,\". Here are the specs for the Raspberry Pi Zero:\"]]],[3,\"ul\",[[[0,[],0,\"1GHz, single-core CPU\"]],[[0,[],0,\"512MB RAM\"]],[[0,[],0,\"Mini HDMI and USB On-The-Go ports\"]],[[0,[],0,\"Micro USB power\"]],[[0,[],0,\"HAT-compatible 40-pin header\"]],[[0,[],0,\"Composite video and reset headers\"]],[[0,[],0,\"CSI camera connector\"]]]],[1,\"p\",[[0,[],0,\"Additional with the Raspberry Pi Zero W:\"]]],[3,\"ul\",[[[0,[],0,\"802.11 b/g/n wireless LAN\"]],[[0,[],0,\"Bluetooth 4.1\"]],[[0,[],0,\"Bluetooth Low Energy (BLE)\"]]]],[1,\"p\",[[0,[],0,\"It genuinely is something to be excited about 😁. I have the slightly more expensive 'W' version - it's twice the price but I'd certainly recommend it for connectivity reasons, otherwise you'll be a little stuck for internet! \"]]],[1,\"h2\",[[0,[],0,\"Step 1: The Ingredients 🤤\"]]],[10,0],[1,\"p\",[[0,[0],1,\"Hardware\"]]],[3,\"ul\",[[[0,[2],1,\"R Pi 0 W - £9.30\"],[0,[],0,\" (~$12USD)\"]],[[0,[3],1,\"R Pi Power Supply - £8.10 (~$10USD)\"]],[[0,[4],1,\"NOOBs 32GB micro sd card - £9 (~$11USD)\"]]]],[1,\"p\",[[0,[],0,\"Total = £26.40\"]]],[1,\"p\",[[0,[0],1,\"Software\"]]],[3,\"ul\",[[[0,[5],1,\"Raspbian\"]],[[0,[6],1,\"Pi-Hole\"]]]],[1,\"h2\",[[0,[],0,\"Step 2: Setting up the Micro SD with the operating system and connectivity 👨‍💻\"]]],[1,\"p\",[[0,[],0,\"The NOOBs micro sd card comes preloaded with raspbian - if you have a blank SD you'll have to find instructions to flash it with Raspbian or a suitable alternative unix flavour. (Re)Connect the flashed SD card to your computer - you should see that it has been recognised as \"],[0,[7],1,\"BOOT\"],[0,[],0,\". In here are a range of files. If you can't already see a \"],[0,[7],1,\"wpa_supplicant.conf\"],[0,[],0,\" file, create one in the root of the sd card i.e.\"],[0,[7],1,\"BOOT/wpa_supplicant.conf\"],[0,[],0,\"and enter the following updating the country and wifi details:\"]]],[10,1],[1,\"p\",[[0,[],0,\"You can add multiple WiFi networks by repeating the above config with the relevant network credentials.\"]]],[1,\"p\",[[0,[],0,\"In the same directory create a blank file \"],[0,[7],1,\"ssh\"],[0,[],0,\", ensuring that there's no extension. \"]]],[10,2],[1,\"h2\",[[0,[],0,\"Step 3: 🟢 Turn on the R Pi connect to it and update\"]]],[1,\"p\",[[0,[],0,\"When you first power up the R Pi it will immediately start flashing a small green LED. However, it may take a little while to be able to ssh into the R Pi - allow up to 2 minutes. Of course if you mistyped your WiFi credentials you won't be able to ssh in 🤦‍♂️. Your network will assign an IP address to the PI but the default hostname is nicer.\"]]],[10,3],[1,\"p\",[[0,[],0,\"Press enter / answer yes to the prompts.  When asked for a password,  \"],[0,[7],1,\"raspberry\"],[0,[],0,\" is the default.\"]]],[1,\"p\",[[0,[],0,\"Run the following command to update the R Pi - this can take around 30mins so take a break at this point 🥱:\"]]],[10,4],[1,\"p\",[[0,[],0,\"Then change password:\"]]],[10,5],[1,\"h2\",[[0,[],0,\"Step 4: \"],[0,[8],1,\"Optional\"],[0,[],0,\" what's the fastest Upstream DNS to select 🏁\"]]],[1,\"p\",[[0,[],0,\"You need to select an upstream DNS as part of the Pi-Hole install. I wanted to know which one was going to give me the best performance. I found \"],[0,[9],1,\"this tool\"],[0,[],0,\" a fork from a broken Google tool that did the job. If you don't have \"],[0,[7],1,\"go-lang\"],[0,[],0,\" installed skip this step and choose a DNS provider you're comfortable with - I'd recommend Cloudflare. \"]]],[10,6],[1,\"p\",[[0,[],0,\"After \"],[0,[7],1,\"NAMEinator\"],[0,[],0,\" finishes testing the DNSs for responses you should get a report print out:\"]]],[10,7],[1,\"p\",[[0,[],0,\"I ran the test a couple of times, \"],[0,[10],0,\"Cloudflare \"],[0,[7],2,\"1.1.1.1\"],[0,[],0,\" came out with the lowest response times both times.\"]]],[1,\"h2\",[[0,[],0,\"Step 5: Install Pi-Hole 🕳\"]]],[1,\"p\",[[0,[],0,\"You can run Pi-Hole in a container, however I chose to directly install this for two reasons, I suspect the overhead of Docker would cause performance issues and I won't be utilising the reasons for Docker i.e. I'm unlikely to be putting other software on this R Pi. I went with the simplest option:\"]]],[10,8],[1,\"p\",[[0,[],0,\"This will take you through a series of prompts that you mostly will press enter/yes to, select your upstream DNS and once you pop out the other end you will have a print out that looks like:\"]]],[10,9],[1,\"p\",[[0,[],0,\"Take note of the IP address at the bottom of the print out next to\"],[0,[7],1,\"Pi-hole DNS (IPv4)\"],[0,[],0,\" and the \"],[0,[7],1,\"Web Interface password\"],[0,[],0,\".\"]]],[1,\"h2\",[[0,[],0,\"Step 6: Configure Your Router 💻\"]]],[1,\"p\",[[0,[],0,\"If you haven't done much tinkering with routers or consumed a bunch networking content this can be a strange place full of foreign acronyms. Here's a simple explaination of the two most important for this exercise:\"]]],[1,\"p\",[[0,[0],1,\"IP address\"],[0,[],0,\" - a device's unique identifier on a network.\"]]],[1,\"p\",[[0,[0],1,\"Domain Name System (DNS)\"],[0,[],0,\" - Resolves domain names into IP addresses e.g. \"],[0,[7],1,\"google.com\"],[0,[],0,\" -> \"],[0,[7],1,\"172.217.169.78\"],[0,[],0,\".  \"]]],[1,\"p\",[[0,[0],1,\"Dynamic Host Configuration Protocol (DHCP)\"],[0,[],0,\" - a process that occurs on your WiFi network to assign unique IPs. \"],[0,[11],1,\"5 min video explainer\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"There are 3 options to get Pi-Hole working outlined in \"],[0,[12],1,\"more detail here\"],[0,[],0,\".\"]]],[3,\"ol\",[[[0,[0],1,\"Configure the DNS to the static IP of the R Pi \"],[0,[],0,\"-  I'd recommend putting in a secondary option of Cloudflare in case your R Pi doesn't work. This option means anyone who connects to your device will be ad free without thinking about it. However no amount of rebooting routers and devices, or forgetting my WiFi network and re-adding it managed to get this option to work for me. It should be as simple as setting this option:\"]]]],[10,10],[1,\"p\",[[0,[],0,\"2. \"],[0,[0],1,\"Let Pi-Hole takeover the DHCP side of things \"],[0,[],0,\"- this worked easily. Turn off the DHCP setting on your router.\"]]],[10,11],[1,\"p\",[[0,[],0,\"Then turn it on in Settings in the Pi-Hole web interface.\"]]],[10,12],[1,\"p\",[[0,[],0,\"You will also need to assign a static IP for the R Pi. However, if your R Pi stops working, so too will your internet - so I went for the option I least wanted, option 3.\"]]],[1,\"p\",[[0,[0],1,\"3. Manually enter your Pi-Hole DNS per device 'opt-in' \"],[0,[],0,\"- It means that instead of everyone benefitting automatically without thinking about DNS config when they sign up to your WiFi - you have to manually put in the DNS config. For OSX that looks like this:\"]]],[10,13],[1,\"p\",[[0,[],0,\"For Android. Go to WiFi networks > select 'Modify Network' > 'Advanced options' > Change 'IP settings' to 'Static' > Enter your R Pi IP address.\"]]],[1,\"p\",[[0,[],0,\"Pretty instantly I ended up seeing lovely blocked results in myPi-Hole web interface.\"]]],[10,14],[1,\"h2\",[[0,[],0,\"Reference 📚\"]]],[3,\"ul\",[[[0,[13],1,\"Pimoroni where I've bought all my R Pi related gear so far and they have great tutorials like this one.\"]],[[0,[14],1,\"Helpful guide for the initial micro sd card setup\"]],[[0,[15],1,\"Reddit thread on all the rebooting advice - didn't solve it for me.\"]]]]]}","html":"<p><strong>Aim: </strong>🛑 Block all advertisements for all devices on my home WiFi network</p><p><strong>Estimated time to complete:</strong> Best part of a morning ⏰ depending on how well things go for you i.e. 1.5-3hrs  <br></p><p>When the Raspberry Pi Zero came out it hit front pages as the <a href=\"https://money.cnn.com/2015/11/27/technology/raspberry-pi-zero/index.html\">$5 computer and sold out instantly</a>. Here are the specs for the Raspberry Pi Zero:</p><ul><li>1GHz, single-core CPU</li><li>512MB RAM</li><li>Mini HDMI and USB On-The-Go ports</li><li>Micro USB power</li><li>HAT-compatible 40-pin header</li><li>Composite video and reset headers</li><li>CSI camera connector</li></ul><p>Additional with the Raspberry Pi Zero W:</p><ul><li>802.11 b/g/n wireless LAN</li><li>Bluetooth 4.1</li><li>Bluetooth Low Energy (BLE)</li></ul><p>It genuinely is something to be excited about 😁. I have the slightly more expensive 'W' version - it's twice the price but I'd certainly recommend it for connectivity reasons, otherwise you'll be a little stuck for internet! </p><h2 id=\"step-1-the-ingredients-\">Step 1: The Ingredients 🤤</h2><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/04/raspberry-pi-zero-w-annotated-2.jpg\" class=\"kg-image\" alt></figure><p><strong>Hardware</strong></p><ul><li><a href=\"https://shop.pimoroni.com/products/raspberry-pi-zero-w?src=raspberrypi#show-accessories\">R Pi 0 W - £9.30</a> (~$12USD)</li><li><a href=\"https://shop.pimoroni.com/products/raspberry-pi-universal-power-supply\">R Pi Power Supply - £8.10 (~$10USD)</a></li><li><a href=\"https://shop.pimoroni.com/products/noobs-32gb-microsd-card-3-1\">NOOBs 32GB micro sd card - £9 (~$11USD)</a></li></ul><p>Total = £26.40</p><p><strong>Software</strong></p><ul><li><a href=\"https://www.raspbian.org/\">Raspbian</a></li><li><a href=\"https://pi-hole.net/\">Pi-Hole</a></li></ul><h2 id=\"step-2-setting-up-the-micro-sd-with-the-operating-system-and-connectivity-\">Step 2: Setting up the Micro SD with the operating system and connectivity 👨‍💻</h2><p>The NOOBs micro sd card comes preloaded with raspbian - if you have a blank SD you'll have to find instructions to flash it with Raspbian or a suitable alternative unix flavour. (Re)Connect the flashed SD card to your computer - you should see that it has been recognised as <code>BOOT</code>. In here are a range of files. If you can't already see a <code>wpa_supplicant.conf</code> file, create one in the root of the sd card i.e.<code>BOOT/wpa_supplicant.conf</code>and enter the following updating the country and wifi details:</p><pre><code class=\"language-bash\">country=GB #your country's 2digit isocode\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\nupdate_config=1\n\nnetwork={\n    ssid=\"yourWiFiNetworkName\"\n    psk=\"yourWiFiPassword\"\n}</code></pre><p>You can add multiple WiFi networks by repeating the above config with the relevant network credentials.</p><p>In the same directory create a blank file <code>ssh</code>, ensuring that there's no extension. </p><pre><code class=\"language-BASH\">$ touch ssh</code></pre><h2 id=\"step-3-turn-on-the-r-pi-connect-to-it-and-update\">Step 3: 🟢 Turn on the R Pi connect to it and update</h2><p>When you first power up the R Pi it will immediately start flashing a small green LED. However, it may take a little while to be able to ssh into the R Pi - allow up to 2 minutes. Of course if you mistyped your WiFi credentials you won't be able to ssh in 🤦‍♂️. Your network will assign an IP address to the PI but the default hostname is nicer.</p><pre><code class=\"language-BASH\">$ ssh pi@raspberrypi.local</code></pre><p>Press enter / answer yes to the prompts.  When asked for a password,  <code>raspberry</code> is the default.</p><p>Run the following command to update the R Pi - this can take around 30mins so take a break at this point 🥱:</p><pre><code>$ sudo apt-get update &amp;&amp; sudo apt-get upgrade -y \n</code></pre><p>Then change password:</p><pre><code class=\"language-BASH\">$ passwd</code></pre><h2 id=\"step-4-optional-what-s-the-fastest-upstream-dns-to-select-\">Step 4: <em>Optional</em> what's the fastest Upstream DNS to select 🏁</h2><p>You need to select an upstream DNS as part of the Pi-Hole install. I wanted to know which one was going to give me the best performance. I found <a href=\"https://github.com/mwiora/NAMEinator\">this tool</a> a fork from a broken Google tool that did the job. If you don't have <code>go-lang</code> installed skip this step and choose a DNS provider you're comfortable with - I'd recommend Cloudflare. </p><pre><code class=\"language-BASH\">$ go get github.com/mwiora/NAMEinator\n$ go get github.com/miekg/dns\n$ cd ~/go/src/github.com/mwiora/NAMEinator/\n$ go build\n$ ./NAMEinator</code></pre><p>After <code>NAMEinator</code> finishes testing the DNSs for responses you should get a report print out:</p><pre><code class=\"language-BASH\">LETS GO - each dot is a completed domain request against all nameservers\n....................................................................................................\nfinished - presenting results:\n\n208.67.222.222:\nAvg. [88.618011ms], Min. [10ms], Max. [606.061151ms]\n\n156.154.71.1:\nAvg. [101.25658ms], Min. [10ms], Max. [2.004470267s]\n\n216.146.35.35:\nAvg. [117.992645ms], Min. [10ms], Max. [1.402119851s]\n\n1.0.0.1:\nAvg. [50.555672ms], Min. [10ms], Max. [615.732289ms]\n\n1.1.1.1:\nAvg. [28.486788ms], Min. [10ms], Max. [329.542844ms]\n\n8.8.8.8:\nAvg. [79.401147ms], Min. [10ms], Max. [2.003982489s]\n\n8.8.4.4:\nAvg. [65.633078ms], Min. [10ms], Max. [491.458848ms]\n\nAu revoir!</code></pre><p>I ran the test a couple of times, <a href=\"https://www.cloudflare.com/en-gb/\">Cloudflare <code>1.1.1.1</code></a> came out with the lowest response times both times.</p><h2 id=\"step-5-install-pi-hole-\">Step 5: Install Pi-Hole 🕳</h2><p>You can run Pi-Hole in a container, however I chose to directly install this for two reasons, I suspect the overhead of Docker would cause performance issues and I won't be utilising the reasons for Docker i.e. I'm unlikely to be putting other software on this R Pi. I went with the simplest option:</p><pre><code>$ curl -sSL https://install.pi-hole.net | bash</code></pre><p>This will take you through a series of prompts that you mostly will press enter/yes to, select your upstream DNS and once you pop out the other end you will have a print out that looks like:</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/04/Screen-Shot-2020-04-27-at-09.39.50.png\" class=\"kg-image\" alt></figure><p>Take note of the IP address at the bottom of the print out next to<code>Pi-hole DNS (IPv4)</code> and the <code>Web Interface password</code>.</p><h2 id=\"step-6-configure-your-router-\">Step 6: Configure Your Router 💻</h2><p>If you haven't done much tinkering with routers or consumed a bunch networking content this can be a strange place full of foreign acronyms. Here's a simple explaination of the two most important for this exercise:</p><p><strong>IP address</strong> - a device's unique identifier on a network.</p><p><strong>Domain Name System (DNS)</strong> - Resolves domain names into IP addresses e.g. <code>google.com</code> -&gt; <code>172.217.169.78</code>.  </p><p><strong>Dynamic Host Configuration Protocol (DHCP)</strong> - a process that occurs on your WiFi network to assign unique IPs. <a href=\"https://www.youtube.com/watch?v=S43CFcpOZSI\">5 min video explainer</a>.</p><p>There are 3 options to get Pi-Hole working outlined in <a href=\"https://discourse.pi-hole.net/t/how-do-i-configure-my-devices-to-use-pi-hole-as-their-dns-server/245\">more detail here</a>.</p><ol><li><strong>Configure the DNS to the static IP of the R Pi </strong>-  I'd recommend putting in a secondary option of Cloudflare in case your R Pi doesn't work. This option means anyone who connects to your device will be ad free without thinking about it. However no amount of rebooting routers and devices, or forgetting my WiFi network and re-adding it managed to get this option to work for me. It should be as simple as setting this option:</li></ol><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/04/Screen-Shot-2020-04-26-at-22.18.25-1.png\" class=\"kg-image\" alt></figure><p>2. <strong>Let Pi-Hole takeover the DHCP side of things </strong>- this worked easily. Turn off the DHCP setting on your router.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/04/Screen-Shot-2020-04-26-at-22.36.07.png\" class=\"kg-image\" alt></figure><p>Then turn it on in Settings in the Pi-Hole web interface.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/04/Screen-Shot-2020-04-26-at-22.38.08-4.png\" class=\"kg-image\" alt></figure><p>You will also need to assign a static IP for the R Pi. However, if your R Pi stops working, so too will your internet - so I went for the option I least wanted, option 3.</p><p><strong>3. Manually enter your Pi-Hole DNS per device 'opt-in' </strong>- It means that instead of everyone benefitting automatically without thinking about DNS config when they sign up to your WiFi - you have to manually put in the DNS config. For OSX that looks like this:</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/04/osx-enter-custom-dns.png\" class=\"kg-image\" alt></figure><p>For Android. Go to WiFi networks &gt; select 'Modify Network' &gt; 'Advanced options' &gt; Change 'IP settings' to 'Static' &gt; Enter your R Pi IP address.</p><p>Pretty instantly I ended up seeing lovely blocked results in myPi-Hole web interface.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/04/Screen-Shot-2020-04-27-at-00.00.33.png\" class=\"kg-image\" alt></figure><h2 id=\"reference-\">Reference 📚</h2><ul><li><a href=\"https://learn.pimoroni.com/tutorial/sandyj/setting-up-a-headless-pi\">Pimoroni where I've bought all my R Pi related gear so far and they have great tutorials like this one.</a></li><li><a href=\"https://www.losant.com/blog/getting-started-with-the-raspberry-pi-zero-w-without-a-monitor?q=%20&amp;hPP=20&amp;idx=production_BLOG&amp;p=0&amp;is_v=1\">Helpful guide for the initial micro sd card setup</a></li><li><a href=\"https://www.reddit.com/r/pihole/comments/61an37/pihole_refusing_to_work_over_wifi/\">Reddit thread on all the rebooting advice - didn't solve it for me.</a></li></ul>","comment_id":"5ea4a551ee6c8e0001bd9fa8","plaintext":"Aim: 🛑 Block all advertisements for all devices on my home WiFi network\n\nEstimated time to complete: Best part of a morning ⏰ depending on how well\nthings go for you i.e. 1.5-3hrs\n\n\nWhen the Raspberry Pi Zero came out it hit front pages as the $5 computer and\nsold out instantly\n[https://money.cnn.com/2015/11/27/technology/raspberry-pi-zero/index.html]. Here\nare the specs for the Raspberry Pi Zero:\n\n * 1GHz, single-core CPU\n * 512MB RAM\n * Mini HDMI and USB On-The-Go ports\n * Micro USB power\n * HAT-compatible 40-pin header\n * Composite video and reset headers\n * CSI camera connector\n\nAdditional with the Raspberry Pi Zero W:\n\n * 802.11 b/g/n wireless LAN\n * Bluetooth 4.1\n * Bluetooth Low Energy (BLE)\n\nIt genuinely is something to be excited about 😁. I have the slightly more\nexpensive 'W' version - it's twice the price but I'd certainly recommend it for\nconnectivity reasons, otherwise you'll be a little stuck for internet! \n\nStep 1: The Ingredients 🤤\nHardware\n\n * R Pi 0 W - £9.30\n   [https://shop.pimoroni.com/products/raspberry-pi-zero-w?src=raspberrypi#show-accessories] \n   (~$12USD)\n * R Pi Power Supply - £8.10 (~$10USD)\n   [https://shop.pimoroni.com/products/raspberry-pi-universal-power-supply]\n * NOOBs 32GB micro sd card - £9 (~$11USD)\n   [https://shop.pimoroni.com/products/noobs-32gb-microsd-card-3-1]\n\nTotal = £26.40\n\nSoftware\n\n * Raspbian [https://www.raspbian.org/]\n * Pi-Hole [https://pi-hole.net/]\n\nStep 2: Setting up the Micro SD with the operating system and connectivity 👨‍💻\nThe NOOBs micro sd card comes preloaded with raspbian - if you have a blank SD\nyou'll have to find instructions to flash it with Raspbian or a suitable\nalternative unix flavour. (Re)Connect the flashed SD card to your computer - you\nshould see that it has been recognised as BOOT. In here are a range of files. If\nyou can't already see a wpa_supplicant.conf file, create one in the root of the\nsd card i.e.BOOT/wpa_supplicant.confand enter the following updating the country\nand wifi details:\n\ncountry=GB #your country's 2digit isocode\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\nupdate_config=1\n\nnetwork={\n    ssid=\"yourWiFiNetworkName\"\n    psk=\"yourWiFiPassword\"\n}\n\nYou can add multiple WiFi networks by repeating the above config with the\nrelevant network credentials.\n\nIn the same directory create a blank file ssh, ensuring that there's no\nextension. \n\n$ touch ssh\n\nStep 3: 🟢 Turn on the R Pi connect to it and update\nWhen you first power up the R Pi it will immediately start flashing a small\ngreen LED. However, it may take a little while to be able to ssh into the R Pi -\nallow up to 2 minutes. Of course if you mistyped your WiFi credentials you won't\nbe able to ssh in 🤦‍♂️. Your network will assign an IP address to the PI but\nthe default hostname is nicer.\n\n$ ssh pi@raspberrypi.local\n\nPress enter / answer yes to the prompts.  When asked for a password,raspberry is\nthe default.\n\nRun the following command to update the R Pi - this can take around 30mins so\ntake a break at this point 🥱:\n\n$ sudo apt-get update && sudo apt-get upgrade -y \n\n\nThen change password:\n\n$ passwd\n\nStep 4: Optional what's the fastest Upstream DNS to select 🏁\nYou need to select an upstream DNS as part of the Pi-Hole install. I wanted to\nknow which one was going to give me the best performance. I found this tool\n[https://github.com/mwiora/NAMEinator] a fork from a broken Google tool that did\nthe job. If you don't have go-lang installed skip this step and choose a DNS\nprovider you're comfortable with - I'd recommend Cloudflare. \n\n$ go get github.com/mwiora/NAMEinator\n$ go get github.com/miekg/dns\n$ cd ~/go/src/github.com/mwiora/NAMEinator/\n$ go build\n$ ./NAMEinator\n\nAfter NAMEinator finishes testing the DNSs for responses you should get a report\nprint out:\n\nLETS GO - each dot is a completed domain request against all nameservers\n....................................................................................................\nfinished - presenting results:\n\n208.67.222.222:\nAvg. [88.618011ms], Min. [10ms], Max. [606.061151ms]\n\n156.154.71.1:\nAvg. [101.25658ms], Min. [10ms], Max. [2.004470267s]\n\n216.146.35.35:\nAvg. [117.992645ms], Min. [10ms], Max. [1.402119851s]\n\n1.0.0.1:\nAvg. [50.555672ms], Min. [10ms], Max. [615.732289ms]\n\n1.1.1.1:\nAvg. [28.486788ms], Min. [10ms], Max. [329.542844ms]\n\n8.8.8.8:\nAvg. [79.401147ms], Min. [10ms], Max. [2.003982489s]\n\n8.8.4.4:\nAvg. [65.633078ms], Min. [10ms], Max. [491.458848ms]\n\nAu revoir!\n\nI ran the test a couple of times, Cloudflare 1.1.1.1\n[https://www.cloudflare.com/en-gb/] came out with the lowest response times both\ntimes.\n\nStep 5: Install Pi-Hole 🕳\nYou can run Pi-Hole in a container, however I chose to directly install this for\ntwo reasons, I suspect the overhead of Docker would cause performance issues and\nI won't be utilising the reasons for Docker i.e. I'm unlikely to be putting\nother software on this R Pi. I went with the simplest option:\n\n$ curl -sSL https://install.pi-hole.net | bash\n\nThis will take you through a series of prompts that you mostly will press\nenter/yes to, select your upstream DNS and once you pop out the other end you\nwill have a print out that looks like:\n\nTake note of the IP address at the bottom of the print out next toPi-hole DNS\n(IPv4) and the Web Interface password.\n\nStep 6: Configure Your Router 💻\nIf you haven't done much tinkering with routers or consumed a bunch networking\ncontent this can be a strange place full of foreign acronyms. Here's a simple\nexplaination of the two most important for this exercise:\n\nIP address - a device's unique identifier on a network.\n\nDomain Name System (DNS) - Resolves domain names into IP addresses e.g. \ngoogle.com -> 172.217.169.78.\n\nDynamic Host Configuration Protocol (DHCP) - a process that occurs on your WiFi\nnetwork to assign unique IPs. 5 min video explainer\n[https://www.youtube.com/watch?v=S43CFcpOZSI].\n\nThere are 3 options to get Pi-Hole working outlined in more detail here\n[https://discourse.pi-hole.net/t/how-do-i-configure-my-devices-to-use-pi-hole-as-their-dns-server/245]\n.\n\n 1. Configure the DNS to the static IP of the R Pi -  I'd recommend putting in a\n    secondary option of Cloudflare in case your R Pi doesn't work. This option\n    means anyone who connects to your device will be ad free without thinking\n    about it. However no amount of rebooting routers and devices, or forgetting\n    my WiFi network and re-adding it managed to get this option to work for me.\n    It should be as simple as setting this option:\n\n2. Let Pi-Hole takeover the DHCP side of things - this worked easily. Turn off\nthe DHCP setting on your router.\n\nThen turn it on in Settings in the Pi-Hole web interface.\n\nYou will also need to assign a static IP for the R Pi. However, if your R Pi\nstops working, so too will your internet - so I went for the option I least\nwanted, option 3.\n\n3. Manually enter your Pi-Hole DNS per device 'opt-in' - It means that instead\nof everyone benefitting automatically without thinking about DNS config when\nthey sign up to your WiFi - you have to manually put in the DNS config. For OSX\nthat looks like this:\n\nFor Android. Go to WiFi networks > select 'Modify Network' > 'Advanced options'\n> Change 'IP settings' to 'Static' > Enter your R Pi IP address.\n\nPretty instantly I ended up seeing lovely blocked results in myPi-Hole web\ninterface.\n\nReference 📚\n * Pimoroni where I've bought all my R Pi related gear so far and they have\n   great tutorials like this one.\n   [https://learn.pimoroni.com/tutorial/sandyj/setting-up-a-headless-pi]\n * Helpful guide for the initial micro sd card setup\n   [https://www.losant.com/blog/getting-started-with-the-raspberry-pi-zero-w-without-a-monitor?q=%20&hPP=20&idx=production_BLOG&p=0&is_v=1]\n * Reddit thread on all the rebooting advice - didn't solve it for me.\n   [https://www.reddit.com/r/pihole/comments/61an37/pihole_refusing_to_work_over_wifi/]","feature_image":"/content/images/2020/04/raspberry-pi-zero-w.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2020-04-25 21:02:09","created_by":"1","updated_at":"2020-05-04 11:31:08","updated_by":null,"published_at":"2020-04-26 21:59:29","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd1d","uuid":"bbe880ed-cad9-4c43-a95e-a7c3eec15122","title":"A Review and Summary of the Javascript30 Course","slug":"summary-of-javascript30","mobiledoc":"{\"version\":\"0.3.2\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"window.addEventListener('keydown', function (e) {\\n    const audio = document.querySelector(`audio[data-key=\\\"${e.keyCode}\\\"]`);\\n});\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"<input id=\\\"spacing\\\"... data-sizing=\\\"px\\\">\\n\",\"language\":\"html\"}],[\"code\",{\"code\":\"function handleUpdate() {\\n  const suffix = this.dataset;\\n}\\n\\ninputs.forEach(node => node.addEventListener('change', handleUpdate));\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"const inputs = document.querySelectorAll('.controls input');\\n\\nfunction handleUpdate() {\\n  const suffix = this.dataset.sizing || '';\\n  document.documentElement.style.setProperty(`--${this.name}`, this.value+suffix);\\n}\\n\\ninputs.forEach(node => node.addEventListener('change', handleUpdate));\\ninputs.forEach(node => node.addEventListener('mousemove', handleUpdate));\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"console.table(sorted);\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"// grab relevant elements\\nvar panels = document.querySelectorAll('.panel');\\n\\n// function to toggle\\nfunction toggleOpen(e) {\\n  this.classList.toggle('open');\\n}\\n\\n// event listener to toggle\\npanels.forEach(x => x.addEventListener('click', toggleOpen));\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"const cities = [];\\nfetch(endpoint)\\n  .then(blob => blob.json())\\n  .then(data => cities.push(...data))\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"// (g)lobal, (i)nsensitive\\nconst regex = new RegExp(wordToMatch, 'gi');\\nreturn place.city.match(regex)\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"<body>\\n<canvas id=\\\"draw\\\" width=\\\"800\\\" height=\\\"800\\\"></canvas>\\n<script>\\nconst canvas = document.querySelector(\\\"#draw\\\");\\nconst ctx = canvas.getContext('2d');\\ncanvas.width = window.innerWidth;\\ncanvas.height = window.innerHeight;\\nctx.strokeStyle = '#BBDA55';\\n\",\"language\":\"html\"}],[\"code\",{\"code\":\"console.groupCollapsed('my group')\\nconsole.log('part1 collapsed')\\nconsole.log('part2 collapsed')\\nconsole.groupEnd('end of my group')\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"  // select the divs\\n  const checkboxes = document.querySelectorAll('.inbox input[type=\\\"checkbox\\\"]');\\n\\n  function handleCheck(e) {\\n    // check if shiftkey pressed AND box is checked\\n    let inBetween = false;\\n    if (e.shiftKey && this.checked) {\\n      // loop through each checkbox\\n      checkboxes.forEach(checkbox => {\\n        // these are the outer bounds of the selection\\n        if (checkbox === this || checkbox === lastChecked) {\\n          inBetween = !inBetween;\\n        }\\n        // whilst in between is true - check those boxes\\n        if (inBetween === true) {\\n          checkbox.checked = true;\\n        }\\n      });\\n    }\\n    lastChecked = this;\\n  }\\n\\n  // add event listener to handle clicks\\n  checkboxes.forEach(checkbox => checkbox.addEventListener('click', handleCheck) );\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"// Add this script tag\\n<script type=\\\"text/javascript\\\" src=\\\"https://www.cornify.com/js/cornify.js\\\"></script>\\n...\\n// Then call this function to put corny shit on your webpage\\ncornify_add()\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"if (conditions) {\\n  image.classList.add('active');\\n} else {\\n  image.classList.remove('active');\\n}\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"/* take half a second to transition,  initial set to invisible*/\\n.slide-in {\\n  opacity: 0;\\n  transition: all .5s;\\n}\\n\\n/* on active,  set to invisible*/\\n.slide-in.active {\\n  opacity: 1;\\n}\\n\",\"language\":\"css\"}],[\"code\",{\"code\":\"// const\\nconst age = 100;\\nconst age2 = age;\\nage2 = \\\"new\\\"; // 🛑 TypeError: invalid assignment to const\\n\\n// both var + let behave the same\\nlet num = 100;\\nlet num2 = num;\\nnum2 = 200;\\nconsole.log(num, num2) // 👍 100 200 \\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"// Make a copy of an array\\nconst players = ['Wes', 'Sarah', 'Ryan', 'Poppy'];\\nconst team = players;\\nteam[1] = \\\"new\\\";\\nconsole.log(players, team)\\n// [ \\\"Wes\\\", \\\"new\\\", \\\"Ryan\\\", \\\"Poppy\\\" ]\\n// [ \\\"Wes\\\", \\\"new\\\", \\\"Ryan\\\", \\\"Poppy\\\" ]\\n// 🛑 changes both `team` and `players` because `team` is just a pointer to `players`\\n\\n// solution\\nconst players = ['Wes', 'Sarah', 'Ryan', 'Poppy'];\\nconst team = players.slice();\\nteam[1] = \\\"new\\\";\\nconsole.log(players, team)\\n// [ \\\"Wes\\\", \\\"Sarah\\\", \\\"Ryan\\\", \\\"Poppy\\\" ]\\n// [ \\\"Wes\\\", \\\"new\\\", \\\"Ryan\\\", \\\"Poppy\\\" ]\\n// 👍 could also do `const team = [...players];`\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"// with Objects\\nconst person = {\\n  name: 'Wes Bos',\\n  age: 80\\n};\\n\\n// attempt to make a copy:\\nconst captain = person;\\ncaptain.number = 99;\\nconsole.log(person)\\n// Object { name: \\\"Wes Bos\\\", age: 80, number: 99 }\\n// 🛑 this has modified the original\\n\\n// solution\\nconst captain2 = Object.assign({}, person)\\ncaptain2.number = 100;\\nconsole.log(person)\\n// Object { name: \\\"Wes Bos\\\", age: 80, number: 99 } 👍\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"// args are key value\\nlocalStorage.setItem('items', JSON.stringify(myObject))\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"const { offsetWidth: width, offsetHeight: height} = hero\\nlet { offsetX: x, offsetY: y } = e;\\n\\n// uses this if statement to handle when the target is the whole screen, to then add on the screen offset\\nif (this !== e.target) {\\n  x = x + e.target.offsetLeft;\\n  y = y + e.target.offsetTop;\\n}\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"let { clientX: x, clientY: y} = e;\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"const walk = 100 //px\\nconst xWalk = (x / width * walk) - (walk / 2);\\nconst yWalk = (y / height * walk) - (walk / 2);\\n\\ntext.style.textShadow = `${xWalk}px ${yWalk}px 0 rgba(255,0,255,0.7)`\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"let sorted = bandsArray.sort((a,b) => {\\n  if (strip(a) > strip(b)) {\\n    return 1\\n  }\\n  if (strip(a) < strip(b)) {\\n    return -1\\n  }\\n  return 0\\n})\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"document.querySelector('#ulDivId').innerHTML =\\n  sortedArray.map(item =>\\n    `<li>${item}</li>`\\n  ).join('');\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"const videos = document.querySelectorAll('li')\\nconst times = [];\\n\\nvideos.forEach(vid => {\\n  times.push(vid.dataset.time)\\n});\\n\\nconst totalTime = times.map(x => {\\n  const [min, sec] = x.split(':');\\n  return Number(min) * 60 + Number(sec);\\n}).reduce((acc, val) => {\\n  return acc + val;\\n})\\n\\nconst mins = Math.trunc(totalTime / 60);\\nconst secs = totalTime % 60;\\nconsole.log(mins, ':', secs)\\n// 298 : 58 - I should handle hours but meh 🤷‍\\n\",\"language\":\"js\"}],[\"code\",{\"code\":\"  <!-- 404 -->\\n  <audio class=\\\"snap\\\" src=\\\"https://wesbos.com/demos/photobooth/snap.mp3\\\" hidden></audio>\\n  <!-- replace with a local sound file -->\\n  <audio class=\\\"snap\\\" src=\\\"camera_sound.mp3\\\" hidden></audio>\",\"language\":\"html\"}],[\"code\",{\"language\":\"js\",\"code\":\"function getVideo() {\\n  navigator.mediaDevices.getUserMedia({\\n    video: true,\\n    audio: false\\n  }).then(localMediaStream => {\\n    video.srcObject = localMediaStream;\\n    video.play();\\n  })\\n  .catch(\\n    err => {\\n      console.error(\\\"Whoops Error: \\\", err);\\n    });\\n}\"}],[\"code\",{\"code\":\"console.log(pixels);\\ndebugger\"}],[\"code\",{\"language\":\"js\",\"code\":\"const recognition = new SpeechRecognition();\\n  recognition.interimResults = true;\\n  recognition.lang = 'en-US';\\n\\n  let p = document.createElement('p');\\n  const words = document.querySelector('.words')\\n  words.appendChild(p)\\n\\n  recognition.addEventListener('result', e => {\\n    const transcript = Array.from(e.results)\\n      .map(result => result[0])\\n      .map(result => result.transcript)\\n      .join('')\\n\\n    p.textContent = transcript;\\n    // create a new paragraph if the speaking has finished\\n    if (e.results[0].isFinal) {\\n      p = document.createElement('p')\\n      words.appendChild(p);\\n    }\\n    console.log(transcript); // the results\\n  })\\n\\n  recognition.addEventListener('end', recognition.start)\\n  recognition.start()\\n\"}],[\"image\",{\"src\":\"/content/images/2020/05/Screen-Shot-2020-05-09-at-20.19.21.png\",\"cardWidth\":\"\"}],[\"image\",{\"src\":\"/content/images/2020/05/Screen-Shot-2020-05-09-at-20.20.36.png\"}],[\"image\",{\"src\":\"/content/images/2020/05/Screen-Shot-2020-05-09-at-20.35.50.png\"}],[\"code\",{\"language\":\"js\",\"code\":\"    // the request to use the geolocation of the client will ask for permissions from user\\n    navigator.geolocation.watchPosition((data) => {\\n      console.log(data) // see the data object\\n\\n      // update dom elements with the data\\n      speed.textContent = data.coords.speed;\\n      arrow.style.transform = `rotate(${data.coords.heading}deg)`\\n    }, (err) => {\\n      // if they reject the permissions\\n      console.log(err)\\n      alert(\\\"you privacy nut\\\")\\n    }\"}],[\"code\",{\"language\":\"js\",\"code\":\"const triggers = document.querySelectorAll('a');\\ntriggers.forEach(a => {\\n    a.addEventListener('mouseenter', addHighlight)\\n}\"}],[\"code\",{\"language\":\"js\",\"code\":\"function addHighlight(e) {\\n    const domDimension = this.getBoundingClientRect();\\n}\"}],[\"code\",{\"language\":\"js\",\"code\":\"const highlight = document.createElement('span');\\nhighlight.classList.add('highlight');\\ndocument.body.appendChild(highlight);\\n\\nfunction addHighlight(e) {\\n    ...\\n    highlight.style.width = `${domDimension.width}px`\\n    highlight.style.height = `${domDimension.height}px`\\n    highlight.style.transform = `translate(${domDimension.x}px,${domDimension.y}px)`\\n}\"}],[\"code\",{\"language\":\"js\",\"code\":\"msg.voice = voices.filter(v => v.name === this.value)[0]\\n// vs find\\nmsg.voice = voices.find(v => v.name === this.value)\\n\"}],[\"code\",{\"language\":\"js\",\"code\":\"const navbar = document.querySelector('#main');\\n// top of navbar location\\nconst navHeight = navbar.offsetTop;\\n\\nfunction moveNavBar(e) {\\n  const scrollAmt = window.scrollY;\\n\\n  if (scrollAmt >= navHeight) {\\n    document.body.classList.add('fixed-nav');\\n    document.body.style.paddingTop = `${navbar.offsetHeight}px`\\n\\n  } else {\\n    document.body.style.paddingTop = 0\\n    document.body.classList.remove('fixed-nav');\\n  }\\n}\\n\\ndocument.addEventListener('scroll', _.debounce(moveNavBar, 10));\"}],[\"code\",{\"language\":\"js\",\"code\":\"divs.forEach(div => {\\n  div.addEventListener('click', logText, {\\n    capture: true, // 'trickle' down events\\n    once: true // removeEventLister - unbind once event occurs once\\n  })\\n});\"}],[\"code\",{\"language\":\"js\",\"code\":\"  const triggers = document.querySelectorAll('.cool > li')\\n  const background = document.querySelector('.dropdownBackground')\\n\\n  function handleEnter() {\\n    background.classList.add('open')\\n    this.classList.add('trigger-enter');\\n    this.classList.add('trigger-enter-active');\\n\\n    // don't forget you can querySelector on `this`\\n    const dropdown = this.querySelector('.dropdown');\\n    const dropdownDim = dropdown.getBoundingClientRect();\\n    background.style.setProperty('width', `${dropdownDim.width}px`)\\n  }\\n\\n  function handleLeave(params) {\\n    this.classList.remove('trigger-enter', 'trigger-enter-active')\\n  }\\n\\n  triggers.forEach(trigger => trigger.addEventListener('mouseenter', handleEnter));\\n  triggers.forEach(trigger => trigger.addEventListener('mouseleave', handleLeave));\\n\"}],[\"code\",{\"code\":\"// 🛑 errors\\nlet yourVar = true;\\nlet yourVar = false;\\n\\n// ✅ happy days\\nlet yourVar = true;\\nyourVar = false;\",\"language\":\"js\"}],[\"code\",{\"language\":\"js\",\"code\":\"const yourVarName = 10;\\n// slow way\\nconsole.log('yourVarName', yourVarName);\\n// fast way\\nconsole.log({yourVarName});\"}],[\"code\",{\"language\":\"html\",\"code\":\"<form name=\\\"customForm\\\" id=\\\"custom\\\">\\n    <input type=\\\"text\\\" name=\\\"minutes\\\" placeholder=\\\"Enter Minutes\\\">\\n</form>\"}],[\"code\",{\"language\":\"js\",\"code\":\"document.customForm.addEventListener('submit', function(e){...})\\n// Nested elements can be accessed too\\ndocument.customForm.minutes.addEventListener('onhover', function(e){...})\"}],[\"code\",{\"code\":\"// <li data-time=\\\"5:23\\\"> Video 26</li>\\ndocument.querySelectorAll('[data-time]')\\n\",\"language\":\"js\"}]],\"markups\":[[\"a\",[\"href\",\"https://javascript30.com/\"]],[\"code\"],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element/audio\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_data_attributes\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors\"]],[\"em\"],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video#Events\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video#Attributes\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Konami_Code\"]],[\"a\",[\"href\",\"https://github.com/ritwickdey/vscode-live-server\"]],[\"a\",[\"href\",\"https://lodash.com/docs/4.17.15#debounce\"]],[\"a\",[\"href\",\"https://lodash.com/\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/Window/scrollY\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/Window/innerHeight\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLelement/offsetParent\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/Element/classList\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Errors/Invalid_const_assignment\"]],[\"a\",[\"href\",\"https://lodash.com/docs/4.17.15#cloneDeep\"]],[\"a\",[\"href\",\"https://stackoverflow.com/a/25419538/3691003\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/MouseEvent/offsetX\"]],[\"a\"],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element/ul\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/Reduce\"]],[\"a\",[\"href\",\"#13-slide-in-on-scroll\"]],[\"a\",[\"href\",\"https://www.browsersync.io/\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/URL/createObjectURL\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/srcObject\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator/mediaDevices/getUserMedia\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/MediaStream\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator/geolocation\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator/connection\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator/webdriver\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/Element/getBoundingClientRect\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/CSS/transform-function/translate\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesis\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/filter\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/find\"]],[\"a\",[\"href\",\"https://stackoverflow.com/a/4106585/3691003\"]],[\"a\",[\"href\",\"https://www.sitepoint.com/event-bubbling-javascript/\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/MouseEvent/pageX\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetLeft\"]],[\"strong\"],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetParent\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/API/Event/isTrusted\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"📚Course: \"],[0,[0],1,\"Javascript30\"],[1,[],0,0],[0,[],0,\"💰Price: Free\"],[1,[],0,1],[0,[],0,\"⭐️Rating: 8.5/10\"]]],[1,\"h2\",[[0,[],0,\"Review\"]]],[1,\"p\",[[0,[],0,\"What could be better? Not a lot. I think Wes (the instructor) could have thought harder about prompting learners to pause the video and trying solo for themselves. He does once or twice somewhere in the first 10 lessons. But didn't do it after, which meant I tended to follow along waiting for the prompt. There are a couple of mistakes here and there - one was on a resource Wes was hosting returning a \"],[0,[1],1,\"404\"],[0,[],0,\" for one of the lessons, but after emailing Wes fixed it about a week later. I thought some items could have been explained better e.g. CSS query selectors and the different syntax to use.\"]]],[1,\"h2\",[[0,[],0,\"01 - JavaScript Drum Kit\"]]],[3,\"ul\",[[[0,[],0,\"There's an \"],[0,[1],1,\"<audio>\"],[0,[],0,\" div \"],[0,[2],1,\"tag for embedded audio.\"]],[[0,[],0,\"Adding event listeners is easy, in this example :\"]]]],[10,0],[3,\"ul\",[[[0,[3],1,\"HTML data attributes\"],[0,[],0,\" - store additional info on html elements using \"],[0,[1],1,\"data-\"],[0,[],0,\" prefix:\"]],[[0,[],0,\"The data of the elements can then be accessed using \"],[0,[1],1,\"dataset\"],[0,[],0,\":\"]]]],[10,1],[10,2],[1,\"h2\",[[0,[],0,\"02 - Clock\"]]],[3,\"ul\",[[[0,[],0,\"Select any element with \"],[0,[1],1,\"document.querySelector('.second-hand');\"],[0,[],0,\". Where \"],[0,[1],1,\".second-hand\"],[0,[],0,\" is a class name. Known as \"],[0,[4],1,\"CSS selectors\"],[0,[],0,\".\"]],[[0,[],0,\"Use \"],[0,[1],1,\"setInterval(yourfn, 1000)\"],[0,[],0,\" to run a function on a repeated time interval.\"]]]],[1,\"h2\",[[0,[],0,\"03 - CSS Variables\"]]],[3,\"ul\",[[[0,[],0,\"Pattern, get all elements you want to manipulate, create a function defining that change, create trigger for that function.\"]]]],[10,3],[1,\"h2\",[[0,[],0,\"04 - array cardio day 1\"]]],[1,\"p\",[[0,[5],1,\"Array methods map(), reduce(), reduce()  and  filter() are introduced.\"]]],[3,\"ul\",[[[0,[],0,\"Log a table to the console, i.e. a list of objects\"]]]],[10,4],[1,\"h1\",[[0,[],0,\"05 - Flex panels\"]]],[3,\"ul\",[[[0,[],0,\"Light touch on some flex panels (pretty foreign though, without much intro)\"]],[[0,[],0,\"Pattern of applying some transforms and transitions css using js\"]]]],[10,5],[1,\"h2\",[[0,[],0,\"06 - Type Ahead\"]]],[1,\"p\",[[0,[],0,\"Implement a search box against json data.\"]]],[3,\"ul\",[[[0,[],0,\"Get data via a promise:\"]]]],[10,6],[3,\"ul\",[[[0,[],0,\"Regex can be used like this:\"]]]],[10,7],[1,\"h2\",[[0,[],0,\"07 - array cardio day 2\"]]],[1,\"p\",[[0,[5],1,\"Some extra array methods\"]]],[3,\"ul\",[[[0,[1],1,\"Array.prototype.some()\"],[0,[],0,\" - checks if at least one element meets a condition is met. Returns Boolean.\"]],[[0,[1],1,\"Array.prototype.every()\"],[0,[],0,\" -  - checks if all elements meet a condition is met. Returns Boolean.\"]],[[0,[1],1,\"Array.prototype.find()\"],[0,[],0,\" - returns element from array that meets the specified condition.\"]],[[0,[1],1,\"Array.prototype.findIndex()\"],[0,[],0,\" - like \"],[0,[1],1,\"find()\"],[0,[],0,\" but returns the index.\"]],[[0,[1],1,\"Array.prototype.splice()\"],[0,[],0,\" - delete the element at a given index.\"]]]],[1,\"h2\",[[0,[],0,\"08 - Fun with HTML5 Canvas\"]]],[1,\"p\",[[0,[5],1,\"Could have provided more context on the why of canvas vs alternatives of divs, could have been more structured as well. What is canvas? What are its features? etc\"]]],[3,\"ul\",[[[0,[],0,\"Canvas has a context, where you can update it's properties:\"]]]],[10,8],[3,\"ul\",[[[0,[],0,\"Instead of \"],[0,[1],1,\"rbg()\"],[0,[],0,\" or hex colors you can set color using hsl(): \"],[0,[1],1,\"style=\\\"color: hsl(200, 100%, 50%)\\\"\"]]]],[1,\"h2\",[[0,[],0,\"09 - Dev Tools Domination\"]]],[1,\"p\",[[0,[5],1,\"Depending on how familiar one is with the dev tools, some very good tips to be had\"]]],[3,\"ul\",[[[0,[],0,\"Various methods of \"],[0,[1],1,\"console\"],[0,[],0,\", e.g. \"],[0,[1],1,\"console.warn()\"],[0,[],0,\" \"],[0,[1],1,\"console.info()\"]],[[0,[],0,\"Break on a paticular element action, inspect the element, right click on element in dev tools and add a 'break on' -> '\"],[0,[5],1,\"choose an option\"],[0,[],0,\"'\"]],[[0,[1],1,\"console.groupCollapsed()\"],[0,[],0,\" group a bunch console logs and display them by default as collapsed.\"]]]],[10,9],[3,\"ul\",[[[0,[1],1,\"console.count\"],[0,[],0,\" will print the contents and also the count of how many times that content has been logged\"]],[[0,[1],1,\"console.time('X operation took:)\"],[0,[],0,\" put your code you want to time and end it with \"],[0,[1],1,\"console.timeEnd(X operation took:)\"]],[[0,[5],1,\"Covered in earlier episodes but very handy\"],[0,[],0,\" display in the console in tabular form an array of objects \"],[0,[1],1,\"console.table(myArrayOfObjects)\"]]]],[1,\"h2\",[[0,[],0,\"10 - Hold Shift and Check Checkboxes\"]]],[1,\"p\",[[0,[5],1,\"I found this one hard - and then returned to it after a 7mth break 😱\"]]],[3,\"ul\",[[[0,[],0,\"No new techniques in this lesson, but a harder application of the pattern of, select DOM elements, define callback function\"]]]],[10,10],[1,\"h2\",[[0,[],0,\"11 - Custom Video Player\"]]],[3,\"ul\",[[[0,[],0,\"I found this easier to fly solo on but also a good few hours of work, you get a video in a page, but the buttons to play/pause, skip etc don't work. The first thing I needed was some documentation on what events I could use on the \"],[0,[1],1,\"video\"],[0,[],0,\". W3 Schools was the first link, but not too helpful. Mozilla to the rescue with a nice list of \"],[0,[6],1,\"video events\"],[0,[],0,\".\"]],[[0,[],0,\"You'll also need to know what the \"],[0,[7],1,\"video attributes are\"],[0,[],0,\".\"]]]],[1,\"h2\",[[0,[],0,\"12 - Key Sequence Detection\"]]],[3,\"ul\",[[[0,[],0,\"This was a half hour one, much better after day 11. The aim is to do some action on the detection of some sequence of keystrokes, which TIL is called a \"],[0,[8],1,\"Konami_Code\"],[0,[],0,\".\"]],[[0,[],0,\"Solution involved adding an eventlistener to the event \"],[0,[1],1,\"keydown\"],[0,[],0,\", push the keys in an array, and checking for a sequence matching the konami code.\"]],[[0,[],0,\"And another amazing tidbit\"]]]],[10,11],[1,\"h2\",[[0,[],0,\"13 - Slide in on Scroll\"]]],[1,\"p\",[[0,[5],1,\"Tip\"],[0,[],0,\" - At this point I'm getting a little tired of the \"],[0,[1],1,\"CMD+S\"],[0,[],0,\" to save in VS Code, \"],[0,[1],1,\"CMD+TAB\"],[0,[],0,\" to switch applications to the browser and then \"],[0,[1],1,\"CMD+SHIFT+R\"],[0,[],0,\" to hard refresh. The \"],[0,[9],1,\"Live Server extension\"],[0,[],0,\" solves that problem - as soon as you save, the page reloads in the browser you launch with it.\"]]],[3,\"ul\",[[[0,[],0,\"For the interest of keeping JS30 plain js Wes copies in the \"],[0,[10,1],1,\"debounce\"],[0,[],1,\" function\"],[0,[],0,\" from the \"],[0,[11,1],1,\"lodash\"],[0,[],1,\" library\"],[0,[],0,\". I personally think importing this library and a brief explainer on why lodash is such a widely popular library would have been good.\"]],[[0,[],0,\"Calculating how far someone has scrolled on a web page:\"]],[[0,[1],1,\"window.scrollY\"],[0,[],0,\" - how far you have scrolled measured at the top of the window \"],[0,[12],1,\"(mdn ref)\"],[0,[],0,\".\"]],[[0,[1],1,\"window.innerHeight\"],[0,[],0,\" - the height of the window, measured using the viewport, i.e. what is visible \"],[0,[13],1,\"(mdn ref)\"],[0,[],0,\".\"]],[[0,[1],1,\"HTMLElement.offsetTop\"],[0,[],0,\" - the distance of the element relative to the offsetParent node, in this tutorial that is the body \"],[0,[14],1,\"(mdn ref)\"],[0,[],0,\".\"]],[[0,[1],1,\"Element.classList\"],[0,[],0,\" - '... class attributes of the element. This can then be used to manipulate the class list.' \"],[0,[15],1,\"(mdn ref)\"],[0,[],0,\".\"]],[[0,[],0,\"crux of the lesson is utilising the above properties to work out where on a page the user has currently scrolled and to conditionally apply CSS already written for you.\"]]]],[10,12],[1,\"p\",[[0,[],0,\"Simplified CSS\"]]],[10,13],[1,\"h2\",[[0,[],0,\"14 - JavaScript References VS Copying\"]]],[3,\"ul\",[[[0,[],0,\"This covers some js idiosyncrasies on reassignment and copying variables, arrays and objects.\"]],[[0,[],0,\"Starting off with variables, variables can be reassigned. \"],[0,[1],1,\"const\"],[0,[],0,\" is not covered which behaves differently. \"],[0,[16],1,\"mdn ref\"]]]],[10,14],[3,\"ul\",[[[0,[],0,\"Arrays are different:\"]]]],[10,15],[3,\"ul\",[[[0,[],0,\"Objects also need a special method to copy, however, the following will only do one level deep of the object.\"]]]],[10,16],[3,\"ul\",[[[0,[],0,\"lodash has a \"],[0,[17],1,\"cloneDeep method\"],[0,[],0,\" to do a full object copy\"]]]],[1,\"h2\",[[0,[],0,\"15 - Local Storage and Event Delegation\"]]],[1,\"p\",[[0,[],0,\"N.b - this one is big, video is over 30mins.\"]]],[3,\"ul\",[[[0,[],0,\"This covers topics that seem like a precursor to react, storing state and using parent elements to instruct child elements.\"]],[[0,[],0,\"When working with forms and wanting to perform actions upon form submission, the appropriate event to listen for is \"],[0,[1],1,\"submit\"],[0,[],0,\". By default the submit form event will reload the page. To prevent this use \"],[0,[1],1,\"e.preventDefault()\"],[0,[],0,\" in the handler.\"]],[[0,[],0,\"Persist data on the client by using local storage, a key value store, where your value needs to be a string\"]]]],[10,17],[3,\"ul\",[[[0,[],0,\"If your data is not what you expect and you see \"],[0,[1],1,\"[object Object]\"],[0,[],0,\" instead - you most likely have tried to convert an object to a string \"],[0,[1],1,\"Object().toString()\"],[0,[],0,\". There are \"],[0,[18],1,\"other variations of this behaviour\"],[0,[],0,\" - because everything in javascript is an object.\"]]]],[1,\"h2\",[[0,[],0,\"16 - CSS Text Shadow Mouse Move Effect\"]]],[3,\"ul\",[[[0,[],0,\"Wes takes a weird approach of grabbing the hero elements offset x and y values and the event's offset x and y values. When the mousemove event occurs over another element on the page the x and  y values are then calculated from \"],[0,[19,5],2,\"the the padding edge of the target node\"],[0,[],0,\". A much simpler approach is taking the clientX and clientY values. Wes approach:\"]]]],[10,18],[1,\"p\",[[0,[],0,\"Much easier to use the following:\"]]],[10,19],[3,\"ul\",[[[0,[],0,\"A graphic effect is applied using the following\"]]]],[10,20],[1,\"h2\",[[0,[],0,\"17 - Sorting Band Names without articles\"]]],[1,\"p\",[[0,[],0,\"N.b. An 'article', being one of the grammatical articles. In this coding example, The definite article 'the' or the indefinite articles 'an' or 'a'.\"]]],[3,\"ul\",[[[0,[],0,\"Wes takes a really nice and succinct approach here utilising the \"],[0,[20],1,\"Array.prototype.sort()\"],[0,[],0,\". However I prefer the mdn docs approach which is a little more verbose:\"]]]],[10,21],[3,\"ul\",[[[0,[],0,\"To append this to a div, e.g. \"],[0,[21],0,\"the unordered list \"],[0,[1],2,\"<ul>\"],[0,[],0,\":\"]]]],[10,22],[1,\"h2\",[[0,[],0,\"18 - Tally String Times with Reduce\"]]],[1,\"p\",[[0,[],0,\"N.b. simple if you have some knowledge of map \"],[0,[22],1,\"reduce functions\"],[0,[],0,\".\"],[1,[],0,2],[0,[],0,\"My answer pretty similar to Wes's except instead of using a forEach to append the values into an array, he used a much nicer \"],[0,[1],1,\"const timeNodes = Array.from(document.querySelectorAll('[data-time]'));\"],[0,[],0,\". My answer:\"]]],[10,23],[1,\"h2\",[[0,[],0,\"19 - Unreal Webcam Fun\"]]],[1,\"p\",[[0,[5],1,\"N.b. This was a slower start with a few issues to iron out cover in the first 3 bullet points...\"]]],[3,\"ul\",[[[0,[],0,\"The files so far have been read locally into the browser, such that in the address bar you'll find the file path \"],[0,[1],1,\"file://\"],[0,[],0,\" instead of \"],[0,[1],1,\"https://\"],[0,[],0,\". However the use of the webcam requires a server due to accessing the computer's camera and the security requirement that the website is a secure origin, \"],[0,[1],1,\"https\"],[0,[],0,\" or \"],[0,[1],1,\"localhost\"],[0,[],0,\". Therefore we need to spin up a local server. If developing with \"],[0,[23],1,\"VSCode with the Live Server extension\"],[0,[],0,\" this has already been done for you. I've used Python's \"],[0,[1,1],2,\"http.server\"],[0,[],0,\" (used to be called \"],[0,[1,1],2,\"SimpleHTTPServer\"],[0,[],0,\"). \"],[0,[24],1,\"Browsersync\"],[0,[],0,\"  seems like the JavaScript equilavent.\"]],[[0,[],0,\"I got some intitial \"],[0,[1],1,\"404\"],[0,[],0,\" errors where the html template pointed to a file that no longer exists. I fixed this by downloading a camera sound locally.\"]]]],[10,24],[3,\"ul\",[[[0,[],0,\"The video tutorial for this instructs you to use \"],[0,[25,1],1,\"createObjectURL\"],[0,[],1,\" which has now been deprecated\"],[0,[],0,\". I wasted some time here, checking I had done things correctly, searching for the error until I saw a note in the solution on this to use \"],[0,[1,26],2,\"srcObject\"],[0,[],0,\".\"]],[[0,[],0,\"To access a user's web camera use \"],[0,[27,1,28],3,\"navigator.mediaDevices.getUserMedia()\"],[0,[],0,\". This returns a \"],[0,[29,1],2,\"Promise\"],[0,[],0,\" that resolves to a \"],[0,[30,1],2,\"MediaStream\"],[0,[],0,\" object.\"]]]],[10,25],[3,\"ul\",[[[0,[31],1,\"The Navigator interface\"],[0,[],0,\" enables you to query some interesting properties of the user e.g. \"],[0,[1,32],2,\"geolocation\"],[0,[],0,\", \"],[0,[1,33],2,\"connection\"],[0,[],0,\"  and \"],[0,[1,34],2,\"webdriver\"],[0,[],0,\".\"]],[[0,[],0,\"If you want to pause the execution and enter the debugger at a particular point of your code, just write \"],[0,[1],1,\"debugger\"],[0,[],0,\".\"]]]],[10,26],[1,\"h2\",[[0,[],0,\"20 - Native Speech Recognition\"]]],[1,\"p\",[[0,[],0,\"N.b. This was definitely a frustrating first 5mins of the tutorial, where if you're following along you don't get anything like what Wes is getting. This is because he is demonstrating the finished app on the left half of the screen whilst coding from scratch the app. It's not until 5mins when switches that he realises that it's not working.\"]]],[3,\"ul\",[[[0,[],0,\"This only works on Chrome, not Firefox.\"]]]],[10,27],[1,\"h2\",[[0,[],0,\"21 - Geolocation based Speedometer and Compass\"]]],[3,\"ul\",[[[0,[],0,\"This requires using Xcode to simulate someone using there phone whilst moving around.\"]]]],[10,28],[3,\"ul\",[[[0,[],0,\"Once the simulator boots you'll have an iphone on screen. Then turn on a 'City Run'.\"]]]],[10,29],[3,\"ul\",[[[0,[],0,\"Then open Safari to connect the dev tools to the simulator iphone\"]]]],[10,30],[3,\"ul\",[[[0,[],0,\"And to access the geo data:\"]]]],[10,31],[1,\"h2\",[[0,[],0,\"22 - Follow Along Links\"]]],[1,\"p\",[[0,[5],1,\"N.b. lesson applies an element that hovers over the link the mouse is currently positioned over.\"]]],[3,\"ul\",[[[0,[],0,\"Method: Create a 'highlight' DOM element, and transition it based on the \"],[0,[1],1,\"mouseenter\"],[0,[],0,\" event on any \"],[0,[1],1,\"<a>\"],[0,[],0,\" divs.\"]],[[0,[],0,\"Create event listener\"]]]],[10,32],[3,\"ul\",[[[0,[],0,\"To get the coodinates and size of the current \"],[0,[1],1,\"a\"],[0,[],0,\" div, use \"],[0,[1,35],2,\"getBoundingClientRect()\"],[0,[],0,\" in the callback.\"]]]],[10,33],[3,\"ul\",[[[0,[1,36],2,\"translate(Xpx, Ypx)\"],[0,[],0,\" - CSS function that repositions an element. Use this in the call back to transition the highlight element. Wes uses the \"],[0,[1],1,\"top\"],[0,[],0,\" and \"],[0,[1],1,\"left\"],[0,[],0,\" properties, but X and Y works just the same and makes more sense to me.\"]]]],[10,34],[1,\"h2\",[[0,[],0,\"23 - Speech Synthesis\"]]],[1,\"p\",[[0,[],0,\"N.b. - Firefox wasn't returning the voice objects from \"],[0,[1],1,\"speechSynthesis.getVoices()\"],[0,[],0,\" at page load - switched to Chrome for this one.\"]]],[3,\"ul\",[[[0,[],0,\"Use the experimental \"],[0,[37],1,\"SpeechSynthesis API\"],[0,[],0,\".\"]],[[0,[],0,\"When filtering an array based on a singular user selection, I naturally wanted to \"],[0,[1,38],2,\"filter()\"],[0,[],0,\" however this returns an array which you will have to select an element from. However, \"],[0,[1,39],2,\"find()\"],[0,[],0,\"will return the first element that meets the match critera.\"]]]],[10,35],[1,\"h2\",[[0,[],0,\"24 - Sticky Nav\"]]],[1,\"p\",[[0,[5],1,\"Fix a navbar to the top as you scroll down.\"]]],[3,\"ul\",[[[0,[],0,\"Add an event listenr on the navbar and for the callback place a control statement to detect when the \"],[0,[5],1,\"fixing\"],[0,[],0,\" of the nav should apply.\"]]]],[10,36],[3,\"ul\",[[[0,[],0,\"To find the hight of a DOM element there looks to be to properties you could use. Here's the \"],[0,[40],1,\"difference between clientHeight vs offsetHeight,\"],[0,[],0,\" go with offsetHeight.\"]]]],[1,\"h2\",[[0,[],0,\"25 - Event Capture, Propagation, Bubbling and Once\"]]],[1,\"p\",[[0,[5],1,\"This is a very quick practical on the concept of bubbling up and propagation. Requires some extra research if new to the concept.\"]]],[3,\"ul\",[[[0,[41],1,\"Event propagation\"],[0,[],0,\" - is the blanket term for both event bubbling and event capturing.\"]],[[0,[],0,\"If you have 3 divs nested and place an event listener on those divs. Then load the page and click on the inner most div and log which div has been clicked on. The browser will show all divs from inner most to outer most \"],[0,[1],1,\"<body>\"],[0,[],0,\" as having been clicked on - this is \"],[0,[5],1,\"bubbling up.\"]],[[0,[],0,\"Behind the scenes the browser is first capturing the event by 'trickling down' the DOM hierarchy until the target on which the event occurred i.e. the click, is reached.\"]],[[0,[],0,\"To make use of the third argument to an event listener:\"]]]],[10,37],[1,\"h2\",[[0,[],0,\"26 - Stripe Follow Along Dropdown\"]]],[1,\"p\",[[0,[5],1,\"Extension of 22.\"]]],[3,\"ul\",[[[0,[],0,\"I was trying to access the child nodes of \"],[0,[1],1,\"this\"],[0,[],0,\" within the context of a callback by traversing the tree e.g. \"],[0,[1],1,\"this.children.item(0)\"],[0,[],0,\". But of course, you can just \"],[0,[1],1,\"this.querySelector\"],[0,[],0,\" to query the tree of \"],[0,[1],1,\"this\"],[0,[],0,\".\"]],[[0,[],0,\"The crux of the logic relied on adding and removing CSS classes. The pared down version:\"]]]],[10,38],[1,\"h2\",[[0,[],0,\"27 - Click and Drag to Scroll\"]]],[3,\"ul\",[[[0,[],0,\"Lesson centres on making use of 4 mouse events \"],[0,[1],1,\"mousedown\"],[0,[],0,\", \"],[0,[1],1,\"mouseleave\"],[0,[],0,\", \"],[0,[1],1,\"mouseup\"],[0,[],0,\" and \"],[0,[1],1,\"mousemove\"],[0,[],0,\" to build the controls around clicking and dragging.\"]],[[0,[],0,\"As flexible as you think \"],[0,[1],1,\"let\"],[0,[],0,\" is, you will encounter errors if you try to redeclare a variable created with \"],[0,[1],1,\"let\"],[0,[],0,\".\"]]]],[10,39],[3,\"ul\",[[[0,[],0,\"You can \"],[0,[1],1,\"console.log\"],[0,[],0,\" the var name and value at the same time by wrapping it in an object:\"]]]],[10,40],[3,\"ul\",[[[0,[42],1,\"MouseEvent.pageX\"],[0,[],0,\" - \\\"returns the X (horizontal) coordinate (in pixels) at which the mouse was clicked, relative to the left edge of the entire document. This includes any portion of the document not currently visible.\\\"\"]],[[0,[43],1,\"HTMLElement.offsetLeft\"],[0,[],0,\" - \\\"The \"],[0,[44,1],2,\"HTMLElement.offsetLeft\"],[0,[],0,\" read-only property returns the number of pixels that the \"],[0,[5],1,\"upper left corner\"],[0,[],0,\" of the current element is offset to the left within the \"],[0,[45,1],2,\"HTMLElement.offsetParent\"],[0,[],0,\" node.\\\"\"]]]],[1,\"h2\",[[0,[],0,\"28 - Video Speed Controller UI\"]]],[1,\"p\",[[0,[5],1,\"This is a small lesson at ~9mins and doesn't introduce any new content.  Mostly a simpler version of Lesson 11.\"]]],[1,\"h2\",[[0,[],0,\"29 - Countdown Clock\"]]],[1,\"p\",[[0,[5],1,\"Not a lot new in this one - but it is a fun use of the skills learnt to date in making a countdown clock.\"]]],[3,\"ul\",[[[0,[1],1,\"setInteval\"],[0,[],0,\" will continue to run. So, if using it within a function that gets called multiple times, you may need to have a \"],[0,[1],1,\"clearInterval()\"],[0,[],0,\" call to clear any existing process.\"]],[[0,[],0,\"If an html element has a name attribute, it can be accessed directly from the document.\"]]]],[10,41],[10,42],[1,\"h2\",[[0,[],0,\"30 - Whack A Mole Game\"]]],[1,\"p\",[[0,[5],1,\"As the title suggests this is a bit of a fun simple game that is using the concepts learnt in previous lessons - a good way to end the course.\"]]],[3,\"ul\",[[[0,[],0,\"On event's \"],[0,[46],0,\"there is a \"],[0,[1],1,\"isTrusted\"],[0,[],1,\" boolean property\"],[0,[],0,\" that can be utilised to ensure the action on the web page is user generated. \"]]]],[1,\"h2\",[[0,[],0,\"Addendum\"]]],[3,\"ul\",[[[0,[],0,\"Review - For someone with some programming experience and understanding who wants to focus on improving JavaScript skills, working with the APIs and developing with the browser this is a fantastic course. A couple of lessons are out-of-date however the finished solutions available in the repo are mostly up-to-date. I really like the honesty in the mistakes and even debugging his lesson code on video - it gives a much more honest representation of what programming is like. If a JavaScript jedi like Wes makes mistakes and spends time debugging so to will you.\"]],[[0,[],0,\"TODO write summary and find good doc on document.querySelector(THISBIT), by\"],[1,[],0,3],[0,[],0,\"- id\"],[1,[],0,4],[0,[],0,\"- class\"],[1,[],0,5],[0,[],0,\"- attribute e.g. \"],[0,[1],1,\"data-time\"]]]],[10,43]]}","html":"<p>📚Course: <a href=\"https://javascript30.com/\">Javascript30</a><br>💰Price: Free<br>⭐️Rating: 8.5/10</p><h2 id=\"review\">Review</h2><p>What could be better? Not a lot. I think Wes (the instructor) could have thought harder about prompting learners to pause the video and trying solo for themselves. He does once or twice somewhere in the first 10 lessons. But didn't do it after, which meant I tended to follow along waiting for the prompt. There are a couple of mistakes here and there - one was on a resource Wes was hosting returning a <code>404</code> for one of the lessons, but after emailing Wes fixed it about a week later. I thought some items could have been explained better e.g. CSS query selectors and the different syntax to use.</p><h2 id=\"01-javascript-drum-kit\">01 - JavaScript Drum Kit</h2><ul><li>There's an <code>&lt;audio&gt;</code> div <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element/audio\">tag for embedded audio.</a></li><li>Adding event listeners is easy, in this example :</li></ul><pre><code class=\"language-js\">window.addEventListener('keydown', function (e) {\n    const audio = document.querySelector(`audio[data-key=\"${e.keyCode}\"]`);\n});\n</code></pre><ul><li><a href=\"https://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_data_attributes\">HTML data attributes</a> - store additional info on html elements using <code>data-</code> prefix:</li><li>The data of the elements can then be accessed using <code>dataset</code>:</li></ul><pre><code class=\"language-html\">&lt;input id=\"spacing\"... data-sizing=\"px\"&gt;\n</code></pre><pre><code class=\"language-js\">function handleUpdate() {\n  const suffix = this.dataset;\n}\n\ninputs.forEach(node =&gt; node.addEventListener('change', handleUpdate));\n</code></pre><h2 id=\"02-clock\">02 - Clock</h2><ul><li>Select any element with <code>document.querySelector('.second-hand');</code>. Where <code>.second-hand</code> is a class name. Known as <a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors\">CSS selectors</a>.</li><li>Use <code>setInterval(yourfn, 1000)</code> to run a function on a repeated time interval.</li></ul><h2 id=\"03-css-variables\">03 - CSS Variables</h2><ul><li>Pattern, get all elements you want to manipulate, create a function defining that change, create trigger for that function.</li></ul><pre><code class=\"language-js\">const inputs = document.querySelectorAll('.controls input');\n\nfunction handleUpdate() {\n  const suffix = this.dataset.sizing || '';\n  document.documentElement.style.setProperty(`--${this.name}`, this.value+suffix);\n}\n\ninputs.forEach(node =&gt; node.addEventListener('change', handleUpdate));\ninputs.forEach(node =&gt; node.addEventListener('mousemove', handleUpdate));\n</code></pre><h2 id=\"04-array-cardio-day-1\">04 - array cardio day 1</h2><p><em>Array methods map(), reduce(), reduce()  and  filter() are introduced.</em></p><ul><li>Log a table to the console, i.e. a list of objects</li></ul><pre><code class=\"language-js\">console.table(sorted);\n</code></pre><h1 id=\"05-flex-panels\">05 - Flex panels</h1><ul><li>Light touch on some flex panels (pretty foreign though, without much intro)</li><li>Pattern of applying some transforms and transitions css using js</li></ul><pre><code class=\"language-js\">// grab relevant elements\nvar panels = document.querySelectorAll('.panel');\n\n// function to toggle\nfunction toggleOpen(e) {\n  this.classList.toggle('open');\n}\n\n// event listener to toggle\npanels.forEach(x =&gt; x.addEventListener('click', toggleOpen));\n</code></pre><h2 id=\"06-type-ahead\">06 - Type Ahead</h2><p>Implement a search box against json data.</p><ul><li>Get data via a promise:</li></ul><pre><code class=\"language-js\">const cities = [];\nfetch(endpoint)\n  .then(blob =&gt; blob.json())\n  .then(data =&gt; cities.push(...data))\n</code></pre><ul><li>Regex can be used like this:</li></ul><pre><code class=\"language-js\">// (g)lobal, (i)nsensitive\nconst regex = new RegExp(wordToMatch, 'gi');\nreturn place.city.match(regex)\n</code></pre><h2 id=\"07-array-cardio-day-2\">07 - array cardio day 2</h2><p><em>Some extra array methods</em></p><ul><li><code>Array.prototype.some()</code> - checks if at least one element meets a condition is met. Returns Boolean.</li><li><code>Array.prototype.every()</code> -  - checks if all elements meet a condition is met. Returns Boolean.</li><li><code>Array.prototype.find()</code> - returns element from array that meets the specified condition.</li><li><code>Array.prototype.findIndex()</code> - like <code>find()</code> but returns the index.</li><li><code>Array.prototype.splice()</code> - delete the element at a given index.</li></ul><h2 id=\"08-fun-with-html5-canvas\">08 - Fun with HTML5 Canvas</h2><p><em>Could have provided more context on the why of canvas vs alternatives of divs, could have been more structured as well. What is canvas? What are its features? etc</em></p><ul><li>Canvas has a context, where you can update it's properties:</li></ul><pre><code class=\"language-html\">&lt;body&gt;\n&lt;canvas id=\"draw\" width=\"800\" height=\"800\"&gt;&lt;/canvas&gt;\n&lt;script&gt;\nconst canvas = document.querySelector(\"#draw\");\nconst ctx = canvas.getContext('2d');\ncanvas.width = window.innerWidth;\ncanvas.height = window.innerHeight;\nctx.strokeStyle = '#BBDA55';\n</code></pre><ul><li>Instead of <code>rbg()</code> or hex colors you can set color using hsl(): <code>style=\"color: hsl(200, 100%, 50%)\"</code></li></ul><h2 id=\"09-dev-tools-domination\">09 - Dev Tools Domination</h2><p><em>Depending on how familiar one is with the dev tools, some very good tips to be had</em></p><ul><li>Various methods of <code>console</code>, e.g. <code>console.warn()</code> <code>console.info()</code></li><li>Break on a paticular element action, inspect the element, right click on element in dev tools and add a 'break on' -&gt; '<em>choose an option</em>'</li><li><code>console.groupCollapsed()</code> group a bunch console logs and display them by default as collapsed.</li></ul><pre><code class=\"language-js\">console.groupCollapsed('my group')\nconsole.log('part1 collapsed')\nconsole.log('part2 collapsed')\nconsole.groupEnd('end of my group')\n</code></pre><ul><li><code>console.count</code> will print the contents and also the count of how many times that content has been logged</li><li><code>console.time('X operation took:)</code> put your code you want to time and end it with <code>console.timeEnd(X operation took:)</code></li><li><em>Covered in earlier episodes but very handy</em> display in the console in tabular form an array of objects <code>console.table(myArrayOfObjects)</code></li></ul><h2 id=\"10-hold-shift-and-check-checkboxes\">10 - Hold Shift and Check Checkboxes</h2><p><em>I found this one hard - and then returned to it after a 7mth break 😱</em></p><ul><li>No new techniques in this lesson, but a harder application of the pattern of, select DOM elements, define callback function</li></ul><pre><code class=\"language-js\">  // select the divs\n  const checkboxes = document.querySelectorAll('.inbox input[type=\"checkbox\"]');\n\n  function handleCheck(e) {\n    // check if shiftkey pressed AND box is checked\n    let inBetween = false;\n    if (e.shiftKey &amp;&amp; this.checked) {\n      // loop through each checkbox\n      checkboxes.forEach(checkbox =&gt; {\n        // these are the outer bounds of the selection\n        if (checkbox === this || checkbox === lastChecked) {\n          inBetween = !inBetween;\n        }\n        // whilst in between is true - check those boxes\n        if (inBetween === true) {\n          checkbox.checked = true;\n        }\n      });\n    }\n    lastChecked = this;\n  }\n\n  // add event listener to handle clicks\n  checkboxes.forEach(checkbox =&gt; checkbox.addEventListener('click', handleCheck) );\n</code></pre><h2 id=\"11-custom-video-player\">11 - Custom Video Player</h2><ul><li>I found this easier to fly solo on but also a good few hours of work, you get a video in a page, but the buttons to play/pause, skip etc don't work. The first thing I needed was some documentation on what events I could use on the <code>video</code>. W3 Schools was the first link, but not too helpful. Mozilla to the rescue with a nice list of <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video#Events\">video events</a>.</li><li>You'll also need to know what the <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video#Attributes\">video attributes are</a>.</li></ul><h2 id=\"12-key-sequence-detection\">12 - Key Sequence Detection</h2><ul><li>This was a half hour one, much better after day 11. The aim is to do some action on the detection of some sequence of keystrokes, which TIL is called a <a href=\"https://en.wikipedia.org/wiki/Konami_Code\">Konami_Code</a>.</li><li>Solution involved adding an eventlistener to the event <code>keydown</code>, push the keys in an array, and checking for a sequence matching the konami code.</li><li>And another amazing tidbit</li></ul><pre><code class=\"language-js\">// Add this script tag\n&lt;script type=\"text/javascript\" src=\"https://www.cornify.com/js/cornify.js\"&gt;&lt;/script&gt;\n...\n// Then call this function to put corny shit on your webpage\ncornify_add()\n</code></pre><h2 id=\"13-slide-in-on-scroll\">13 - Slide in on Scroll</h2><p><em>Tip</em> - At this point I'm getting a little tired of the <code>CMD+S</code> to save in VS Code, <code>CMD+TAB</code> to switch applications to the browser and then <code>CMD+SHIFT+R</code> to hard refresh. The <a href=\"https://github.com/ritwickdey/vscode-live-server\">Live Server extension</a> solves that problem - as soon as you save, the page reloads in the browser you launch with it.</p><ul><li>For the interest of keeping JS30 plain js Wes copies in the <a href=\"https://lodash.com/docs/4.17.15#debounce\"><code>debounce</code> function</a> from the <a href=\"https://lodash.com/\"><code>lodash</code> library</a>. I personally think importing this library and a brief explainer on why lodash is such a widely popular library would have been good.</li><li>Calculating how far someone has scrolled on a web page:</li><li><code>window.scrollY</code> - how far you have scrolled measured at the top of the window <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Window/scrollY\">(mdn ref)</a>.</li><li><code>window.innerHeight</code> - the height of the window, measured using the viewport, i.e. what is visible <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Window/innerHeight\">(mdn ref)</a>.</li><li><code>HTMLElement.offsetTop</code> - the distance of the element relative to the offsetParent node, in this tutorial that is the body <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLelement/offsetParent\">(mdn ref)</a>.</li><li><code>Element.classList</code> - '... class attributes of the element. This can then be used to manipulate the class list.' <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Element/classList\">(mdn ref)</a>.</li><li>crux of the lesson is utilising the above properties to work out where on a page the user has currently scrolled and to conditionally apply CSS already written for you.</li></ul><pre><code class=\"language-js\">if (conditions) {\n  image.classList.add('active');\n} else {\n  image.classList.remove('active');\n}\n</code></pre><p>Simplified CSS</p><pre><code class=\"language-css\">/* take half a second to transition,  initial set to invisible*/\n.slide-in {\n  opacity: 0;\n  transition: all .5s;\n}\n\n/* on active,  set to invisible*/\n.slide-in.active {\n  opacity: 1;\n}\n</code></pre><h2 id=\"14-javascript-references-vs-copying\">14 - JavaScript References VS Copying</h2><ul><li>This covers some js idiosyncrasies on reassignment and copying variables, arrays and objects.</li><li>Starting off with variables, variables can be reassigned. <code>const</code> is not covered which behaves differently. <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Errors/Invalid_const_assignment\">mdn ref</a></li></ul><pre><code class=\"language-js\">// const\nconst age = 100;\nconst age2 = age;\nage2 = \"new\"; // 🛑 TypeError: invalid assignment to const\n\n// both var + let behave the same\nlet num = 100;\nlet num2 = num;\nnum2 = 200;\nconsole.log(num, num2) // 👍 100 200 \n</code></pre><ul><li>Arrays are different:</li></ul><pre><code class=\"language-js\">// Make a copy of an array\nconst players = ['Wes', 'Sarah', 'Ryan', 'Poppy'];\nconst team = players;\nteam[1] = \"new\";\nconsole.log(players, team)\n// [ \"Wes\", \"new\", \"Ryan\", \"Poppy\" ]\n// [ \"Wes\", \"new\", \"Ryan\", \"Poppy\" ]\n// 🛑 changes both `team` and `players` because `team` is just a pointer to `players`\n\n// solution\nconst players = ['Wes', 'Sarah', 'Ryan', 'Poppy'];\nconst team = players.slice();\nteam[1] = \"new\";\nconsole.log(players, team)\n// [ \"Wes\", \"Sarah\", \"Ryan\", \"Poppy\" ]\n// [ \"Wes\", \"new\", \"Ryan\", \"Poppy\" ]\n// 👍 could also do `const team = [...players];`\n</code></pre><ul><li>Objects also need a special method to copy, however, the following will only do one level deep of the object.</li></ul><pre><code class=\"language-js\">// with Objects\nconst person = {\n  name: 'Wes Bos',\n  age: 80\n};\n\n// attempt to make a copy:\nconst captain = person;\ncaptain.number = 99;\nconsole.log(person)\n// Object { name: \"Wes Bos\", age: 80, number: 99 }\n// 🛑 this has modified the original\n\n// solution\nconst captain2 = Object.assign({}, person)\ncaptain2.number = 100;\nconsole.log(person)\n// Object { name: \"Wes Bos\", age: 80, number: 99 } 👍\n</code></pre><ul><li>lodash has a <a href=\"https://lodash.com/docs/4.17.15#cloneDeep\">cloneDeep method</a> to do a full object copy</li></ul><h2 id=\"15-local-storage-and-event-delegation\">15 - Local Storage and Event Delegation</h2><p>N.b - this one is big, video is over 30mins.</p><ul><li>This covers topics that seem like a precursor to react, storing state and using parent elements to instruct child elements.</li><li>When working with forms and wanting to perform actions upon form submission, the appropriate event to listen for is <code>submit</code>. By default the submit form event will reload the page. To prevent this use <code>e.preventDefault()</code> in the handler.</li><li>Persist data on the client by using local storage, a key value store, where your value needs to be a string</li></ul><pre><code class=\"language-js\">// args are key value\nlocalStorage.setItem('items', JSON.stringify(myObject))\n</code></pre><ul><li>If your data is not what you expect and you see <code>[object Object]</code> instead - you most likely have tried to convert an object to a string <code>Object().toString()</code>. There are <a href=\"https://stackoverflow.com/a/25419538/3691003\">other variations of this behaviour</a> - because everything in javascript is an object.</li></ul><h2 id=\"16-css-text-shadow-mouse-move-effect\">16 - CSS Text Shadow Mouse Move Effect</h2><ul><li>Wes takes a weird approach of grabbing the hero elements offset x and y values and the event's offset x and y values. When the mousemove event occurs over another element on the page the x and  y values are then calculated from <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/MouseEvent/offsetX\"><em>the the padding edge of the target node</em></a>. A much simpler approach is taking the clientX and clientY values. Wes approach:</li></ul><pre><code class=\"language-js\">const { offsetWidth: width, offsetHeight: height} = hero\nlet { offsetX: x, offsetY: y } = e;\n\n// uses this if statement to handle when the target is the whole screen, to then add on the screen offset\nif (this !== e.target) {\n  x = x + e.target.offsetLeft;\n  y = y + e.target.offsetTop;\n}\n</code></pre><p>Much easier to use the following:</p><pre><code class=\"language-js\">let { clientX: x, clientY: y} = e;\n</code></pre><ul><li>A graphic effect is applied using the following</li></ul><pre><code class=\"language-js\">const walk = 100 //px\nconst xWalk = (x / width * walk) - (walk / 2);\nconst yWalk = (y / height * walk) - (walk / 2);\n\ntext.style.textShadow = `${xWalk}px ${yWalk}px 0 rgba(255,0,255,0.7)`\n</code></pre><h2 id=\"17-sorting-band-names-without-articles\">17 - Sorting Band Names without articles</h2><p>N.b. An 'article', being one of the grammatical articles. In this coding example, The definite article 'the' or the indefinite articles 'an' or 'a'.</p><ul><li>Wes takes a really nice and succinct approach here utilising the <a>Array.prototype.sort()</a>. However I prefer the mdn docs approach which is a little more verbose:</li></ul><pre><code class=\"language-js\">let sorted = bandsArray.sort((a,b) =&gt; {\n  if (strip(a) &gt; strip(b)) {\n    return 1\n  }\n  if (strip(a) &lt; strip(b)) {\n    return -1\n  }\n  return 0\n})\n</code></pre><ul><li>To append this to a div, e.g. <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element/ul\">the unordered list <code>&lt;ul&gt;</code></a>:</li></ul><pre><code class=\"language-js\">document.querySelector('#ulDivId').innerHTML =\n  sortedArray.map(item =&gt;\n    `&lt;li&gt;${item}&lt;/li&gt;`\n  ).join('');\n</code></pre><h2 id=\"18-tally-string-times-with-reduce\">18 - Tally String Times with Reduce</h2><p>N.b. simple if you have some knowledge of map <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/Reduce\">reduce functions</a>.<br>My answer pretty similar to Wes's except instead of using a forEach to append the values into an array, he used a much nicer <code>const timeNodes = Array.from(document.querySelectorAll('[data-time]'));</code>. My answer:</p><pre><code class=\"language-js\">const videos = document.querySelectorAll('li')\nconst times = [];\n\nvideos.forEach(vid =&gt; {\n  times.push(vid.dataset.time)\n});\n\nconst totalTime = times.map(x =&gt; {\n  const [min, sec] = x.split(':');\n  return Number(min) * 60 + Number(sec);\n}).reduce((acc, val) =&gt; {\n  return acc + val;\n})\n\nconst mins = Math.trunc(totalTime / 60);\nconst secs = totalTime % 60;\nconsole.log(mins, ':', secs)\n// 298 : 58 - I should handle hours but meh 🤷‍\n</code></pre><h2 id=\"19-unreal-webcam-fun\">19 - Unreal Webcam Fun</h2><p><em>N.b. This was a slower start with a few issues to iron out cover in the first 3 bullet points...</em></p><ul><li>The files so far have been read locally into the browser, such that in the address bar you'll find the file path <code>file://</code> instead of <code>https://</code>. However the use of the webcam requires a server due to accessing the computer's camera and the security requirement that the website is a secure origin, <code>https</code> or <code>localhost</code>. Therefore we need to spin up a local server. If developing with <a href=\"#13-slide-in-on-scroll\">VSCode with the Live Server extension</a> this has already been done for you. I've used Python's <code><code>http.server</code></code> (used to be called <code><code>SimpleHTTPServer</code></code>). <a href=\"https://www.browsersync.io/\">Browsersync</a>  seems like the JavaScript equilavent.</li><li>I got some intitial <code>404</code> errors where the html template pointed to a file that no longer exists. I fixed this by downloading a camera sound locally.</li></ul><pre><code class=\"language-html\">  &lt;!-- 404 --&gt;\n  &lt;audio class=\"snap\" src=\"https://wesbos.com/demos/photobooth/snap.mp3\" hidden&gt;&lt;/audio&gt;\n  &lt;!-- replace with a local sound file --&gt;\n  &lt;audio class=\"snap\" src=\"camera_sound.mp3\" hidden&gt;&lt;/audio&gt;</code></pre><ul><li>The video tutorial for this instructs you to use <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/URL/createObjectURL\"><code>createObjectURL</code> which has now been deprecated</a>. I wasted some time here, checking I had done things correctly, searching for the error until I saw a note in the solution on this to use <code><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/srcObject\">srcObject</a></code>.</li><li>To access a user's web camera use <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator/mediaDevices/getUserMedia\"><code><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia\">navigator.mediaDevices.getUserMedia()</a></code></a>. This returns a <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise\"><code>Promise</code></a> that resolves to a <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/MediaStream\"><code>MediaStream</code></a> object.</li></ul><pre><code class=\"language-js\">function getVideo() {\n  navigator.mediaDevices.getUserMedia({\n    video: true,\n    audio: false\n  }).then(localMediaStream =&gt; {\n    video.srcObject = localMediaStream;\n    video.play();\n  })\n  .catch(\n    err =&gt; {\n      console.error(\"Whoops Error: \", err);\n    });\n}</code></pre><ul><li><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator\">The Navigator interface</a> enables you to query some interesting properties of the user e.g. <code><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator/geolocation\">geolocation</a></code>, <code><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator/connection\">connection</a></code>  and <code><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Navigator/webdriver\">webdriver</a></code>.</li><li>If you want to pause the execution and enter the debugger at a particular point of your code, just write <code>debugger</code>.</li></ul><pre><code>console.log(pixels);\ndebugger</code></pre><h2 id=\"20-native-speech-recognition\">20 - Native Speech Recognition</h2><p>N.b. This was definitely a frustrating first 5mins of the tutorial, where if you're following along you don't get anything like what Wes is getting. This is because he is demonstrating the finished app on the left half of the screen whilst coding from scratch the app. It's not until 5mins when switches that he realises that it's not working.</p><ul><li>This only works on Chrome, not Firefox.</li></ul><pre><code class=\"language-js\">const recognition = new SpeechRecognition();\n  recognition.interimResults = true;\n  recognition.lang = 'en-US';\n\n  let p = document.createElement('p');\n  const words = document.querySelector('.words')\n  words.appendChild(p)\n\n  recognition.addEventListener('result', e =&gt; {\n    const transcript = Array.from(e.results)\n      .map(result =&gt; result[0])\n      .map(result =&gt; result.transcript)\n      .join('')\n\n    p.textContent = transcript;\n    // create a new paragraph if the speaking has finished\n    if (e.results[0].isFinal) {\n      p = document.createElement('p')\n      words.appendChild(p);\n    }\n    console.log(transcript); // the results\n  })\n\n  recognition.addEventListener('end', recognition.start)\n  recognition.start()\n</code></pre><h2 id=\"21-geolocation-based-speedometer-and-compass\">21 - Geolocation based Speedometer and Compass</h2><ul><li>This requires using Xcode to simulate someone using there phone whilst moving around.</li></ul><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/Screen-Shot-2020-05-09-at-20.19.21.png\" class=\"kg-image\" alt></figure><ul><li>Once the simulator boots you'll have an iphone on screen. Then turn on a 'City Run'.</li></ul><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/Screen-Shot-2020-05-09-at-20.20.36.png\" class=\"kg-image\" alt></figure><ul><li>Then open Safari to connect the dev tools to the simulator iphone</li></ul><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/Screen-Shot-2020-05-09-at-20.35.50.png\" class=\"kg-image\" alt></figure><ul><li>And to access the geo data:</li></ul><pre><code class=\"language-js\">    // the request to use the geolocation of the client will ask for permissions from user\n    navigator.geolocation.watchPosition((data) =&gt; {\n      console.log(data) // see the data object\n\n      // update dom elements with the data\n      speed.textContent = data.coords.speed;\n      arrow.style.transform = `rotate(${data.coords.heading}deg)`\n    }, (err) =&gt; {\n      // if they reject the permissions\n      console.log(err)\n      alert(\"you privacy nut\")\n    }</code></pre><h2 id=\"22-follow-along-links\">22 - Follow Along Links</h2><p><em>N.b. lesson applies an element that hovers over the link the mouse is currently positioned over.</em></p><ul><li>Method: Create a 'highlight' DOM element, and transition it based on the <code>mouseenter</code> event on any <code>&lt;a&gt;</code> divs.</li><li>Create event listener</li></ul><pre><code class=\"language-js\">const triggers = document.querySelectorAll('a');\ntriggers.forEach(a =&gt; {\n    a.addEventListener('mouseenter', addHighlight)\n}</code></pre><ul><li>To get the coodinates and size of the current <code>a</code> div, use <code><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Element/getBoundingClientRect\">getBoundingClientRect()</a></code> in the callback.</li></ul><pre><code class=\"language-js\">function addHighlight(e) {\n    const domDimension = this.getBoundingClientRect();\n}</code></pre><ul><li><code><a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/transform-function/translate\">translate(Xpx, Ypx)</a></code> - CSS function that repositions an element. Use this in the call back to transition the highlight element. Wes uses the <code>top</code> and <code>left</code> properties, but X and Y works just the same and makes more sense to me.</li></ul><pre><code class=\"language-js\">const highlight = document.createElement('span');\nhighlight.classList.add('highlight');\ndocument.body.appendChild(highlight);\n\nfunction addHighlight(e) {\n    ...\n    highlight.style.width = `${domDimension.width}px`\n    highlight.style.height = `${domDimension.height}px`\n    highlight.style.transform = `translate(${domDimension.x}px,${domDimension.y}px)`\n}</code></pre><h2 id=\"23-speech-synthesis\">23 - Speech Synthesis</h2><p>N.b. - Firefox wasn't returning the voice objects from <code>speechSynthesis.getVoices()</code> at page load - switched to Chrome for this one.</p><ul><li>Use the experimental <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesis\">SpeechSynthesis API</a>.</li><li>When filtering an array based on a singular user selection, I naturally wanted to <code><a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/filter\">filter()</a></code> however this returns an array which you will have to select an element from. However, <code><a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/find\">find()</a></code>will return the first element that meets the match critera.</li></ul><pre><code class=\"language-js\">msg.voice = voices.filter(v =&gt; v.name === this.value)[0]\n// vs find\nmsg.voice = voices.find(v =&gt; v.name === this.value)\n</code></pre><h2 id=\"24-sticky-nav\">24 - Sticky Nav</h2><p><em>Fix a navbar to the top as you scroll down.</em></p><ul><li>Add an event listenr on the navbar and for the callback place a control statement to detect when the <em>fixing</em> of the nav should apply.</li></ul><pre><code class=\"language-js\">const navbar = document.querySelector('#main');\n// top of navbar location\nconst navHeight = navbar.offsetTop;\n\nfunction moveNavBar(e) {\n  const scrollAmt = window.scrollY;\n\n  if (scrollAmt &gt;= navHeight) {\n    document.body.classList.add('fixed-nav');\n    document.body.style.paddingTop = `${navbar.offsetHeight}px`\n\n  } else {\n    document.body.style.paddingTop = 0\n    document.body.classList.remove('fixed-nav');\n  }\n}\n\ndocument.addEventListener('scroll', _.debounce(moveNavBar, 10));</code></pre><ul><li>To find the hight of a DOM element there looks to be to properties you could use. Here's the <a href=\"https://stackoverflow.com/a/4106585/3691003\">difference between clientHeight vs offsetHeight,</a> go with offsetHeight.</li></ul><h2 id=\"25-event-capture-propagation-bubbling-and-once\">25 - Event Capture, Propagation, Bubbling and Once</h2><p><em>This is a very quick practical on the concept of bubbling up and propagation. Requires some extra research if new to the concept.</em></p><ul><li><a href=\"https://www.sitepoint.com/event-bubbling-javascript/\">Event propagation</a> - is the blanket term for both event bubbling and event capturing.</li><li>If you have 3 divs nested and place an event listener on those divs. Then load the page and click on the inner most div and log which div has been clicked on. The browser will show all divs from inner most to outer most <code>&lt;body&gt;</code> as having been clicked on - this is <em>bubbling up.</em></li><li>Behind the scenes the browser is first capturing the event by 'trickling down' the DOM hierarchy until the target on which the event occurred i.e. the click, is reached.</li><li>To make use of the third argument to an event listener:</li></ul><pre><code class=\"language-js\">divs.forEach(div =&gt; {\n  div.addEventListener('click', logText, {\n    capture: true, // 'trickle' down events\n    once: true // removeEventLister - unbind once event occurs once\n  })\n});</code></pre><h2 id=\"26-stripe-follow-along-dropdown\">26 - Stripe Follow Along Dropdown</h2><p><em>Extension of 22.</em></p><ul><li>I was trying to access the child nodes of <code>this</code> within the context of a callback by traversing the tree e.g. <code>this.children.item(0)</code>. But of course, you can just <code>this.querySelector</code> to query the tree of <code>this</code>.</li><li>The crux of the logic relied on adding and removing CSS classes. The pared down version:</li></ul><pre><code class=\"language-js\">  const triggers = document.querySelectorAll('.cool &gt; li')\n  const background = document.querySelector('.dropdownBackground')\n\n  function handleEnter() {\n    background.classList.add('open')\n    this.classList.add('trigger-enter');\n    this.classList.add('trigger-enter-active');\n\n    // don't forget you can querySelector on `this`\n    const dropdown = this.querySelector('.dropdown');\n    const dropdownDim = dropdown.getBoundingClientRect();\n    background.style.setProperty('width', `${dropdownDim.width}px`)\n  }\n\n  function handleLeave(params) {\n    this.classList.remove('trigger-enter', 'trigger-enter-active')\n  }\n\n  triggers.forEach(trigger =&gt; trigger.addEventListener('mouseenter', handleEnter));\n  triggers.forEach(trigger =&gt; trigger.addEventListener('mouseleave', handleLeave));\n</code></pre><h2 id=\"27-click-and-drag-to-scroll\">27 - Click and Drag to Scroll</h2><ul><li>Lesson centres on making use of 4 mouse events <code>mousedown</code>, <code>mouseleave</code>, <code>mouseup</code> and <code>mousemove</code> to build the controls around clicking and dragging.</li><li>As flexible as you think <code>let</code> is, you will encounter errors if you try to redeclare a variable created with <code>let</code>.</li></ul><pre><code class=\"language-js\">// 🛑 errors\nlet yourVar = true;\nlet yourVar = false;\n\n// ✅ happy days\nlet yourVar = true;\nyourVar = false;</code></pre><ul><li>You can <code>console.log</code> the var name and value at the same time by wrapping it in an object:</li></ul><pre><code class=\"language-js\">const yourVarName = 10;\n// slow way\nconsole.log('yourVarName', yourVarName);\n// fast way\nconsole.log({yourVarName});</code></pre><ul><li><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/MouseEvent/pageX\">MouseEvent.pageX</a> - \"returns the X (horizontal) coordinate (in pixels) at which the mouse was clicked, relative to the left edge of the entire document. This includes any portion of the document not currently visible.\"</li><li><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetLeft\">HTMLElement.offsetLeft</a> - \"The <strong><code>HTMLElement.offsetLeft</code></strong> read-only property returns the number of pixels that the <em>upper left corner</em> of the current element is offset to the left within the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetParent\"><code>HTMLElement.offsetParent</code></a> node.\"</li></ul><h2 id=\"28-video-speed-controller-ui\">28 - Video Speed Controller UI</h2><p><em>This is a small lesson at ~9mins and doesn't introduce any new content.  Mostly a simpler version of Lesson 11.</em></p><h2 id=\"29-countdown-clock\">29 - Countdown Clock</h2><p><em>Not a lot new in this one - but it is a fun use of the skills learnt to date in making a countdown clock.</em></p><ul><li><code>setInteval</code> will continue to run. So, if using it within a function that gets called multiple times, you may need to have a <code>clearInterval()</code> call to clear any existing process.</li><li>If an html element has a name attribute, it can be accessed directly from the document.</li></ul><pre><code class=\"language-html\">&lt;form name=\"customForm\" id=\"custom\"&gt;\n    &lt;input type=\"text\" name=\"minutes\" placeholder=\"Enter Minutes\"&gt;\n&lt;/form&gt;</code></pre><pre><code class=\"language-js\">document.customForm.addEventListener('submit', function(e){...})\n// Nested elements can be accessed too\ndocument.customForm.minutes.addEventListener('onhover', function(e){...})</code></pre><h2 id=\"30-whack-a-mole-game\">30 - Whack A Mole Game</h2><p><em>As the title suggests this is a bit of a fun simple game that is using the concepts learnt in previous lessons - a good way to end the course.</em></p><ul><li>On event's <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Event/isTrusted\">there is a <code>isTrusted</code> boolean property</a> that can be utilised to ensure the action on the web page is user generated. </li></ul><h2 id=\"addendum\">Addendum</h2><ul><li>Review - For someone with some programming experience and understanding who wants to focus on improving JavaScript skills, working with the APIs and developing with the browser this is a fantastic course. A couple of lessons are out-of-date however the finished solutions available in the repo are mostly up-to-date. I really like the honesty in the mistakes and even debugging his lesson code on video - it gives a much more honest representation of what programming is like. If a JavaScript jedi like Wes makes mistakes and spends time debugging so to will you.</li><li>TODO write summary and find good doc on document.querySelector(THISBIT), by<br>- id<br>- class<br>- attribute e.g. <code>data-time</code></li></ul><pre><code class=\"language-js\">// &lt;li data-time=\"5:23\"&gt; Video 26&lt;/li&gt;\ndocument.querySelectorAll('[data-time]')\n</code></pre>","comment_id":"5eb32f72ee6c8e0001bda2fe","plaintext":"📚Course: Javascript30 [https://javascript30.com/]\n💰Price: Free\n⭐️Rating: 8.5/10\n\nReview\nWhat could be better? Not a lot. I think Wes (the instructor) could have thought\nharder about prompting learners to pause the video and trying solo for\nthemselves. He does once or twice somewhere in the first 10 lessons. But didn't\ndo it after, which meant I tended to follow along waiting for the prompt. There\nare a couple of mistakes here and there - one was on a resource Wes was hosting\nreturning a 404 for one of the lessons, but after emailing Wes fixed it about a\nweek later. I thought some items could have been explained better e.g. CSS query\nselectors and the different syntax to use.\n\n01 - JavaScript Drum Kit\n * There's an <audio> div tag for embedded audio.\n   [https://developer.mozilla.org/en-US/docs/Web/HTML/Element/audio]\n * Adding event listeners is easy, in this example :\n\nwindow.addEventListener('keydown', function (e) {\n    const audio = document.querySelector(`audio[data-key=\"${e.keyCode}\"]`);\n});\n\n\n * HTML data attributes\n   [https://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_data_attributes] \n   - store additional info on html elements using data- prefix:\n * The data of the elements can then be accessed using dataset:\n\n<input id=\"spacing\"... data-sizing=\"px\">\n\n\nfunction handleUpdate() {\n  const suffix = this.dataset;\n}\n\ninputs.forEach(node => node.addEventListener('change', handleUpdate));\n\n\n02 - Clock\n * Select any element with document.querySelector('.second-hand');. Where \n   .second-hand is a class name. Known as CSS selectors\n   [https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors].\n * Use setInterval(yourfn, 1000) to run a function on a repeated time interval.\n\n03 - CSS Variables\n * Pattern, get all elements you want to manipulate, create a function defining\n   that change, create trigger for that function.\n\nconst inputs = document.querySelectorAll('.controls input');\n\nfunction handleUpdate() {\n  const suffix = this.dataset.sizing || '';\n  document.documentElement.style.setProperty(`--${this.name}`, this.value+suffix);\n}\n\ninputs.forEach(node => node.addEventListener('change', handleUpdate));\ninputs.forEach(node => node.addEventListener('mousemove', handleUpdate));\n\n\n04 - array cardio day 1\nArray methods map(), reduce(), reduce()  and  filter() are introduced.\n\n * Log a table to the console, i.e. a list of objects\n\nconsole.table(sorted);\n\n\n05 - Flex panels\n * Light touch on some flex panels (pretty foreign though, without much intro)\n * Pattern of applying some transforms and transitions css using js\n\n// grab relevant elements\nvar panels = document.querySelectorAll('.panel');\n\n// function to toggle\nfunction toggleOpen(e) {\n  this.classList.toggle('open');\n}\n\n// event listener to toggle\npanels.forEach(x => x.addEventListener('click', toggleOpen));\n\n\n06 - Type Ahead\nImplement a search box against json data.\n\n * Get data via a promise:\n\nconst cities = [];\nfetch(endpoint)\n  .then(blob => blob.json())\n  .then(data => cities.push(...data))\n\n\n * Regex can be used like this:\n\n// (g)lobal, (i)nsensitive\nconst regex = new RegExp(wordToMatch, 'gi');\nreturn place.city.match(regex)\n\n\n07 - array cardio day 2\nSome extra array methods\n\n * Array.prototype.some() - checks if at least one element meets a condition is\n   met. Returns Boolean.\n * Array.prototype.every() -  - checks if all elements meet a condition is met.\n   Returns Boolean.\n * Array.prototype.find() - returns element from array that meets the specified\n   condition.\n * Array.prototype.findIndex() - like find() but returns the index.\n * Array.prototype.splice() - delete the element at a given index.\n\n08 - Fun with HTML5 Canvas\nCould have provided more context on the why of canvas vs alternatives of divs,\ncould have been more structured as well. What is canvas? What are its features?\netc\n\n * Canvas has a context, where you can update it's properties:\n\n<body>\n<canvas id=\"draw\" width=\"800\" height=\"800\"></canvas>\n<script>\nconst canvas = document.querySelector(\"#draw\");\nconst ctx = canvas.getContext('2d');\ncanvas.width = window.innerWidth;\ncanvas.height = window.innerHeight;\nctx.strokeStyle = '#BBDA55';\n\n\n * Instead of rbg() or hex colors you can set color using hsl(): style=\"color:\n   hsl(200, 100%, 50%)\"\n\n09 - Dev Tools Domination\nDepending on how familiar one is with the dev tools, some very good tips to be\nhad\n\n * Various methods of console, e.g. console.warn() console.info()\n * Break on a paticular element action, inspect the element, right click on\n   element in dev tools and add a 'break on' -> 'choose an option'\n * console.groupCollapsed() group a bunch console logs and display them by\n   default as collapsed.\n\nconsole.groupCollapsed('my group')\nconsole.log('part1 collapsed')\nconsole.log('part2 collapsed')\nconsole.groupEnd('end of my group')\n\n\n * console.count will print the contents and also the count of how many times\n   that content has been logged\n * console.time('X operation took:) put your code you want to time and end it\n   with console.timeEnd(X operation took:)\n * Covered in earlier episodes but very handy display in the console in tabular\n   form an array of objects console.table(myArrayOfObjects)\n\n10 - Hold Shift and Check Checkboxes\nI found this one hard - and then returned to it after a 7mth break 😱\n\n * No new techniques in this lesson, but a harder application of the pattern of,\n   select DOM elements, define callback function\n\n  // select the divs\n  const checkboxes = document.querySelectorAll('.inbox input[type=\"checkbox\"]');\n\n  function handleCheck(e) {\n    // check if shiftkey pressed AND box is checked\n    let inBetween = false;\n    if (e.shiftKey && this.checked) {\n      // loop through each checkbox\n      checkboxes.forEach(checkbox => {\n        // these are the outer bounds of the selection\n        if (checkbox === this || checkbox === lastChecked) {\n          inBetween = !inBetween;\n        }\n        // whilst in between is true - check those boxes\n        if (inBetween === true) {\n          checkbox.checked = true;\n        }\n      });\n    }\n    lastChecked = this;\n  }\n\n  // add event listener to handle clicks\n  checkboxes.forEach(checkbox => checkbox.addEventListener('click', handleCheck) );\n\n\n11 - Custom Video Player\n * I found this easier to fly solo on but also a good few hours of work, you get\n   a video in a page, but the buttons to play/pause, skip etc don't work. The\n   first thing I needed was some documentation on what events I could use on the \n   video. W3 Schools was the first link, but not too helpful. Mozilla to the\n   rescue with a nice list of video events\n   [https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video#Events].\n * You'll also need to know what the video attributes are\n   [https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video#Attributes].\n\n12 - Key Sequence Detection\n * This was a half hour one, much better after day 11. The aim is to do some\n   action on the detection of some sequence of keystrokes, which TIL is called a \n   Konami_Code [https://en.wikipedia.org/wiki/Konami_Code].\n * Solution involved adding an eventlistener to the event keydown, push the keys\n   in an array, and checking for a sequence matching the konami code.\n * And another amazing tidbit\n\n// Add this script tag\n<script type=\"text/javascript\" src=\"https://www.cornify.com/js/cornify.js\"></script>\n...\n// Then call this function to put corny shit on your webpage\ncornify_add()\n\n\n13 - Slide in on Scroll\nTip - At this point I'm getting a little tired of the CMD+S to save in VS Code, \nCMD+TAB to switch applications to the browser and then CMD+SHIFT+R to hard\nrefresh. The Live Server extension\n[https://github.com/ritwickdey/vscode-live-server] solves that problem - as soon\nas you save, the page reloads in the browser you launch with it.\n\n * For the interest of keeping JS30 plain js Wes copies in the debounce function\n   [https://lodash.com/docs/4.17.15#debounce] from the lodash library\n   [https://lodash.com/]. I personally think importing this library and a brief\n   explainer on why lodash is such a widely popular library would have been\n   good.\n * Calculating how far someone has scrolled on a web page:\n * window.scrollY - how far you have scrolled measured at the top of the window \n   (mdn ref) [https://developer.mozilla.org/en-US/docs/Web/API/Window/scrollY].\n * window.innerHeight - the height of the window, measured using the viewport,\n   i.e. what is visible (mdn ref)\n   [https://developer.mozilla.org/en-US/docs/Web/API/Window/innerHeight].\n * HTMLElement.offsetTop - the distance of the element relative to the\n   offsetParent node, in this tutorial that is the body (mdn ref)\n   [https://developer.mozilla.org/en-US/docs/Web/API/HTMLelement/offsetParent].\n * Element.classList - '... class attributes of the element. This can then be\n   used to manipulate the class list.' (mdn ref)\n   [https://developer.mozilla.org/en-US/docs/Web/API/Element/classList].\n * crux of the lesson is utilising the above properties to work out where on a\n   page the user has currently scrolled and to conditionally apply CSS already\n   written for you.\n\nif (conditions) {\n  image.classList.add('active');\n} else {\n  image.classList.remove('active');\n}\n\n\nSimplified CSS\n\n/* take half a second to transition,  initial set to invisible*/\n.slide-in {\n  opacity: 0;\n  transition: all .5s;\n}\n\n/* on active,  set to invisible*/\n.slide-in.active {\n  opacity: 1;\n}\n\n\n14 - JavaScript References VS Copying\n * This covers some js idiosyncrasies on reassignment and copying variables,\n   arrays and objects.\n * Starting off with variables, variables can be reassigned. const is not\n   covered which behaves differently. mdn ref\n   [https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Errors/Invalid_const_assignment]\n\n// const\nconst age = 100;\nconst age2 = age;\nage2 = \"new\"; // 🛑 TypeError: invalid assignment to const\n\n// both var + let behave the same\nlet num = 100;\nlet num2 = num;\nnum2 = 200;\nconsole.log(num, num2) // 👍 100 200 \n\n\n * Arrays are different:\n\n// Make a copy of an array\nconst players = ['Wes', 'Sarah', 'Ryan', 'Poppy'];\nconst team = players;\nteam[1] = \"new\";\nconsole.log(players, team)\n// [ \"Wes\", \"new\", \"Ryan\", \"Poppy\" ]\n// [ \"Wes\", \"new\", \"Ryan\", \"Poppy\" ]\n// 🛑 changes both `team` and `players` because `team` is just a pointer to `players`\n\n// solution\nconst players = ['Wes', 'Sarah', 'Ryan', 'Poppy'];\nconst team = players.slice();\nteam[1] = \"new\";\nconsole.log(players, team)\n// [ \"Wes\", \"Sarah\", \"Ryan\", \"Poppy\" ]\n// [ \"Wes\", \"new\", \"Ryan\", \"Poppy\" ]\n// 👍 could also do `const team = [...players];`\n\n\n * Objects also need a special method to copy, however, the following will only\n   do one level deep of the object.\n\n// with Objects\nconst person = {\n  name: 'Wes Bos',\n  age: 80\n};\n\n// attempt to make a copy:\nconst captain = person;\ncaptain.number = 99;\nconsole.log(person)\n// Object { name: \"Wes Bos\", age: 80, number: 99 }\n// 🛑 this has modified the original\n\n// solution\nconst captain2 = Object.assign({}, person)\ncaptain2.number = 100;\nconsole.log(person)\n// Object { name: \"Wes Bos\", age: 80, number: 99 } 👍\n\n\n * lodash has a cloneDeep method [https://lodash.com/docs/4.17.15#cloneDeep] to\n   do a full object copy\n\n15 - Local Storage and Event Delegation\nN.b - this one is big, video is over 30mins.\n\n * This covers topics that seem like a precursor to react, storing state and\n   using parent elements to instruct child elements.\n * When working with forms and wanting to perform actions upon form submission,\n   the appropriate event to listen for is submit. By default the submit form\n   event will reload the page. To prevent this use e.preventDefault() in the\n   handler.\n * Persist data on the client by using local storage, a key value store, where\n   your value needs to be a string\n\n// args are key value\nlocalStorage.setItem('items', JSON.stringify(myObject))\n\n\n * If your data is not what you expect and you see [object Object] instead - you\n   most likely have tried to convert an object to a string Object().toString().\n   There are other variations of this behaviour\n   [https://stackoverflow.com/a/25419538/3691003] - because everything in\n   javascript is an object.\n\n16 - CSS Text Shadow Mouse Move Effect\n * Wes takes a weird approach of grabbing the hero elements offset x and y\n   values and the event's offset x and y values. When the mousemove event occurs\n   over another element on the page the x and  y values are then calculated from \n   the the padding edge of the target node\n   [https://developer.mozilla.org/en-US/docs/Web/API/MouseEvent/offsetX]. A much\n   simpler approach is taking the clientX and clientY values. Wes approach:\n\nconst { offsetWidth: width, offsetHeight: height} = hero\nlet { offsetX: x, offsetY: y } = e;\n\n// uses this if statement to handle when the target is the whole screen, to then add on the screen offset\nif (this !== e.target) {\n  x = x + e.target.offsetLeft;\n  y = y + e.target.offsetTop;\n}\n\n\nMuch easier to use the following:\n\nlet { clientX: x, clientY: y} = e;\n\n\n * A graphic effect is applied using the following\n\nconst walk = 100 //px\nconst xWalk = (x / width * walk) - (walk / 2);\nconst yWalk = (y / height * walk) - (walk / 2);\n\ntext.style.textShadow = `${xWalk}px ${yWalk}px 0 rgba(255,0,255,0.7)`\n\n\n17 - Sorting Band Names without articles\nN.b. An 'article', being one of the grammatical articles. In this coding\nexample, The definite article 'the' or the indefinite articles 'an' or 'a'.\n\n * Wes takes a really nice and succinct approach here utilising the \n   Array.prototype.sort(). However I prefer the mdn docs approach which is a\n   little more verbose:\n\nlet sorted = bandsArray.sort((a,b) => {\n  if (strip(a) > strip(b)) {\n    return 1\n  }\n  if (strip(a) < strip(b)) {\n    return -1\n  }\n  return 0\n})\n\n\n * To append this to a div, e.g. the unordered list <ul>\n   [https://developer.mozilla.org/en-US/docs/Web/HTML/Element/ul]:\n\ndocument.querySelector('#ulDivId').innerHTML =\n  sortedArray.map(item =>\n    `<li>${item}</li>`\n  ).join('');\n\n\n18 - Tally String Times with Reduce\nN.b. simple if you have some knowledge of map reduce functions\n[https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/Reduce]\n.\nMy answer pretty similar to Wes's except instead of using a forEach to append\nthe values into an array, he used a much nicer const timeNodes =\nArray.from(document.querySelectorAll('[data-time]'));. My answer:\n\nconst videos = document.querySelectorAll('li')\nconst times = [];\n\nvideos.forEach(vid => {\n  times.push(vid.dataset.time)\n});\n\nconst totalTime = times.map(x => {\n  const [min, sec] = x.split(':');\n  return Number(min) * 60 + Number(sec);\n}).reduce((acc, val) => {\n  return acc + val;\n})\n\nconst mins = Math.trunc(totalTime / 60);\nconst secs = totalTime % 60;\nconsole.log(mins, ':', secs)\n// 298 : 58 - I should handle hours but meh 🤷‍\n\n\n19 - Unreal Webcam Fun\nN.b. This was a slower start with a few issues to iron out cover in the first 3\nbullet points...\n\n * The files so far have been read locally into the browser, such that in the\n   address bar you'll find the file path file:// instead of https://. However\n   the use of the webcam requires a server due to accessing the computer's\n   camera and the security requirement that the website is a secure origin, \n   https or localhost. Therefore we need to spin up a local server. If\n   developing with VSCode with the Live Server extension this has already been\n   done for you. I've used Python's http.server (used to be called \n   SimpleHTTPServer). Browsersync [https://www.browsersync.io/] seems like the\n   JavaScript equilavent.\n * I got some intitial 404 errors where the html template pointed to a file that\n   no longer exists. I fixed this by downloading a camera sound locally.\n\n  <!-- 404 -->\n  <audio class=\"snap\" src=\"https://wesbos.com/demos/photobooth/snap.mp3\" hidden></audio>\n  <!-- replace with a local sound file -->\n  <audio class=\"snap\" src=\"camera_sound.mp3\" hidden></audio>\n\n * The video tutorial for this instructs you to use createObjectURL which has\n   now been deprecated\n   [https://developer.mozilla.org/en-US/docs/Web/API/URL/createObjectURL]. I\n   wasted some time here, checking I had done things correctly, searching for\n   the error until I saw a note in the solution on this to use srcObject\n   [https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/srcObject]\n   .\n * To access a user's web camera use navigator.mediaDevices.getUserMedia()\n   [https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia]\n   [https://developer.mozilla.org/en-US/docs/Web/API/Navigator/mediaDevices/getUserMedia]\n   . This returns a Promise\n   [https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise] \n   that resolves to a MediaStream\n   [https://developer.mozilla.org/en-US/docs/Web/API/MediaStream] object.\n\nfunction getVideo() {\n  navigator.mediaDevices.getUserMedia({\n    video: true,\n    audio: false\n  }).then(localMediaStream => {\n    video.srcObject = localMediaStream;\n    video.play();\n  })\n  .catch(\n    err => {\n      console.error(\"Whoops Error: \", err);\n    });\n}\n\n * The Navigator interface\n   [https://developer.mozilla.org/en-US/docs/Web/API/Navigator] enables you to\n   query some interesting properties of the user e.g. geolocation\n   [https://developer.mozilla.org/en-US/docs/Web/API/Navigator/geolocation], \n   connection\n   [https://developer.mozilla.org/en-US/docs/Web/API/Navigator/connection] and \n   webdriver\n   [https://developer.mozilla.org/en-US/docs/Web/API/Navigator/webdriver].\n * If you want to pause the execution and enter the debugger at a particular\n   point of your code, just write debugger.\n\nconsole.log(pixels);\ndebugger\n\n20 - Native Speech Recognition\nN.b. This was definitely a frustrating first 5mins of the tutorial, where if\nyou're following along you don't get anything like what Wes is getting. This is\nbecause he is demonstrating the finished app on the left half of the screen\nwhilst coding from scratch the app. It's not until 5mins when switches that he\nrealises that it's not working.\n\n * This only works on Chrome, not Firefox.\n\nconst recognition = new SpeechRecognition();\n  recognition.interimResults = true;\n  recognition.lang = 'en-US';\n\n  let p = document.createElement('p');\n  const words = document.querySelector('.words')\n  words.appendChild(p)\n\n  recognition.addEventListener('result', e => {\n    const transcript = Array.from(e.results)\n      .map(result => result[0])\n      .map(result => result.transcript)\n      .join('')\n\n    p.textContent = transcript;\n    // create a new paragraph if the speaking has finished\n    if (e.results[0].isFinal) {\n      p = document.createElement('p')\n      words.appendChild(p);\n    }\n    console.log(transcript); // the results\n  })\n\n  recognition.addEventListener('end', recognition.start)\n  recognition.start()\n\n\n21 - Geolocation based Speedometer and Compass\n * This requires using Xcode to simulate someone using there phone whilst moving\n   around.\n\n * Once the simulator boots you'll have an iphone on screen. Then turn on a\n   'City Run'.\n\n * Then open Safari to connect the dev tools to the simulator iphone\n\n * And to access the geo data:\n\n    // the request to use the geolocation of the client will ask for permissions from user\n    navigator.geolocation.watchPosition((data) => {\n      console.log(data) // see the data object\n\n      // update dom elements with the data\n      speed.textContent = data.coords.speed;\n      arrow.style.transform = `rotate(${data.coords.heading}deg)`\n    }, (err) => {\n      // if they reject the permissions\n      console.log(err)\n      alert(\"you privacy nut\")\n    }\n\n22 - Follow Along Links\nN.b. lesson applies an element that hovers over the link the mouse is currently\npositioned over.\n\n * Method: Create a 'highlight' DOM element, and transition it based on the \n   mouseenter event on any <a> divs.\n * Create event listener\n\nconst triggers = document.querySelectorAll('a');\ntriggers.forEach(a => {\n    a.addEventListener('mouseenter', addHighlight)\n}\n\n * To get the coodinates and size of the current a div, use \n   getBoundingClientRect()\n   [https://developer.mozilla.org/en-US/docs/Web/API/Element/getBoundingClientRect] \n   in the callback.\n\nfunction addHighlight(e) {\n    const domDimension = this.getBoundingClientRect();\n}\n\n * translate(Xpx, Ypx)\n   [https://developer.mozilla.org/en-US/docs/Web/CSS/transform-function/translate] \n   - CSS function that repositions an element. Use this in the call back to\n   transition the highlight element. Wes uses the top and left properties, but X\n   and Y works just the same and makes more sense to me.\n\nconst highlight = document.createElement('span');\nhighlight.classList.add('highlight');\ndocument.body.appendChild(highlight);\n\nfunction addHighlight(e) {\n    ...\n    highlight.style.width = `${domDimension.width}px`\n    highlight.style.height = `${domDimension.height}px`\n    highlight.style.transform = `translate(${domDimension.x}px,${domDimension.y}px)`\n}\n\n23 - Speech Synthesis\nN.b. - Firefox wasn't returning the voice objects from \nspeechSynthesis.getVoices() at page load - switched to Chrome for this one.\n\n * Use the experimental SpeechSynthesis API\n   [https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesis].\n * When filtering an array based on a singular user selection, I naturally\n   wanted to filter()\n   [https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/filter] \n   however this returns an array which you will have to select an element from.\n   However, find()\n   [https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/find]\n   will return the first element that meets the match critera.\n\nmsg.voice = voices.filter(v => v.name === this.value)[0]\n// vs find\nmsg.voice = voices.find(v => v.name === this.value)\n\n\n24 - Sticky Nav\nFix a navbar to the top as you scroll down.\n\n * Add an event listenr on the navbar and for the callback place a control\n   statement to detect when the fixing of the nav should apply.\n\nconst navbar = document.querySelector('#main');\n// top of navbar location\nconst navHeight = navbar.offsetTop;\n\nfunction moveNavBar(e) {\n  const scrollAmt = window.scrollY;\n\n  if (scrollAmt >= navHeight) {\n    document.body.classList.add('fixed-nav');\n    document.body.style.paddingTop = `${navbar.offsetHeight}px`\n\n  } else {\n    document.body.style.paddingTop = 0\n    document.body.classList.remove('fixed-nav');\n  }\n}\n\ndocument.addEventListener('scroll', _.debounce(moveNavBar, 10));\n\n * To find the hight of a DOM element there looks to be to properties you could\n   use. Here's the difference between clientHeight vs offsetHeight,\n   [https://stackoverflow.com/a/4106585/3691003] go with offsetHeight.\n\n25 - Event Capture, Propagation, Bubbling and Once\nThis is a very quick practical on the concept of bubbling up and propagation.\nRequires some extra research if new to the concept.\n\n * Event propagation [https://www.sitepoint.com/event-bubbling-javascript/] - is\n   the blanket term for both event bubbling and event capturing.\n * If you have 3 divs nested and place an event listener on those divs. Then\n   load the page and click on the inner most div and log which div has been\n   clicked on. The browser will show all divs from inner most to outer most \n   <body> as having been clicked on - this is bubbling up.\n * Behind the scenes the browser is first capturing the event by 'trickling\n   down' the DOM hierarchy until the target on which the event occurred i.e. the\n   click, is reached.\n * To make use of the third argument to an event listener:\n\ndivs.forEach(div => {\n  div.addEventListener('click', logText, {\n    capture: true, // 'trickle' down events\n    once: true // removeEventLister - unbind once event occurs once\n  })\n});\n\n26 - Stripe Follow Along Dropdown\nExtension of 22.\n\n * I was trying to access the child nodes of this within the context of a\n   callback by traversing the tree e.g. this.children.item(0). But of course,\n   you can just this.querySelector to query the tree of this.\n * The crux of the logic relied on adding and removing CSS classes. The pared\n   down version:\n\n  const triggers = document.querySelectorAll('.cool > li')\n  const background = document.querySelector('.dropdownBackground')\n\n  function handleEnter() {\n    background.classList.add('open')\n    this.classList.add('trigger-enter');\n    this.classList.add('trigger-enter-active');\n\n    // don't forget you can querySelector on `this`\n    const dropdown = this.querySelector('.dropdown');\n    const dropdownDim = dropdown.getBoundingClientRect();\n    background.style.setProperty('width', `${dropdownDim.width}px`)\n  }\n\n  function handleLeave(params) {\n    this.classList.remove('trigger-enter', 'trigger-enter-active')\n  }\n\n  triggers.forEach(trigger => trigger.addEventListener('mouseenter', handleEnter));\n  triggers.forEach(trigger => trigger.addEventListener('mouseleave', handleLeave));\n\n\n27 - Click and Drag to Scroll\n * Lesson centres on making use of 4 mouse events mousedown, mouseleave, mouseup \n   and mousemove to build the controls around clicking and dragging.\n * As flexible as you think let is, you will encounter errors if you try to\n   redeclare a variable created with let.\n\n// 🛑 errors\nlet yourVar = true;\nlet yourVar = false;\n\n// ✅ happy days\nlet yourVar = true;\nyourVar = false;\n\n * You can console.log the var name and value at the same time by wrapping it in\n   an object:\n\nconst yourVarName = 10;\n// slow way\nconsole.log('yourVarName', yourVarName);\n// fast way\nconsole.log({yourVarName});\n\n * MouseEvent.pageX\n   [https://developer.mozilla.org/en-US/docs/Web/API/MouseEvent/pageX] -\n   \"returns the X (horizontal) coordinate (in pixels) at which the mouse was\n   clicked, relative to the left edge of the entire document. This includes any\n   portion of the document not currently visible.\"\n * HTMLElement.offsetLeft\n   [https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetLeft] -\n   \"The HTMLElement.offsetLeft read-only property returns the number of pixels\n   that the upper left corner of the current element is offset to the left\n   within the HTMLElement.offsetParent\n   [https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetParent] \n   node.\"\n\n28 - Video Speed Controller UI\nThis is a small lesson at ~9mins and doesn't introduce any new content.  Mostly\na simpler version of Lesson 11.\n\n29 - Countdown Clock\nNot a lot new in this one - but it is a fun use of the skills learnt to date in\nmaking a countdown clock.\n\n * setInteval will continue to run. So, if using it within a function that gets\n   called multiple times, you may need to have a clearInterval() call to clear\n   any existing process.\n * If an html element has a name attribute, it can be accessed directly from the\n   document.\n\n<form name=\"customForm\" id=\"custom\">\n    <input type=\"text\" name=\"minutes\" placeholder=\"Enter Minutes\">\n</form>\n\ndocument.customForm.addEventListener('submit', function(e){...})\n// Nested elements can be accessed too\ndocument.customForm.minutes.addEventListener('onhover', function(e){...})\n\n30 - Whack A Mole Game\nAs the title suggests this is a bit of a fun simple game that is using the\nconcepts learnt in previous lessons - a good way to end the course.\n\n * On event's there is a isTrusted boolean property\n   [https://developer.mozilla.org/en-US/docs/Web/API/Event/isTrusted] that can\n   be utilised to ensure the action on the web page is user generated. \n\nAddendum\n * Review - For someone with some programming experience and understanding who\n   wants to focus on improving JavaScript skills, working with the APIs and\n   developing with the browser this is a fantastic course. A couple of lessons\n   are out-of-date however the finished solutions available in the repo are\n   mostly up-to-date. I really like the honesty in the mistakes and even\n   debugging his lesson code on video - it gives a much more honest\n   representation of what programming is like. If a JavaScript jedi like Wes\n   makes mistakes and spends time debugging so to will you.\n * TODO write summary and find good doc on document.querySelector(THISBIT), by\n   - id\n   - class\n   - attribute e.g. data-time\n\n// <li data-time=\"5:23\"> Video 26</li>\ndocument.querySelectorAll('[data-time]')","feature_image":"/content/images/2020/05/mfo_park.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2020-05-06 21:43:14","created_by":"1","updated_at":"2020-06-03 21:31:11","updated_by":null,"published_at":"2020-05-06 22:13:40","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd1e","uuid":"181986f2-bc13-4145-bb17-b1620bf8e2c9","title":"Explain ELT ETL datawarehouse/lake","slug":"explain-elt-etl-datawarehouse-lake","mobiledoc":"{\"version\":\"0.3.2\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"https://docs.dataform.co/introduction/modern-data-stack\"]]],\"sections\":[[1,\"p\",[[0,[0],1,\"https://docs.dataform.co/introduction/modern-data-stack\"]]],[1,\"p\",[]]]}","html":"<p><a href=\"https://docs.dataform.co/introduction/modern-data-stack\">https://docs.dataform.co/introduction/modern-data-stack</a></p>","comment_id":"5eb5654eee6c8e0001bda348","plaintext":"https://docs.dataform.co/introduction/modern-data-stack","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2020-05-08 13:57:34","created_by":"1","updated_at":"2020-09-02 17:28:44","updated_by":null,"published_at":null,"published_by":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd1f","uuid":"1bbe955b-5b6d-4824-af46-e53edcae3371","title":"How to setup a Ubiquiti Access Point","slug":"ubiquiti-ap-setup","mobiledoc":"{\"version\":\"0.3.2\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"/content/images/2020/05/Screen-Shot-2020-05-17-at-22.02.26.png\"}],[\"embed\",{\"url\":\"https://tongfamily.com/2019/10/08/ubiquiti-unifi-controller-4-11-47-and-java-runtime-nightmare/\",\"html\":\"<blockquote class=\\\"wp-embedded-content\\\"><a href=\\\"https://tongfamily.com/2019/10/08/ubiquiti-unifi-controller-4-11-47-and-java-runtime-nightmare/\\\">Ubiquiti Unifi Controller 4.11.47 and Java Runtime nightmare</a></blockquote>\\n<script type='text/javascript'>\\n<!--//--><![CDATA[//><!--\\n\\t\\t/*! This file is auto-generated */\\n\\t\\t!function(d,l){\\\"use strict\\\";var e=!1,o=!1;if(l.querySelector)if(d.addEventListener)e=!0;if(d.wp=d.wp||{},!d.wp.receiveEmbedMessage)if(d.wp.receiveEmbedMessage=function(e){var t=e.data;if(t)if(t.secret||t.message||t.value)if(!/[^a-zA-Z0-9]/.test(t.secret)){var r,a,i,s,n,o=l.querySelectorAll('iframe[data-secret=\\\"'+t.secret+'\\\"]'),c=l.querySelectorAll('blockquote[data-secret=\\\"'+t.secret+'\\\"]');for(r=0;r<c.length;r++)c[r].style.display=\\\"none\\\";for(r=0;r<o.length;r++)if(a=o[r],e.source===a.contentWindow){if(a.removeAttribute(\\\"style\\\"),\\\"height\\\"===t.message){if(1e3<(i=parseInt(t.value,10)))i=1e3;else if(~~i<200)i=200;a.height=i}if(\\\"link\\\"===t.message)if(s=l.createElement(\\\"a\\\"),n=l.createElement(\\\"a\\\"),s.href=a.getAttribute(\\\"src\\\"),n.href=t.value,n.host===s.host)if(l.activeElement===a)d.top.location.href=t.value}}},e)d.addEventListener(\\\"message\\\",d.wp.receiveEmbedMessage,!1),l.addEventListener(\\\"DOMContentLoaded\\\",t,!1),d.addEventListener(\\\"load\\\",t,!1);function t(){if(!o){o=!0;var e,t,r,a,i=-1!==navigator.appVersion.indexOf(\\\"MSIE 10\\\"),s=!!navigator.userAgent.match(/Trident.*rv:11\\\\./),n=l.querySelectorAll(\\\"iframe.wp-embedded-content\\\");for(t=0;t<n.length;t++){if(!(r=n[t]).getAttribute(\\\"data-secret\\\"))a=Math.random().toString(36).substr(2,10),r.src+=\\\"#?secret=\\\"+a,r.setAttribute(\\\"data-secret\\\",a);if(i||s)(e=r.cloneNode(!0)).removeAttribute(\\\"security\\\"),r.parentNode.replaceChild(e,r)}}}}(window,document);\\n//--><!]]>\\n</script><iframe sandbox=\\\"allow-scripts\\\" security=\\\"restricted\\\" src=\\\"https://tongfamily.com/2019/10/08/ubiquiti-unifi-controller-4-11-47-and-java-runtime-nightmare/embed/\\\" width=\\\"600\\\" height=\\\"338\\\" title=\\\"&#8220;Ubiquiti Unifi Controller 4.11.47 and Java Runtime nightmare&#8221; &#8212; Tong Family\\\" frameborder=\\\"0\\\" marginwidth=\\\"0\\\" marginheight=\\\"0\\\" scrolling=\\\"no\\\" class=\\\"wp-embedded-content\\\"></iframe>\",\"type\":\"rich\"}],[\"image\",{\"src\":\"/content/images/2020/05/Screenshot_20200517-223735-1.png\"}],[\"image\",{\"src\":\"/content/images/2020/05/Screenshot_20200517-223744-1.png\"}],[\"image\",{\"src\":\"/content/images/2020/05/Screenshot_20200517-221603-4.png\"}],[\"bookmark\",{\"type\":\"bookmark\",\"url\":\"https://help.ui.com/hc/en-us/articles/204909754-UniFi-Device-Adoption-Methods-for-Remote-UniFi-Controllers\",\"metadata\":{\"url\":\"http://help.ui.com/hc/en-us/articles/204909754\",\"title\":\"UniFi - Device Adoption Methods for Remote UniFi Controllers\",\"description\":\"Overview This article describes several different layer-3 methods for adopting and deploying UniFi devices remotely. Our recommended methods are found below under the Chrome Web Browser and Mobile...\",\"author\":null,\"publisher\":\"Ubiquiti Networks Support and Help Center\",\"thumbnail\":\"https://theme.zdassets.com/theme_assets/77613/ff7ff89edfceb228b54443702ffba57c08d686fc.png\",\"icon\":\"https://theme.zdassets.com/theme_assets/77613/9582a1ec2edfbb499a97e723894d2e9a4d8c66dd.png\"}}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://www.amazon.co.uk/gp/r.html?C=38VLIQ1P7ES9H&K=32BU4EO7DARI&M=urn:rtn:msg:20200430132013e0b9bda6501e4b1fa04e0a873fc0p0eu&R=O36YOI0VYLLB&T=C&U=https%3A%2F%2Fwww.amazon.co.uk%2Fdp%2FB016K5A06C%2Fref%3Dpe_3187911_189395841_TE_dp_1&H=HKVAUIQE2WJTRQIFRXVXS8OPE24A&ref_=pe_3187911_189395841_TE_dp_1\"]],[\"a\",[\"href\",\"https://play.google.com/store/apps/details?id=com.ubnt.unifi.edu\"]],[\"a\",[\"href\",\"https://play.google.com/store/apps/details?id=com.ubnt.easyunifi&hl=en\"]]],\"sections\":[[1,\"p\",[[0,[0],1,\"Aim\"],[0,[],0,\": Have strong WiFi across a property. In other words elimate the deadzone.\"]]],[1,\"p\",[[0,[0],1,\"Solution:\"],[0,[],0,\" Keep existing ISP provided router and plugin a Ubiquiti Long Range Access Point.\"]]],[1,\"p\",[[0,[0],1,\"tl:dr\"],[0,[],0,\" - ✅ Solved - No deadzone. But, setting up up commercial grade wifi was not user friendly.\"]]],[1,\"p\",[[0,[0],1,\"Ingredients:\"],[0,[],0,\" \"]]],[3,\"ul\",[[[0,[1],1,\"Ubiquiti Networks UAP-AC-LR 175.7 x 43.2 mm 2.4-5 GHz 802.11ac Dual-Radio Long Range Access Point - White\"]],[[0,[],0,\"2m Ethernet Cable (+ later a 10m)\"]]]],[1,\"p\",[[0,[],0,\"There are various ways to solve internet problems. I've tried stronger routers, stronger receivers, 30m ethernet cables, TP-Link powerlines. Except for the 30m cable, all the others were disappointments. But a cable doesn't solve the WiFi. So some reading on network topologies led me to evaluating mesh networks, boosters and access points. With a trusted friends recommendation I was swayed to go with a commercial grade access point. There were many failures along the way.\"]]],[1,\"h2\",[[0,[],0,\"🤦‍♀️ Unboxing...\"]]],[1,\"p\",[[0,[],0,\"🛑 Mistake 1 - Doesn't come with an ethernet cable.\"],[1,[],0,0],[0,[],0,\"Yes this £100 solution doesn't come with even the shortest of ethernet cables to connect it to my router. I also had no power adaptor space. Amazon Prime to the rescue.\"]]],[1,\"h2\",[[0,[],0,\"🤔 All plugged in: Now how do I connect it up?\"]]],[1,\"p\",[[0,[],0,\"The instructions that come with it don't help. Next stop, interwebs. I start to hit a lot of content that is sending me wayward. \"]]],[1,\"p\",[[0,[],0,\"🛑 Mistake 2 - This is not the \"],[0,[2],1,\"UniFi software controller\"],[0,[],0,\" you are looking for. This android app was the first UniFi app to come up and was a pure dead end.\"]]],[1,\"p\",[[0,[],0,\"🛑 Mistake 3 - \"],[0,[2],1,\"N\"],[0,[],0,\"ext attempt was the OSX software controller. Run for the hills if you see this error:\"]]],[10,0],[1,\"p\",[[0,[],0,\"I messed around with some open JDKs but ultimately was entering a rabbit hole. Seems like others have shared this 'nightmare' and didn't come up with much in the way of solution. \"]]],[10,1],[1,\"p\",[[0,[],0,\"I started to think I needed a physical controller since most of the docs are oriented around his setup. However, I revisted the Android options and downloaded the \"],[0,[3],1,\"Unifi Network app\"],[0,[],0,\". Upon opening you get the following screen:\"]]],[10,2],[1,\"p\",[[0,[],0,\"Again with the controller emphasis. Unintuitively, if you click on account you can then access setting up 'Standalone Devices'.\"]]],[10,3],[1,\"p\",[[0,[],0,\"It's at this point you can search and add your new Unifi device. Once added, you can then tinker with the configuration. What's really nice is that the 2G and 5G by default share the same SSID - i.e. unlike lots of homes that have a network for each, the UniFi device optimises the network based on the strength of connection with a device on the network. 😎\"]]],[10,4],[1,\"h2\",[[0,[],0,\"Results \"]]],[1,\"p\",[[0,[],0,\"I had installed the access point right next to the router. I experienced immediate results as I walked through the deadzone. My mobile switched automatically to the Unifi network once a little way down the garden. And all the way to the garden shed. What I didn't test was inside the shed - it's amazing how buildings kill WiFi. Patch at best to no signal inside. 🤯\"],[1,[],0,1],[0,[],0,\"I decided to run a 10m cable between the router and the Unifi AP such that the Unifi device sits right at the window onlooking the garden. Now getting very strong signal across the garden - significant drop off ~80% once inside the shed, but at least it was a consistent connection. Based on several speed tests, I was getting 3mbs - 13mbs. That'll do pig. 👍\"]]],[1,\"h3\",[[0,[],0,\"Semi-helpful docs\"]]],[10,5],[1,\"p\",[]]]}","html":"<p><strong>Aim</strong>: Have strong WiFi across a property. In other words elimate the deadzone.</p><p><strong>Solution:</strong> Keep existing ISP provided router and plugin a Ubiquiti Long Range Access Point.</p><p><strong>tl:dr</strong> - ✅ Solved - No deadzone. But, setting up up commercial grade wifi was not user friendly.</p><p><strong>Ingredients:</strong> </p><ul><li><a href=\"https://www.amazon.co.uk/gp/r.html?C=38VLIQ1P7ES9H&amp;K=32BU4EO7DARI&amp;M=urn:rtn:msg:20200430132013e0b9bda6501e4b1fa04e0a873fc0p0eu&amp;R=O36YOI0VYLLB&amp;T=C&amp;U=https%3A%2F%2Fwww.amazon.co.uk%2Fdp%2FB016K5A06C%2Fref%3Dpe_3187911_189395841_TE_dp_1&amp;H=HKVAUIQE2WJTRQIFRXVXS8OPE24A&amp;ref_=pe_3187911_189395841_TE_dp_1\">Ubiquiti Networks UAP-AC-LR 175.7 x 43.2 mm 2.4-5 GHz 802.11ac Dual-Radio Long Range Access Point - White</a></li><li>2m Ethernet Cable (+ later a 10m)</li></ul><p>There are various ways to solve internet problems. I've tried stronger routers, stronger receivers, 30m ethernet cables, TP-Link powerlines. Except for the 30m cable, all the others were disappointments. But a cable doesn't solve the WiFi. So some reading on network topologies led me to evaluating mesh networks, boosters and access points. With a trusted friends recommendation I was swayed to go with a commercial grade access point. There were many failures along the way.</p><h2 id=\"-unboxing-\">🤦‍♀️ Unboxing...</h2><p>🛑 Mistake 1 - Doesn't come with an ethernet cable.<br>Yes this £100 solution doesn't come with even the shortest of ethernet cables to connect it to my router. I also had no power adaptor space. Amazon Prime to the rescue.</p><h2 id=\"-all-plugged-in-now-how-do-i-connect-it-up\">🤔 All plugged in: Now how do I connect it up?</h2><p>The instructions that come with it don't help. Next stop, interwebs. I start to hit a lot of content that is sending me wayward. </p><p>🛑 Mistake 2 - This is not the <a href=\"https://play.google.com/store/apps/details?id=com.ubnt.unifi.edu\">UniFi software controller</a> you are looking for. This android app was the first UniFi app to come up and was a pure dead end.</p><p>🛑 Mistake 3 - <a href=\"https://play.google.com/store/apps/details?id=com.ubnt.unifi.edu\">N</a>ext attempt was the OSX software controller. Run for the hills if you see this error:</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/Screen-Shot-2020-05-17-at-22.02.26.png\" class=\"kg-image\" alt></figure><p>I messed around with some open JDKs but ultimately was entering a rabbit hole. Seems like others have shared this 'nightmare' and didn't come up with much in the way of solution. </p><figure class=\"kg-card kg-embed-card\"><blockquote class=\"wp-embedded-content\"><a href=\"https://tongfamily.com/2019/10/08/ubiquiti-unifi-controller-4-11-47-and-java-runtime-nightmare/\">Ubiquiti Unifi Controller 4.11.47 and Java Runtime nightmare</a></blockquote>\n<script type='text/javascript'>\n<!--//--><![CDATA[//><!--\n\t\t/*! This file is auto-generated */\n\t\t!function(d,l){\"use strict\";var e=!1,o=!1;if(l.querySelector)if(d.addEventListener)e=!0;if(d.wp=d.wp||{},!d.wp.receiveEmbedMessage)if(d.wp.receiveEmbedMessage=function(e){var t=e.data;if(t)if(t.secret||t.message||t.value)if(!/[^a-zA-Z0-9]/.test(t.secret)){var r,a,i,s,n,o=l.querySelectorAll('iframe[data-secret=\"'+t.secret+'\"]'),c=l.querySelectorAll('blockquote[data-secret=\"'+t.secret+'\"]');for(r=0;r<c.length;r++)c[r].style.display=\"none\";for(r=0;r<o.length;r++)if(a=o[r],e.source===a.contentWindow){if(a.removeAttribute(\"style\"),\"height\"===t.message){if(1e3<(i=parseInt(t.value,10)))i=1e3;else if(~~i<200)i=200;a.height=i}if(\"link\"===t.message)if(s=l.createElement(\"a\"),n=l.createElement(\"a\"),s.href=a.getAttribute(\"src\"),n.href=t.value,n.host===s.host)if(l.activeElement===a)d.top.location.href=t.value}}},e)d.addEventListener(\"message\",d.wp.receiveEmbedMessage,!1),l.addEventListener(\"DOMContentLoaded\",t,!1),d.addEventListener(\"load\",t,!1);function t(){if(!o){o=!0;var e,t,r,a,i=-1!==navigator.appVersion.indexOf(\"MSIE 10\"),s=!!navigator.userAgent.match(/Trident.*rv:11\\./),n=l.querySelectorAll(\"iframe.wp-embedded-content\");for(t=0;t<n.length;t++){if(!(r=n[t]).getAttribute(\"data-secret\"))a=Math.random().toString(36).substr(2,10),r.src+=\"#?secret=\"+a,r.setAttribute(\"data-secret\",a);if(i||s)(e=r.cloneNode(!0)).removeAttribute(\"security\"),r.parentNode.replaceChild(e,r)}}}}(window,document);\n//--><!]]>\n</script><iframe sandbox=\"allow-scripts\" security=\"restricted\" src=\"https://tongfamily.com/2019/10/08/ubiquiti-unifi-controller-4-11-47-and-java-runtime-nightmare/embed/\" width=\"600\" height=\"338\" title=\"&#8220;Ubiquiti Unifi Controller 4.11.47 and Java Runtime nightmare&#8221; &#8212; Tong Family\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" class=\"wp-embedded-content\"></iframe></figure><p>I started to think I needed a physical controller since most of the docs are oriented around his setup. However, I revisted the Android options and downloaded the <a href=\"https://play.google.com/store/apps/details?id=com.ubnt.easyunifi&amp;hl=en\">Unifi Network app</a>. Upon opening you get the following screen:</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/Screenshot_20200517-223735-1.png\" class=\"kg-image\" alt></figure><p>Again with the controller emphasis. Unintuitively, if you click on account you can then access setting up 'Standalone Devices'.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/Screenshot_20200517-223744-1.png\" class=\"kg-image\" alt></figure><p>It's at this point you can search and add your new Unifi device. Once added, you can then tinker with the configuration. What's really nice is that the 2G and 5G by default share the same SSID - i.e. unlike lots of homes that have a network for each, the UniFi device optimises the network based on the strength of connection with a device on the network. 😎</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/Screenshot_20200517-221603-4.png\" class=\"kg-image\" alt></figure><h2 id=\"results\">Results </h2><p>I had installed the access point right next to the router. I experienced immediate results as I walked through the deadzone. My mobile switched automatically to the Unifi network once a little way down the garden. And all the way to the garden shed. What I didn't test was inside the shed - it's amazing how buildings kill WiFi. Patch at best to no signal inside. 🤯<br>I decided to run a 10m cable between the router and the Unifi AP such that the Unifi device sits right at the window onlooking the garden. Now getting very strong signal across the garden - significant drop off ~80% once inside the shed, but at least it was a consistent connection. Based on several speed tests, I was getting 3mbs - 13mbs. That'll do pig. 👍</p><h3 id=\"semi-helpful-docs\">Semi-helpful docs</h3><figure class=\"kg-card kg-bookmark-card\"><a class=\"kg-bookmark-container\" href=\"https://help.ui.com/hc/en-us/articles/204909754-UniFi-Device-Adoption-Methods-for-Remote-UniFi-Controllers\"><div class=\"kg-bookmark-content\"><div class=\"kg-bookmark-title\">UniFi - Device Adoption Methods for Remote UniFi Controllers</div><div class=\"kg-bookmark-description\">Overview This article describes several different layer-3 methods for adopting and deploying UniFi devices remotely. Our recommended methods are found below under the Chrome Web Browser and Mobile...</div><div class=\"kg-bookmark-metadata\"><img class=\"kg-bookmark-icon\" src=\"https://theme.zdassets.com/theme_assets/77613/9582a1ec2edfbb499a97e723894d2e9a4d8c66dd.png\"><span class=\"kg-bookmark-publisher\">Ubiquiti Networks Support and Help Center</span></div></div><div class=\"kg-bookmark-thumbnail\"><img src=\"https://theme.zdassets.com/theme_assets/77613/ff7ff89edfceb228b54443702ffba57c08d686fc.png\"></div></a></figure>","comment_id":"5ebf0a39ee6c8e0001bda35d","plaintext":"Aim: Have strong WiFi across a property. In other words elimate the deadzone.\n\nSolution: Keep existing ISP provided router and plugin a Ubiquiti Long Range\nAccess Point.\n\ntl:dr - ✅ Solved - No deadzone. But, setting up up commercial grade wifi was not\nuser friendly.\n\nIngredients: \n\n * Ubiquiti Networks UAP-AC-LR 175.7 x 43.2 mm 2.4-5 GHz 802.11ac Dual-Radio\n   Long Range Access Point - White\n   [https://www.amazon.co.uk/gp/r.html?C=38VLIQ1P7ES9H&K=32BU4EO7DARI&M=urn:rtn:msg:20200430132013e0b9bda6501e4b1fa04e0a873fc0p0eu&R=O36YOI0VYLLB&T=C&U=https%3A%2F%2Fwww.amazon.co.uk%2Fdp%2FB016K5A06C%2Fref%3Dpe_3187911_189395841_TE_dp_1&H=HKVAUIQE2WJTRQIFRXVXS8OPE24A&ref_=pe_3187911_189395841_TE_dp_1]\n * 2m Ethernet Cable (+ later a 10m)\n\nThere are various ways to solve internet problems. I've tried stronger routers,\nstronger receivers, 30m ethernet cables, TP-Link powerlines. Except for the 30m\ncable, all the others were disappointments. But a cable doesn't solve the WiFi.\nSo some reading on network topologies led me to evaluating mesh networks,\nboosters and access points. With a trusted friends recommendation I was swayed\nto go with a commercial grade access point. There were many failures along the\nway.\n\n🤦‍♀️ Unboxing...\n🛑 Mistake 1 - Doesn't come with an ethernet cable.\nYes this £100 solution doesn't come with even the shortest of ethernet cables to\nconnect it to my router. I also had no power adaptor space. Amazon Prime to the\nrescue.\n\n🤔 All plugged in: Now how do I connect it up?\nThe instructions that come with it don't help. Next stop, interwebs. I start to\nhit a lot of content that is sending me wayward. \n\n🛑 Mistake 2 - This is not the UniFi software controller\n[https://play.google.com/store/apps/details?id=com.ubnt.unifi.edu] you are\nlooking for. This android app was the first UniFi app to come up and was a pure\ndead end.\n\n🛑 Mistake 3 - N\n[https://play.google.com/store/apps/details?id=com.ubnt.unifi.edu]ext attempt\nwas the OSX software controller. Run for the hills if you see this error:\n\nI messed around with some open JDKs but ultimately was entering a rabbit hole.\nSeems like others have shared this 'nightmare' and didn't come up with much in\nthe way of solution. \n\n> Ubiquiti Unifi Controller 4.11.47 and Java Runtime nightmare\n[https://tongfamily.com/2019/10/08/ubiquiti-unifi-controller-4-11-47-and-java-runtime-nightmare/]\nI started to think I needed a physical controller since most of the docs are\noriented around his setup. However, I revisted the Android options and\ndownloaded the Unifi Network app\n[https://play.google.com/store/apps/details?id=com.ubnt.easyunifi&hl=en]. Upon\nopening you get the following screen:\n\nAgain with the controller emphasis. Unintuitively, if you click on account you\ncan then access setting up 'Standalone Devices'.\n\nIt's at this point you can search and add your new Unifi device. Once added, you\ncan then tinker with the configuration. What's really nice is that the 2G and 5G\nby default share the same SSID - i.e. unlike lots of homes that have a network\nfor each, the UniFi device optimises the network based on the strength of\nconnection with a device on the network. 😎\n\nResults \nI had installed the access point right next to the router. I experienced\nimmediate results as I walked through the deadzone. My mobile switched\nautomatically to the Unifi network once a little way down the garden. And all\nthe way to the garden shed. What I didn't test was inside the shed - it's\namazing how buildings kill WiFi. Patch at best to no signal inside. 🤯\nI decided to run a 10m cable between the router and the Unifi AP such that the\nUnifi device sits right at the window onlooking the garden. Now getting very\nstrong signal across the garden - significant drop off ~80% once inside the\nshed, but at least it was a consistent connection. Based on several speed tests,\nI was getting 3mbs - 13mbs. That'll do pig. 👍\n\nSemi-helpful docs\nUniFi - Device Adoption Methods for Remote UniFi ControllersOverview This\narticle describes several different layer-3 methods for adopting and deploying\nUniFi devices remotely. Our recommended methods are found below under the\nChrome\nWeb Browser and Mobile...Ubiquiti Networks Support and Help Center\n[https://help.ui.com/hc/en-us/articles/204909754-UniFi-Device-Adoption-Methods-for-Remote-UniFi-Controllers]","feature_image":"/content/images/2020/05/wifi-aim-1.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2020-05-15 21:31:37","created_by":"1","updated_at":"2020-05-17 22:10:47","updated_by":null,"published_at":"2020-05-17 22:10:47","published_by":"1","custom_excerpt":"How to setup a Ubiquiti Access Point to resolve Wifi deadzones. And, how to avoid some mistakes in setting it up.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null},{"id":"5f527ea284e1ba4f6553dd20","uuid":"da2486c9-cccf-4d2e-b57d-5b5912e3cfe4","title":"The Best Self-Hosted RSS Feed Readers","slug":"rss-feed-reader","mobiledoc":"{\"version\":\"0.3.2\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"docker run --rm -it -e DATABASE_URL=\\\"sqlite3:':memory:'\\\" -p 8080:8080 mdswanson/stringer\\n\",\"language\":\"bash\"}],[\"image\",{\"src\":\"/content/images/2020/05/image.png\"}],[\"image\",{\"src\":\"/content/images/2020/05/image-1.png\"}],[\"image\",{\"src\":\"/content/images/2020/05/image-2.png\"}],[\"image\",{\"src\":\"/content/images/2020/05/Screen-Shot-2020-05-31-at-09.41.46.png\"}],[\"code\",{\"code\":\"version: '3'\\nservices:\\n  miniflux:\\n    image: miniflux/miniflux:latest\\n    ports:\\n      - \\\"8050:8080\\\"\\n    depends_on:\\n      - db\\n    environment:\\n      - DATABASE_URL=postgres://miniflux:secret@db/miniflux?sslmode=disable\\n      - RUN_MIGRATIONS=1\\n      - CREATE_ADMIN=1\\n      - ADMIN_USERNAME=admin\\n      - ADMIN_PASSWORD=test123\\n  db:\\n    image: postgres:latest\\n    environment:\\n      - POSTGRES_USER=miniflux\\n      - POSTGRES_PASSWORD=secret\\n    volumes:\\n      - miniflux-db:/var/lib/postgresql/data\\nvolumes:\\n  miniflux-db:  \\n\",\"language\":\"docker\"}],[\"code\",{\"language\":\"bash\",\"code\":\"$ docker-compose up -d db\\n$ docker-compose up miniflux\"}],[\"image\",{\"src\":\"/content/images/2020/05/Screen-Shot-2020-05-31-at-09.46.28.png\"}],[\"image\",{\"src\":\"/content/images/2020/05/Screen-Shot-2020-05-31-at-13.36.10.png\"}],[\"image\",{\"src\":\"/content/images/2020/05/Screen-Shot-2020-05-31-at-14.02.22.png\"}],[\"code\",{\"code\":\"const urls = document.querySelectorAll('.blogUrl');\\nconst url_list = [];\\n\\nurls.forEach(url => {\\n  url_list.push(url.innerText);\\n});\\nconsole.log(url_list);\",\"language\":\"js\"}]],\"markups\":[[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/RSS\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Closed_platform\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Google_Reader\"]],[\"em\"],[\"a\",[\"href\",\"https://github.com/awesome-selfhosted/awesome-selfhosted#feed-readers\"]],[\"a\",[\"href\",\"https://getstream.io/winds/\"]],[\"a\",[\"href\",\"https://github.com/swanson/stringer\"]],[\"a\",[\"href\",\"https://vimium.github.io/\"]],[\"a\",[\"href\",\"https://selfoss.aditu.de/\"]],[\"a\",[\"href\",\"https://github.com/miniflux/miniflux\"]],[\"a\",[\"href\",\"https://github.com/FreshRSS/FreshRSS\"]],[\"a\",[\"href\",\"https://demo.freshrss.org/i/\"]],[\"a\",[\"href\",\"https://github.com/feedbin/feedbin\"]],[\"a\",[\"href\",\"https://github.com/Athou/commafeed\"]],[\"strong\"],[\"code\"],[\"a\",[\"href\",\"https://github.com/swanson/stringer/issues/512\"]],[\"a\",[\"href\",\"https://postgresapp.com/\"]],[\"a\",[\"href\",\"https://miniflux.app/docs/installation.html#docker\"]],[\"a\",[\"href\",\"https://getstream.io/\"]],[\"a\",[\"href\",\"https://www.algolia.com/\"]],[\"a\",[\"href\",\"https://www.mongodb.com/cloud/atlas\"]],[\"a\",[\"href\",\"https://apps.apple.com/us/app/winds-by-getstream-io/id1381446741\"]],[\"a\",[\"href\",\"https://medium.com/feed/@cjolowicz/\"]],[\"a\",[\"href\",\"https://opml-gen.ovh/\"]],[\"a\",[\"href\",\"https://www.blogger.com/manage-blogs-following.g\"]],[\"a\",[\"href\",\"https://github.com/RSS-Bridge/rss-bridge\"]],[\"a\",[\"href\",\"https://github.com/wallabag/wallabag\"]],[\"a\",[\"href\",\"https://archivebox.io/\"]]],\"sections\":[[1,\"h2\",[[0,[],0,\"What is \"],[0,[0],1,\"RSS\"],[0,[],0,\"?\"]]],[1,\"p\",[[0,[],0,\"It aims to be an open standard for the open web, where applications can get updates from websites. This runs against the current closed off portions of the web like Facebook that keep content behind their account login - i.e. the internet's \"],[0,[1],1,\"'walled gardens'\"],[0,[],0,\". For a while, a lot of people used the popular \"],[0,[2],1,\"Google Reader\"],[0,[],0,\" RSS application - unfortunately, as with many loved Google projects it got killed off because of declining usage and it didn't generate revenue. \"]]],[1,\"p\",[[0,[],0,\"So why get one now? \"]]],[3,\"ul\",[[[0,[],0,\"Ideals of an open web. I don't have a Facebook account. I believe there are interesting blogs out there that I don't necessarily see unless they surfaced through the front page of search results or aggregators like Hacker News or Reddit. \"]],[[0,[],0,\"Solve the problem of - \"],[0,[3],1,\"huh this blog/site was really interesting. How do I keep track of new content without signing up to a thousand newsletters or entering a walled garden?\"]]]],[1,\"h2\",[[0,[],0,\"My Starting Point\"]]],[1,\"p\",[[0,[],0,\"There's a bunch of open source Feed Readers listed on the \"],[0,[4],1,\"Feed Reader section of Awesome Self-Hosted\"],[0,[],0,\". My criteria to filter the list were:\"]]],[3,\"ul\",[[[0,[],0,\"Minimum 1000 Github Stars\"]],[[0,[],0,\"Commit in the last 6 months\"]]]],[1,\"p\",[[0,[],0,\"This is what I was left with. Which I then filtered based on my UI and tech preferences i.e. don't be ugly and don't use PHP/Java.\"]]],[1,\"h2\",[[0,[],0,\"👨‍👩‍👧‍👧 The Contenders \"]]],[1,\"h3\",[[0,[5],1,\"Winds\"]]],[1,\"p\",[[0,[3],1,\"The kitchen sink reader\"],[1,[],0,0],[0,[],0,\"Github Stars: 7.5k\"],[1,[],0,1],[0,[],0,\"Built With: JavaScript (React + Express)\"],[1,[],0,2],[0,[],0,\"Test Website: No, but free account creation\"],[1,[],0,3],[0,[],0,\"Features:\"]]],[3,\"ul\",[[[0,[],0,\"Recommender service via external service\"]],[[0,[],0,\"Very nice UI\"]],[[0,[],0,\"'Listen and read' i.e. podcast player too\"]]]],[1,\"h3\",[[0,[6],1,\"Stringer\"]]],[1,\"p\",[[0,[3],1,\"Stringer is simple... 'has no external dependencies, no social recommendations/sharing, and no fancy machine learning algorithms.'\"],[1,[],0,4],[0,[],0,\"Github Stars: 3.2k\"],[1,[],0,5],[0,[],0,\"Built With: Ruby\"],[1,[],0,6],[0,[],0,\"Test Website: No, but has a click to deploy to Heroku option\"],[1,[],0,7],[0,[],0,\"Features:\"]]],[3,\"ul\",[[[0,[],0,\"Simple and nice UI\"]],[[0,[],0,\"Keyboard shortcuts (I use \"],[0,[7],1,\"Vimium\"],[0,[],0,\" so not sold on this)\"]]]],[1,\"h2\",[[0,[8],1,\"selfoss\"]]],[1,\"p\",[[0,[3],1,\"Didn't look further at this one PHP+ugly =\"],[0,[],0,\" 😱🏃‍♂️.\"],[1,[],0,8],[0,[],0,\"Github Stars: 1.9k\"],[1,[],0,9],[0,[],0,\"Built With: PHP\"],[1,[],0,10],[0,[],0,\"Test Website: No\"]]],[1,\"h2\",[[0,[9],1,\"Miniflux\"]]],[1,\"p\",[[0,[3],1,\"Ultra simplicity in terms of stack and UI. Slightly ugly, but I agree with the software philosophy of keeping it simple.\"],[1,[],0,11],[0,[],0,\"Github Stars: 1.9k\"],[1,[],0,12],[0,[],0,\"Built With: Plain go-lang+minimal vanilla JavaScript\"],[1,[],0,13],[0,[],0,\"Test Website: No\"],[1,[],0,14],[0,[],0,\"Features:\"]]],[3,\"ul\",[[[0,[],0,\"Support multiple enclosures/attachments (Podcasts, videos, music, and images)\"]],[[0,[],0,\"Play videos from YouTube channels directly inside Miniflux\"]],[[0,[],0,\"Categories\"]],[[0,[],0,\"Bookmarks\"]],[[0,[],0,\"Fetch website icons (favicons)\"]],[[0,[],0,\"Save articles to third-party services e.g. Pocket\"]],[[0,[],0,\"Dark theme (otherwise kind of ugly)\"]],[[0,[],0,\"Optional keyboard shortcuts\"]]]],[1,\"h2\",[[0,[10,10],2,\"FreshRSS\"]]],[1,\"p\",[[0,[3],1,\"Like Selfoss - Didn't look further at this one PHP+ugly =\"],[0,[],0,\" 😱🏃‍♂️.\"],[1,[],0,15],[0,[],0,\"Github Stars: 2.3k\"],[1,[],0,16],[0,[],0,\"Built With: PHP\"],[1,[],0,17],[0,[],0,\"Test website: \"],[0,[11],1,\"Yes\"]]],[1,\"h2\",[[0,[12,12],2,\"feedbin\"]]],[1,\"p\",[[0,[3],1,\"Didn't look further as the app tech + ui wasn't appealing to me\"],[0,[],0,\".\"],[1,[],0,18],[0,[],0,\"Github Stars: 2.5k\"],[1,[],0,19],[0,[],0,\"Built With: Ruby on Rails\"],[1,[],0,20],[0,[],0,\"Test Website: No - but free trial\"]]],[1,\"h2\",[[0,[13],0,\"C\"],[0,[13],2,\"ommafeed\"]]],[1,\"p\",[[0,[3],0,\"Github at the time had a \"],[0,[14],1,\"🛑build error\"],[0,[],1,\" + Java turned me away\"],[1,[],0,21],[0,[],0,\"Github Stars: 1.7k\"],[1,[],0,22],[0,[],0,\"Built With: Java 😱 at the time of reviewing had a 'build: error'\"],[1,[],0,23],[0,[],0,\"Test Website: no - but free trial\"]]],[1,\"p\",[[0,[14],1,\"Final round contenders: Miniflux, Stringer and Winds\"]]],[1,\"h2\",[[0,[],0,\"🏁 Top 3 contenders - Install and Review \"]]],[1,\"p\",[[0,[],0,\"N.B. I'm running these locally to assess before deploying to my server.\"]]],[1,\"h3\",[[0,[6],1,\"Stringer\"]]],[10,0],[1,\"p\",[[0,[],0,\"Opened on \"],[0,[15],1,\"localhost:8080\"],[0,[],0,\" - Simple as that. UI feels nice and simple. Unfortunately, the only import methods are from the long defunct Google Reader or with one URL at a time. \"]]],[10,1],[1,\"p\",[[0,[],0,\"I tried importing the OPML file through this method but that seemed to crash the application - poor error handling?\"]]],[10,2],[1,\"p\",[[0,[],0,\"Once you do manually load some feeds the \"],[0,[15],1,\"/feeds\"],[0,[],0,\" interface is nice and clean.\"]]],[10,3],[1,\"p\",[[0,[],0,\"The \"],[0,[15],1,\"/news\"],[0,[],0,\" view is well laid out and the fonts used are a nice reading experience.\"]]],[10,4],[1,\"p\",[[0,[],0,\"Lastly, whilst poking around the Github repo I found this comment on \"],[0,[16],1,\"this recent issue\"],[0,[],0,\":\"]]],[1,\"blockquote\",[[0,[],0,\"... I don't think anyone is currently developing new features. I'm available to review code and merge it if you want to submit a patch.\"]]],[1,\"p\",[[0,[],0,\"So whilst the UI is nice and the setup is incredibly easy. The active development of  Miniflux and Wind have started to look more appealing.\"]]],[1,\"p\",[[0,[],0,\"⭐️ 6.5 / 10 - Easy install, nice UI, functionality is low and active development has slowed.\"]]],[1,\"h3\",[[0,[9],1,\"Miniflux\"]]],[3,\"ol\",[[[0,[],0,\"You need postgres which I had not setup on my mac. I recommend doing this the easy way with \"],[0,[17],1,\"postgres.app\"],[0,[],0,\".\"]],[[0,[],0,\"The \"],[0,[18],1,\"instructions \"],[0,[],0,\"here are a little hairy. I went with saving the following to a \"],[0,[15],1,\"docker-compose.yml\"],[0,[],0,\". I couldn't get it working on port 80 so switched the port to 8050. \"]]]],[10,5],[1,\"p\",[[0,[],0,\"Then run the following commands and go to \"],[0,[15],1,\"localhost:8050\"],[0,[],0,\" once the containers have finished booting: \"]]],[10,6],[1,\"p\",[[0,[],0,\"Firstly, the initial UI is \"],[0,[3],1,\"very \"],[0,[],0,\"simple, whilst incredibly plain is very functional. Navigating the app is explicit with text whereas Stringer you had to poke around using icons without text descriptions.\"]]],[10,7],[1,\"p\",[[0,[],0,\"There are several integrations to apps like Pocket, Instapaper and Wallabag. And after changing the theme. the reader is quite nice. Albeit, I do notice the entire page refresh from the server side oriented app - but I have to tell myself that the simplicity philosophy trumps the complexity of introducing a frontend framework.\"]]],[10,8],[1,\"p\",[[0,[],0,\"⭐️ 8.5 / 10 - Whilst being minimalistic has all the features you need, dark theme and the most active development of them all with a clear software philosophy.\"]]],[1,\"h2\",[[0,[5],1,\"Winds\"]]],[1,\"p\",[[0,[],0,\"The install is relatively long particularly with the external dependencies which include:\"]]],[3,\"ul\",[[[0,[19],1,\"Stream\"],[0,[],0,\" - an API for building activity feeds + handle personalization (machine learning). \"]],[[0,[20],1,\"Algolia\"],[0,[],0,\" - for search\"]],[[0,[21],1,\"MongoDB Atlas\"],[0,[],0,\" - a DB as a service (DBaaS)\"]]]],[1,\"p\",[[0,[],0,\"At this point I opt to sign up for a free account to try it out. I get the \"],[0,[22],1,\"app from the apple store\"],[0,[],0,\". Upon opening Winds, you are first confronted with a selection of interests like 'Programming', 'News' etc. I pick a couple and click next. I then have to create an account 😑 which I do. Then once logged in I have a very populated set of feeds and podcasts.\"]]],[10,9],[1,\"p\",[[0,[],0,\"I then want to try and remove all of these and start fresh, which no amount of clicking around can I figure out how to do other than clicking individually on each feed to remove it. I could create a new account and then add my OPML file - however at this point I'm already missing the bloat free approach of Miniflux.\"]]],[1,\"p\",[[0,[],0,\"⭐️ 7 / 10  - for a non self-hosted piece of software I can see this is actually really nice. However major downsides of the installation complexity for self-hosted installation combined with features I don't particularly want. For instance, I don't want a recommender system - I can find my own content and probably consume too much as it is! If I want reinforcing recommender systems I'd go into a walled garden like Facebook.\"]]],[1,\"h2\",[[0,[],0,\"🎉 Winner - Miniflux 🎉\"]]],[1,\"p\",[]],[1,\"h2\",[[0,[],0,\"RSS Tips\"]]],[1,\"p\",[[0,[],0,\"Do you have a list of websites you want to follow?\"]]],[3,\"ul\",[[[0,[],0,\"A lot of sites you can just \"],[0,[15],1,\"/rss\"],[0,[],0,\" e.g \"],[0,[15],1,\"domainname.com/rss\"]],[[0,[],0,\"For medium sites add \"],[0,[15],1,\"/feed/\"],[0,[],0,\" e.g. \"],[0,[23,15],2,\"https://medium.com/feed/@verygoodblogger/\"]],[[0,[],0,\"I used \"],[0,[24],1,\"this site\"],[0,[],0,\" to convert a list feeds into an OPML file I could mass import to the various RSS feed reader apps.\"]],[[0,[],0,\"About a decade ago I used blogspot/blogger. It had a feed reader, but no export facility. Nostagilically, I thought I'd get the list from there. Here's a hack to export the list. First navigate to '\"],[0,[25],1,\"Manage blogs I'm following\"],[0,[],0,\"', then open your browser dev tools and run the following JavaScript to get a list of the links.\"]]]],[10,10],[1,\"h2\",[[0,[],0,\"Related Interesting Projects\"]]],[3,\"ul\",[[[0,[26],1,\"RSS-Bridge\"],[0,[],0,\" - Create RSS feeds for websites without a feed. \"]],[[0,[27],1,\"Wallabag\"],[0,[],0,\" - Save articles from RSS feeds.\"]],[[0,[28],1,\"Archivebox\"],[0,[],0,\" - Save all content e.g. videos, entire web pages and configure that save in multiple format. Solves link rot issues.\"]]]]]}","html":"<h2 id=\"what-is-rss\">What is <a href=\"https://en.wikipedia.org/wiki/RSS\">RSS</a>?</h2><p>It aims to be an open standard for the open web, where applications can get updates from websites. This runs against the current closed off portions of the web like Facebook that keep content behind their account login - i.e. the internet's <a href=\"https://en.wikipedia.org/wiki/Closed_platform\">'walled gardens'</a>. For a while, a lot of people used the popular <a href=\"https://en.wikipedia.org/wiki/Google_Reader\">Google Reader</a> RSS application - unfortunately, as with many loved Google projects it got killed off because of declining usage and it didn't generate revenue. </p><p>So why get one now? </p><ul><li>Ideals of an open web. I don't have a Facebook account. I believe there are interesting blogs out there that I don't necessarily see unless they surfaced through the front page of search results or aggregators like Hacker News or Reddit. </li><li>Solve the problem of - <em>huh this blog/site was really interesting. How do I keep track of new content without signing up to a thousand newsletters or entering a walled garden?</em></li></ul><h2 id=\"my-starting-point\">My Starting Point</h2><p>There's a bunch of open source Feed Readers listed on the <a href=\"https://github.com/awesome-selfhosted/awesome-selfhosted#feed-readers\">Feed Reader section of Awesome Self-Hosted</a>. My criteria to filter the list were:</p><ul><li>Minimum 1000 Github Stars</li><li>Commit in the last 6 months</li></ul><p>This is what I was left with. Which I then filtered based on my UI and tech preferences i.e. don't be ugly and don't use PHP/Java.</p><h2 id=\"-the-contenders\">👨‍👩‍👧‍👧 The Contenders </h2><h3 id=\"winds\"><a href=\"https://getstream.io/winds/\">Winds</a></h3><p><em>The kitchen sink reader</em><br>Github Stars: 7.5k<br>Built With: JavaScript (React + Express)<br>Test Website: No, but free account creation<br>Features:</p><ul><li>Recommender service via external service</li><li>Very nice UI</li><li>'Listen and read' i.e. podcast player too</li></ul><h3 id=\"stringer\"><a href=\"https://github.com/swanson/stringer\">Stringer</a></h3><p><em>Stringer is simple... 'has no external dependencies, no social recommendations/sharing, and no fancy machine learning algorithms.'</em><br>Github Stars: 3.2k<br>Built With: Ruby<br>Test Website: No, but has a click to deploy to Heroku option<br>Features:</p><ul><li>Simple and nice UI</li><li>Keyboard shortcuts (I use <a href=\"https://vimium.github.io/\">Vimium</a> so not sold on this)</li></ul><h2 id=\"selfoss\"><a href=\"https://selfoss.aditu.de/\">selfoss</a></h2><p><em>Didn't look further at this one PHP+ugly =</em> 😱🏃‍♂️.<br>Github Stars: 1.9k<br>Built With: PHP<br>Test Website: No</p><h2 id=\"miniflux\"><a href=\"https://github.com/miniflux/miniflux\">Miniflux</a></h2><p><em>Ultra simplicity in terms of stack and UI. Slightly ugly, but I agree with the software philosophy of keeping it simple.</em><br>Github Stars: 1.9k<br>Built With: Plain go-lang+minimal vanilla JavaScript<br>Test Website: No<br>Features:</p><ul><li>Support multiple enclosures/attachments (Podcasts, videos, music, and images)</li><li>Play videos from YouTube channels directly inside Miniflux</li><li>Categories</li><li>Bookmarks</li><li>Fetch website icons (favicons)</li><li>Save articles to third-party services e.g. Pocket</li><li>Dark theme (otherwise kind of ugly)</li><li>Optional keyboard shortcuts</li></ul><h2 id=\"freshrss\"><a href=\"https://github.com/FreshRSS/FreshRSS\"><a href=\"https://github.com/FreshRSS/FreshRSS\">FreshRSS</a></a></h2><p><em>Like Selfoss - Didn't look further at this one PHP+ugly =</em> 😱🏃‍♂️.<br>Github Stars: 2.3k<br>Built With: PHP<br>Test website: <a href=\"https://demo.freshrss.org/i/\">Yes</a></p><h2 id=\"feedbin\"><a href=\"https://github.com/feedbin/feedbin\"><a href=\"https://github.com/feedbin/feedbin\">feedbin</a></a></h2><p><em>Didn't look further as the app tech + ui wasn't appealing to me</em>.<br>Github Stars: 2.5k<br>Built With: Ruby on Rails<br>Test Website: No - but free trial</p><h2 id=\"commafeed\"><a href=\"https://github.com/Athou/commafeed\">C<a href=\"https://github.com/Athou/commafeed\">ommafeed</a></a></h2><p><em>Github at the time had a <strong>🛑build error</strong> + Java turned me away</em><br>Github Stars: 1.7k<br>Built With: Java 😱 at the time of reviewing had a 'build: error'<br>Test Website: no - but free trial</p><p><strong>Final round contenders: Miniflux, Stringer and Winds</strong></p><h2 id=\"-top-3-contenders-install-and-review\">🏁 Top 3 contenders - Install and Review </h2><p>N.B. I'm running these locally to assess before deploying to my server.</p><h3 id=\"stringer-1\"><a href=\"https://github.com/swanson/stringer\">Stringer</a></h3><pre><code class=\"language-bash\">docker run --rm -it -e DATABASE_URL=\"sqlite3:':memory:'\" -p 8080:8080 mdswanson/stringer\n</code></pre><p>Opened on <code>localhost:8080</code> - Simple as that. UI feels nice and simple. Unfortunately, the only import methods are from the long defunct Google Reader or with one URL at a time. </p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/image.png\" class=\"kg-image\" alt></figure><p>I tried importing the OPML file through this method but that seemed to crash the application - poor error handling?</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/image-1.png\" class=\"kg-image\" alt></figure><p>Once you do manually load some feeds the <code>/feeds</code> interface is nice and clean.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/image-2.png\" class=\"kg-image\" alt></figure><p>The <code>/news</code> view is well laid out and the fonts used are a nice reading experience.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/Screen-Shot-2020-05-31-at-09.41.46.png\" class=\"kg-image\" alt></figure><p>Lastly, whilst poking around the Github repo I found this comment on <a href=\"https://github.com/swanson/stringer/issues/512\">this recent issue</a>:</p><blockquote>... I don't think anyone is currently developing new features. I'm available to review code and merge it if you want to submit a patch.</blockquote><p>So whilst the UI is nice and the setup is incredibly easy. The active development of  Miniflux and Wind have started to look more appealing.</p><p>⭐️ 6.5 / 10 - Easy install, nice UI, functionality is low and active development has slowed.</p><h3 id=\"miniflux-1\"><a href=\"https://github.com/miniflux/miniflux\">Miniflux</a></h3><ol><li>You need postgres which I had not setup on my mac. I recommend doing this the easy way with <a href=\"https://postgresapp.com/\">postgres.app</a>.</li><li>The <a href=\"https://miniflux.app/docs/installation.html#docker\">instructions </a>here are a little hairy. I went with saving the following to a <code>docker-compose.yml</code>. I couldn't get it working on port 80 so switched the port to 8050. </li></ol><pre><code class=\"language-docker\">version: '3'\nservices:\n  miniflux:\n    image: miniflux/miniflux:latest\n    ports:\n      - \"8050:8080\"\n    depends_on:\n      - db\n    environment:\n      - DATABASE_URL=postgres://miniflux:secret@db/miniflux?sslmode=disable\n      - RUN_MIGRATIONS=1\n      - CREATE_ADMIN=1\n      - ADMIN_USERNAME=admin\n      - ADMIN_PASSWORD=test123\n  db:\n    image: postgres:latest\n    environment:\n      - POSTGRES_USER=miniflux\n      - POSTGRES_PASSWORD=secret\n    volumes:\n      - miniflux-db:/var/lib/postgresql/data\nvolumes:\n  miniflux-db:  \n</code></pre><p>Then run the following commands and go to <code>localhost:8050</code> once the containers have finished booting: </p><pre><code class=\"language-bash\">$ docker-compose up -d db\n$ docker-compose up miniflux</code></pre><p>Firstly, the initial UI is <em>very </em>simple, whilst incredibly plain is very functional. Navigating the app is explicit with text whereas Stringer you had to poke around using icons without text descriptions.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/Screen-Shot-2020-05-31-at-09.46.28.png\" class=\"kg-image\" alt></figure><p>There are several integrations to apps like Pocket, Instapaper and Wallabag. And after changing the theme. the reader is quite nice. Albeit, I do notice the entire page refresh from the server side oriented app - but I have to tell myself that the simplicity philosophy trumps the complexity of introducing a frontend framework.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/Screen-Shot-2020-05-31-at-13.36.10.png\" class=\"kg-image\" alt></figure><p>⭐️ 8.5 / 10 - Whilst being minimalistic has all the features you need, dark theme and the most active development of them all with a clear software philosophy.</p><h2 id=\"winds-1\"><a href=\"https://getstream.io/winds/\">Winds</a></h2><p>The install is relatively long particularly with the external dependencies which include:</p><ul><li><a href=\"https://getstream.io/\">Stream</a> - an API for building activity feeds + handle personalization (machine learning). </li><li><a href=\"https://www.algolia.com/\">Algolia</a> - for search</li><li><a href=\"https://www.mongodb.com/cloud/atlas\">MongoDB Atlas</a> - a DB as a service (DBaaS)</li></ul><p>At this point I opt to sign up for a free account to try it out. I get the <a href=\"https://apps.apple.com/us/app/winds-by-getstream-io/id1381446741\">app from the apple store</a>. Upon opening Winds, you are first confronted with a selection of interests like 'Programming', 'News' etc. I pick a couple and click next. I then have to create an account 😑 which I do. Then once logged in I have a very populated set of feeds and podcasts.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2020/05/Screen-Shot-2020-05-31-at-14.02.22.png\" class=\"kg-image\" alt></figure><p>I then want to try and remove all of these and start fresh, which no amount of clicking around can I figure out how to do other than clicking individually on each feed to remove it. I could create a new account and then add my OPML file - however at this point I'm already missing the bloat free approach of Miniflux.</p><p>⭐️ 7 / 10  - for a non self-hosted piece of software I can see this is actually really nice. However major downsides of the installation complexity for self-hosted installation combined with features I don't particularly want. For instance, I don't want a recommender system - I can find my own content and probably consume too much as it is! If I want reinforcing recommender systems I'd go into a walled garden like Facebook.</p><h2 id=\"-winner-miniflux-\">🎉 Winner - Miniflux 🎉</h2><p></p><h2 id=\"rss-tips\">RSS Tips</h2><p>Do you have a list of websites you want to follow?</p><ul><li>A lot of sites you can just <code>/rss</code> e.g <code>domainname.com/rss</code></li><li>For medium sites add <code>/feed/</code> e.g. <a href=\"https://medium.com/feed/@cjolowicz/\"><code>https://medium.com/feed/@verygoodblogger/</code></a></li><li>I used <a href=\"https://opml-gen.ovh/\">this site</a> to convert a list feeds into an OPML file I could mass import to the various RSS feed reader apps.</li><li>About a decade ago I used blogspot/blogger. It had a feed reader, but no export facility. Nostagilically, I thought I'd get the list from there. Here's a hack to export the list. First navigate to '<a href=\"https://www.blogger.com/manage-blogs-following.g\">Manage blogs I'm following</a>', then open your browser dev tools and run the following JavaScript to get a list of the links.</li></ul><pre><code class=\"language-js\">const urls = document.querySelectorAll('.blogUrl');\nconst url_list = [];\n\nurls.forEach(url =&gt; {\n  url_list.push(url.innerText);\n});\nconsole.log(url_list);</code></pre><h2 id=\"related-interesting-projects\">Related Interesting Projects</h2><ul><li><a href=\"https://github.com/RSS-Bridge/rss-bridge\">RSS-Bridge</a> - Create RSS feeds for websites without a feed. </li><li><a href=\"https://github.com/wallabag/wallabag\">Wallabag</a> - Save articles from RSS feeds.</li><li><a href=\"https://archivebox.io/\">Archivebox</a> - Save all content e.g. videos, entire web pages and configure that save in multiple format. Solves link rot issues.</li></ul>","comment_id":"5ed297d1ee6c8e0001bda4ad","plaintext":"What is RSS [https://en.wikipedia.org/wiki/RSS]?\nIt aims to be an open standard for the open web, where applications can get\nupdates from websites. This runs against the current closed off portions of the\nweb like Facebook that keep content behind their account login - i.e. the\ninternet's 'walled gardens' [https://en.wikipedia.org/wiki/Closed_platform]. For\na while, a lot of people used the popular Google Reader\n[https://en.wikipedia.org/wiki/Google_Reader] RSS application - unfortunately,\nas with many loved Google projects it got killed off because of declining usage\nand it didn't generate revenue. \n\nSo why get one now? \n\n * Ideals of an open web. I don't have a Facebook account. I believe there are\n   interesting blogs out there that I don't necessarily see unless they surfaced\n   through the front page of search results or aggregators like Hacker News or\n   Reddit. \n * Solve the problem of - huh this blog/site was really interesting. How do I\n   keep track of new content without signing up to a thousand newsletters or\n   entering a walled garden?\n\nMy Starting Point\nThere's a bunch of open source Feed Readers listed on the Feed Reader section\nof\nAwesome Self-Hosted\n[https://github.com/awesome-selfhosted/awesome-selfhosted#feed-readers]. My\ncriteria to filter the list were:\n\n * Minimum 1000 Github Stars\n * Commit in the last 6 months\n\nThis is what I was left with. Which I then filtered based on my UI and tech\npreferences i.e. don't be ugly and don't use PHP/Java.\n\n👨‍👩‍👧‍👧 The Contenders \nWinds [https://getstream.io/winds/]\nThe kitchen sink reader\nGithub Stars: 7.5k\nBuilt With: JavaScript (React + Express)\nTest Website: No, but free account creation\nFeatures:\n\n * Recommender service via external service\n * Very nice UI\n * 'Listen and read' i.e. podcast player too\n\nStringer [https://github.com/swanson/stringer]\nStringer is simple... 'has no external dependencies, no social\nrecommendations/sharing, and no fancy machine learning algorithms.'\nGithub Stars: 3.2k\nBuilt With: Ruby\nTest Website: No, but has a click to deploy to Heroku option\nFeatures:\n\n * Simple and nice UI\n * Keyboard shortcuts (I use Vimium [https://vimium.github.io/] so not sold on\n   this)\n\nselfoss [https://selfoss.aditu.de/]\nDidn't look further at this one PHP+ugly = 😱🏃‍♂️.\nGithub Stars: 1.9k\nBuilt With: PHP\nTest Website: No\n\nMiniflux [https://github.com/miniflux/miniflux]\nUltra simplicity in terms of stack and UI. Slightly ugly, but I agree with the\nsoftware philosophy of keeping it simple.\nGithub Stars: 1.9k\nBuilt With: Plain go-lang+minimal vanilla JavaScript\nTest Website: No\nFeatures:\n\n * Support multiple enclosures/attachments (Podcasts, videos, music, and images)\n * Play videos from YouTube channels directly inside Miniflux\n * Categories\n * Bookmarks\n * Fetch website icons (favicons)\n * Save articles to third-party services e.g. Pocket\n * Dark theme (otherwise kind of ugly)\n * Optional keyboard shortcuts\n\nFreshRSS [https://github.com/FreshRSS/FreshRSS]\n[https://github.com/FreshRSS/FreshRSS]\nLike Selfoss - Didn't look further at this one PHP+ugly = 😱🏃‍♂️.\nGithub Stars: 2.3k\nBuilt With: PHP\nTest website: Yes [https://demo.freshrss.org/i/]\n\nfeedbin [https://github.com/feedbin/feedbin]\n[https://github.com/feedbin/feedbin]\nDidn't look further as the app tech + ui wasn't appealing to me.\nGithub Stars: 2.5k\nBuilt With: Ruby on Rails\nTest Website: No - but free trial\n\nCommafeed [https://github.com/Athou/commafeed]\n[https://github.com/Athou/commafeed]\nGithub at the time had a 🛑build error + Java turned me away\nGithub Stars: 1.7k\nBuilt With: Java 😱 at the time of reviewing had a 'build: error'\nTest Website: no - but free trial\n\nFinal round contenders: Miniflux, Stringer and Winds\n\n🏁 Top 3 contenders - Install and Review \nN.B. I'm running these locally to assess before deploying to my server.\n\nStringer [https://github.com/swanson/stringer]\ndocker run --rm -it -e DATABASE_URL=\"sqlite3:':memory:'\" -p 8080:8080 mdswanson/stringer\n\n\nOpened on localhost:8080 - Simple as that. UI feels nice and simple.\nUnfortunately, the only import methods are from the long defunct Google Reader\nor with one URL at a time. \n\nI tried importing the OPML file through this method but that seemed to crash the\napplication - poor error handling?\n\nOnce you do manually load some feeds the /feeds interface is nice and clean.\n\nThe /news view is well laid out and the fonts used are a nice reading\nexperience.\n\nLastly, whilst poking around the Github repo I found this comment on this\nrecent\nissue [https://github.com/swanson/stringer/issues/512]:\n\n> ... I don't think anyone is currently developing new features. I'm available to\nreview code and merge it if you want to submit a patch.\nSo whilst the UI is nice and the setup is incredibly easy. The active\ndevelopment of  Miniflux and Wind have started to look more appealing.\n\n⭐️ 6.5 / 10 - Easy install, nice UI, functionality is low and active development\nhas slowed.\n\nMiniflux [https://github.com/miniflux/miniflux]\n 1. You need postgres which I had not setup on my mac. I recommend doing this\n    the easy way with postgres.app [https://postgresapp.com/].\n 2. The instructions [https://miniflux.app/docs/installation.html#docker]here\n    are a little hairy. I went with saving the following to a docker-compose.yml\n    . I couldn't get it working on port 80 so switched the port to 8050. \n\nversion: '3'\nservices:\n  miniflux:\n    image: miniflux/miniflux:latest\n    ports:\n      - \"8050:8080\"\n    depends_on:\n      - db\n    environment:\n      - DATABASE_URL=postgres://miniflux:secret@db/miniflux?sslmode=disable\n      - RUN_MIGRATIONS=1\n      - CREATE_ADMIN=1\n      - ADMIN_USERNAME=admin\n      - ADMIN_PASSWORD=test123\n  db:\n    image: postgres:latest\n    environment:\n      - POSTGRES_USER=miniflux\n      - POSTGRES_PASSWORD=secret\n    volumes:\n      - miniflux-db:/var/lib/postgresql/data\nvolumes:\n  miniflux-db:  \n\n\nThen run the following commands and go to localhost:8050 once the containers\nhave finished booting: \n\n$ docker-compose up -d db\n$ docker-compose up miniflux\n\nFirstly, the initial UI is very simple, whilst incredibly plain is very\nfunctional. Navigating the app is explicit with text whereas Stringer you had to\npoke around using icons without text descriptions.\n\nThere are several integrations to apps like Pocket, Instapaper and Wallabag. And\nafter changing the theme. the reader is quite nice. Albeit, I do notice the\nentire page refresh from the server side oriented app - but I have to tell\nmyself that the simplicity philosophy trumps the complexity of introducing a\nfrontend framework.\n\n⭐️ 8.5 / 10 - Whilst being minimalistic has all the features you need, dark\ntheme and the most active development of them all with a clear software\nphilosophy.\n\nWinds [https://getstream.io/winds/]\nThe install is relatively long particularly with the external dependencies which\ninclude:\n\n * Stream [https://getstream.io/] - an API for building activity feeds + handle\n   personalization (machine learning). \n * Algolia [https://www.algolia.com/] - for search\n * MongoDB Atlas [https://www.mongodb.com/cloud/atlas] - a DB as a service\n   (DBaaS)\n\nAt this point I opt to sign up for a free account to try it out. I get the app\nfrom the apple store\n[https://apps.apple.com/us/app/winds-by-getstream-io/id1381446741]. Upon opening\nWinds, you are first confronted with a selection of interests like\n'Programming', 'News' etc. I pick a couple and click next. I then have to create\nan account 😑 which I do. Then once logged in I have a very populated set of\nfeeds and podcasts.\n\nI then want to try and remove all of these and start fresh, which no amount of\nclicking around can I figure out how to do other than clicking individually on\neach feed to remove it. I could create a new account and then add my OPML file -\nhowever at this point I'm already missing the bloat free approach of Miniflux.\n\n⭐️ 7 / 10  - for a non self-hosted piece of software I can see this is actually\nreally nice. However major downsides of the installation complexity for\nself-hosted installation combined with features I don't particularly want. For\ninstance, I don't want a recommender system - I can find my own content and\nprobably consume too much as it is! If I want reinforcing recommender systems\nI'd go into a walled garden like Facebook.\n\n🎉 Winner - Miniflux 🎉\n\n\nRSS Tips\nDo you have a list of websites you want to follow?\n\n * A lot of sites you can just /rss e.g domainname.com/rss\n * For medium sites add /feed/ e.g. https://medium.com/feed/@verygoodblogger/\n   [https://medium.com/feed/@cjolowicz/]\n * I used this site [https://opml-gen.ovh/] to convert a list feeds into an OPML\n   file I could mass import to the various RSS feed reader apps.\n * About a decade ago I used blogspot/blogger. It had a feed reader, but no\n   export facility. Nostagilically, I thought I'd get the list from there.\n   Here's a hack to export the list. First navigate to 'Manage blogs I'm\n   following [https://www.blogger.com/manage-blogs-following.g]', then open your\n   browser dev tools and run the following JavaScript to get a list of the\n   links.\n\nconst urls = document.querySelectorAll('.blogUrl');\nconst url_list = [];\n\nurls.forEach(url => {\n  url_list.push(url.innerText);\n});\nconsole.log(url_list);\n\nRelated Interesting Projects\n * RSS-Bridge [https://github.com/RSS-Bridge/rss-bridge] - Create RSS feeds for\n   websites without a feed. \n * Wallabag [https://github.com/wallabag/wallabag] - Save articles from RSS\n   feeds.\n * Archivebox [https://archivebox.io/] - Save all content e.g. videos, entire\n   web pages and configure that save in multiple format. Solves link rot issues.","feature_image":"/content/images/2020/05/westminster-small.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","send_email_when_published":0,"author_id":"1","created_at":"2020-05-30 17:28:49","created_by":"1","updated_at":"2020-05-31 16:32:54","updated_by":null,"published_at":"2020-05-31 15:06:00","published_by":"1","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null}],"posts_meta":[{"id":"5f527ea284e1ba4f6553dd27","post_id":"5f527ea284e1ba4f6553dcdd","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"A step through of making a wordcloud visualisation using R, the text mining package tm and the wordcloud library.","email_subject":null},{"id":"5f527ea284e1ba4f6553dd2e","post_id":"5f527ea284e1ba4f6553dcdf","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"What are stamp duties? The economics and politics of stamp duties and why they should be replaced with a land tax.","email_subject":null},{"id":"5f527ea284e1ba4f6553dd3b","post_id":"5f527ea284e1ba4f6553dce6","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"How to create an interactive choropleth/heatmap of Australia using Open Street Maps, Leaflet.js, R and PSMA data.","email_subject":null},{"id":"5f527ea284e1ba4f6553dd40","post_id":"5f527ea284e1ba4f6553dce7","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"A how-to in getting started with Package Management in R using the packrat package.","email_subject":null},{"id":"5f527ea284e1ba4f6553dd54","post_id":"5f527ea284e1ba4f6553dcf1","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"Contrasting anonymous functions in R and Python. An overview of how to use them and conventions of how anonymous functions are used in the R and Python communities.","email_subject":null},{"id":"5f527ea284e1ba4f6553dd6c","post_id":"5f527ea284e1ba4f6553dcfb","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"How to use Shiny Containers with Shinyproxy","meta_description":"A how to in setting up shinyproxy as a production level way of deploying multiple containerised shiny as well as python apps with authentication. ","email_subject":null},{"id":"5f527ea384e1ba4f6553dd78","post_id":"5f527ea284e1ba4f6553dd02","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"A walkthrough of the GET request problems encountered when deploying a Dash app on Shinyproxy and the best solution to make it work.","email_subject":null},{"id":"5f527ea384e1ba4f6553dd9d","post_id":"5f527ea284e1ba4f6553dd1c","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Setting up Pi-Hole on a Raspberry Pi Zero W using SSH","meta_description":"A how-to guide on setting up Pi-Hole an ad blocking software on a Raspberry Pi Zero W","email_subject":null},{"id":"5f527ea384e1ba4f6553dd9f","post_id":"5f527ea284e1ba4f6553dd1d","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"A review of Wes Bos's free Javascript30 course and my own summary notes on what I learnt","email_subject":null},{"id":"5f527ea384e1ba4f6553dda9","post_id":"5f527ea284e1ba4f6553dd20","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"What is the best open source RSS Feed Reader? A how-to-install and review of the best readers out there.","email_subject":null}],"users":[{"id":"1","name":"Luke Singham","slug":"luke","password":"$2a$10$s8XIIHsQCDw/FKf64uy4H.brnh4sy2Bok8QA3AVVjjVVnQZJA.k9W","email":"lukesingham@gmail.com","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":null,"status":"active","locale":null,"visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2020-09-04 17:49:36","created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:49:36","updated_by":"1"},{"id":"5951f5fca366002ebd5dbef7","name":"Ghost","slug":"ghost","password":"$2a$10$kjs0oEP.pNnc/xI.EcmPaO.oRxsfdiiZv8rTeyobS6U.BShFHBEMy","email":"ghost-author@example.com","profile_image":"https://static.ghost.org/v3.0.0/images/ghost.png","cover_image":null,"bio":"You can delete this user to remove all the welcome posts","website":"https://ghost.org","location":"The Internet","facebook":"ghost","twitter":"ghost","accessibility":null,"status":"active","locale":null,"visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_seen":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"}],"posts_authors":[{"id":"5f527c0484e1ba4f6553db81","post_id":"5f527c0484e1ba4f6553db80","author_id":"5951f5fca366002ebd5dbef7","sort_order":0},{"id":"5f527c0484e1ba4f6553db83","post_id":"5f527c0484e1ba4f6553db82","author_id":"5951f5fca366002ebd5dbef7","sort_order":0},{"id":"5f527c0484e1ba4f6553db85","post_id":"5f527c0484e1ba4f6553db84","author_id":"5951f5fca366002ebd5dbef7","sort_order":0},{"id":"5f527c0484e1ba4f6553db87","post_id":"5f527c0484e1ba4f6553db86","author_id":"5951f5fca366002ebd5dbef7","sort_order":0},{"id":"5f527c0484e1ba4f6553db89","post_id":"5f527c0484e1ba4f6553db88","author_id":"5951f5fca366002ebd5dbef7","sort_order":0},{"id":"5f527c0484e1ba4f6553db8b","post_id":"5f527c0484e1ba4f6553db8a","author_id":"5951f5fca366002ebd5dbef7","sort_order":0},{"id":"5f527c0484e1ba4f6553db8d","post_id":"5f527c0484e1ba4f6553db8c","author_id":"5951f5fca366002ebd5dbef7","sort_order":0},{"id":"5f527ea284e1ba4f6553dd21","post_id":"5f527ea284e1ba4f6553dcdb","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd22","post_id":"5f527ea284e1ba4f6553dcdc","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd26","post_id":"5f527ea284e1ba4f6553dcdd","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd28","post_id":"5f527ea284e1ba4f6553dcde","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd2d","post_id":"5f527ea284e1ba4f6553dcdf","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd2f","post_id":"5f527ea284e1ba4f6553dce0","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd30","post_id":"5f527ea284e1ba4f6553dce1","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd31","post_id":"5f527ea284e1ba4f6553dce2","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd32","post_id":"5f527ea284e1ba4f6553dce3","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd33","post_id":"5f527ea284e1ba4f6553dce4","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd34","post_id":"5f527ea284e1ba4f6553dce5","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd3a","post_id":"5f527ea284e1ba4f6553dce6","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd3f","post_id":"5f527ea284e1ba4f6553dce7","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd41","post_id":"5f527ea284e1ba4f6553dce8","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd45","post_id":"5f527ea284e1ba4f6553dce9","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd49","post_id":"5f527ea284e1ba4f6553dcea","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd4b","post_id":"5f527ea284e1ba4f6553dceb","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd4c","post_id":"5f527ea284e1ba4f6553dcec","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd4d","post_id":"5f527ea284e1ba4f6553dced","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd4e","post_id":"5f527ea284e1ba4f6553dcee","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd4f","post_id":"5f527ea284e1ba4f6553dcef","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd50","post_id":"5f527ea284e1ba4f6553dcf0","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd53","post_id":"5f527ea284e1ba4f6553dcf1","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd55","post_id":"5f527ea284e1ba4f6553dcf2","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd56","post_id":"5f527ea284e1ba4f6553dcf3","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd57","post_id":"5f527ea284e1ba4f6553dcf4","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd5b","post_id":"5f527ea284e1ba4f6553dcf5","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd5c","post_id":"5f527ea284e1ba4f6553dcf6","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd5d","post_id":"5f527ea284e1ba4f6553dcf7","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd5e","post_id":"5f527ea284e1ba4f6553dcf8","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd5f","post_id":"5f527ea284e1ba4f6553dcf9","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd63","post_id":"5f527ea284e1ba4f6553dcfa","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd6b","post_id":"5f527ea284e1ba4f6553dcfb","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd6d","post_id":"5f527ea284e1ba4f6553dcfc","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd6e","post_id":"5f527ea284e1ba4f6553dcfd","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd6f","post_id":"5f527ea284e1ba4f6553dcfe","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd70","post_id":"5f527ea284e1ba4f6553dcff","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd71","post_id":"5f527ea284e1ba4f6553dd00","author_id":"1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd72","post_id":"5f527ea284e1ba4f6553dd01","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd77","post_id":"5f527ea284e1ba4f6553dd02","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd79","post_id":"5f527ea284e1ba4f6553dd03","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd7c","post_id":"5f527ea284e1ba4f6553dd04","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd7d","post_id":"5f527ea284e1ba4f6553dd05","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd7e","post_id":"5f527ea284e1ba4f6553dd06","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd7f","post_id":"5f527ea284e1ba4f6553dd07","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd80","post_id":"5f527ea284e1ba4f6553dd08","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd81","post_id":"5f527ea284e1ba4f6553dd09","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd82","post_id":"5f527ea284e1ba4f6553dd0a","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd83","post_id":"5f527ea284e1ba4f6553dd0b","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd84","post_id":"5f527ea284e1ba4f6553dd0c","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd85","post_id":"5f527ea284e1ba4f6553dd0d","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd86","post_id":"5f527ea284e1ba4f6553dd0e","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd87","post_id":"5f527ea284e1ba4f6553dd0f","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd88","post_id":"5f527ea284e1ba4f6553dd10","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd89","post_id":"5f527ea284e1ba4f6553dd11","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd8a","post_id":"5f527ea284e1ba4f6553dd12","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd8b","post_id":"5f527ea284e1ba4f6553dd13","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd8c","post_id":"5f527ea284e1ba4f6553dd14","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd8d","post_id":"5f527ea284e1ba4f6553dd15","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd8e","post_id":"5f527ea284e1ba4f6553dd16","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd8f","post_id":"5f527ea284e1ba4f6553dd17","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd90","post_id":"5f527ea284e1ba4f6553dd18","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd97","post_id":"5f527ea284e1ba4f6553dd19","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd98","post_id":"5f527ea284e1ba4f6553dd1a","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd99","post_id":"5f527ea284e1ba4f6553dd1b","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd9c","post_id":"5f527ea284e1ba4f6553dd1c","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dd9e","post_id":"5f527ea284e1ba4f6553dd1d","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dda0","post_id":"5f527ea284e1ba4f6553dd1e","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dda3","post_id":"5f527ea284e1ba4f6553dd1f","author_id":"1","sort_order":0},{"id":"5f527ea384e1ba4f6553dda8","post_id":"5f527ea284e1ba4f6553dd20","author_id":"1","sort_order":0}],"roles":[{"id":"5f527c0384e1ba4f6553db2c","name":"Administrator","description":"Administrators","created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db2d","name":"Editor","description":"Editors","created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db2e","name":"Author","description":"Authors","created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db2f","name":"Contributor","description":"Contributors","created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db30","name":"Owner","description":"Blog Owner","created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db31","name":"Admin Integration","description":"External Apps","created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db32","name":"DB Backup Integration","description":"Internal DB Backup Client","created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db33","name":"Scheduler Integration","description":"Internal Scheduler Client","created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"}],"roles_users":[{"id":"5f527c0484e1ba4f6553db7f","role_id":"5f527c0384e1ba4f6553db2e","user_id":"5951f5fca366002ebd5dbef7"},{"id":"5f527c0484e1ba4f6553dc63","role_id":"5f527c0384e1ba4f6553db30","user_id":"1"}],"permissions":[{"id":"5f527c0384e1ba4f6553db34","name":"Export database","object_type":"db","action_type":"exportContent","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db35","name":"Import database","object_type":"db","action_type":"importContent","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db36","name":"Delete all content","object_type":"db","action_type":"deleteAllContent","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db37","name":"Send mail","object_type":"mail","action_type":"send","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db38","name":"Browse notifications","object_type":"notification","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db39","name":"Add notifications","object_type":"notification","action_type":"add","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db3a","name":"Delete notifications","object_type":"notification","action_type":"destroy","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db3b","name":"Browse posts","object_type":"post","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db3c","name":"Read posts","object_type":"post","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db3d","name":"Edit posts","object_type":"post","action_type":"edit","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db3e","name":"Add posts","object_type":"post","action_type":"add","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db3f","name":"Delete posts","object_type":"post","action_type":"destroy","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db40","name":"Browse settings","object_type":"setting","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db41","name":"Read settings","object_type":"setting","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db42","name":"Edit settings","object_type":"setting","action_type":"edit","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db43","name":"Generate slugs","object_type":"slug","action_type":"generate","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db44","name":"Browse tags","object_type":"tag","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db45","name":"Read tags","object_type":"tag","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db46","name":"Edit tags","object_type":"tag","action_type":"edit","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db47","name":"Add tags","object_type":"tag","action_type":"add","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db48","name":"Delete tags","object_type":"tag","action_type":"destroy","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db49","name":"Browse themes","object_type":"theme","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db4a","name":"Edit themes","object_type":"theme","action_type":"edit","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db4b","name":"Activate themes","object_type":"theme","action_type":"activate","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db4c","name":"Upload themes","object_type":"theme","action_type":"add","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db4d","name":"Download themes","object_type":"theme","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db4e","name":"Delete themes","object_type":"theme","action_type":"destroy","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db4f","name":"Browse users","object_type":"user","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db50","name":"Read users","object_type":"user","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db51","name":"Edit users","object_type":"user","action_type":"edit","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db52","name":"Add users","object_type":"user","action_type":"add","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db53","name":"Delete users","object_type":"user","action_type":"destroy","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db54","name":"Assign a role","object_type":"role","action_type":"assign","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db55","name":"Browse roles","object_type":"role","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db56","name":"Browse invites","object_type":"invite","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db57","name":"Read invites","object_type":"invite","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db58","name":"Edit invites","object_type":"invite","action_type":"edit","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db59","name":"Add invites","object_type":"invite","action_type":"add","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db5a","name":"Delete invites","object_type":"invite","action_type":"destroy","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db5b","name":"Download redirects","object_type":"redirect","action_type":"download","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0384e1ba4f6553db5c","name":"Upload redirects","object_type":"redirect","action_type":"upload","object_id":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db5d","name":"Add webhooks","object_type":"webhook","action_type":"add","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db5e","name":"Edit webhooks","object_type":"webhook","action_type":"edit","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db5f","name":"Delete webhooks","object_type":"webhook","action_type":"destroy","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db60","name":"Browse integrations","object_type":"integration","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db61","name":"Read integrations","object_type":"integration","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db62","name":"Edit integrations","object_type":"integration","action_type":"edit","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db63","name":"Add integrations","object_type":"integration","action_type":"add","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db64","name":"Delete integrations","object_type":"integration","action_type":"destroy","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db65","name":"Browse API keys","object_type":"api_key","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db66","name":"Read API keys","object_type":"api_key","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db67","name":"Edit API keys","object_type":"api_key","action_type":"edit","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db68","name":"Add API keys","object_type":"api_key","action_type":"add","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db69","name":"Delete API keys","object_type":"api_key","action_type":"destroy","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db6a","name":"Browse Actions","object_type":"action","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db6b","name":"Browse Members","object_type":"member","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db6c","name":"Read Members","object_type":"member","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db6d","name":"Edit Members","object_type":"member","action_type":"edit","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db6e","name":"Add Members","object_type":"member","action_type":"add","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db6f","name":"Delete Members","object_type":"member","action_type":"destroy","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db70","name":"Publish posts","object_type":"post","action_type":"publish","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db71","name":"Backup database","object_type":"db","action_type":"backupContent","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db72","name":"Email preview","object_type":"email_preview","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db73","name":"Send test email","object_type":"email_preview","action_type":"sendTestEmail","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db74","name":"Browse emails","object_type":"email","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db75","name":"Read emails","object_type":"email","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db76","name":"Retry emails","object_type":"email","action_type":"retry","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db77","name":"Browse labels","object_type":"label","action_type":"browse","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db78","name":"Read labels","object_type":"label","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db79","name":"Edit labels","object_type":"label","action_type":"edit","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db7a","name":"Add labels","object_type":"label","action_type":"add","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db7b","name":"Delete labels","object_type":"label","action_type":"destroy","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db7c","name":"Read member signin urls","object_type":"member_signin_url","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db7d","name":"Read identities","object_type":"identity","action_type":"read","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db7e","name":"Auth Stripe Connect for Members","object_type":"members_stripe_connect","action_type":"auth","object_id":null,"created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"}],"permissions_users":[],"permissions_roles":[{"id":"5f527c0484e1ba4f6553db94","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db34"},{"id":"5f527c0484e1ba4f6553db95","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db35"},{"id":"5f527c0484e1ba4f6553db96","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db36"},{"id":"5f527c0484e1ba4f6553db97","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db71"},{"id":"5f527c0484e1ba4f6553db98","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db37"},{"id":"5f527c0484e1ba4f6553db99","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db38"},{"id":"5f527c0484e1ba4f6553db9b","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db3a"},{"id":"5f527c0484e1ba4f6553db9a","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db39"},{"id":"5f527c0484e1ba4f6553db9c","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db3b"},{"id":"5f527c0484e1ba4f6553db9e","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db3d"},{"id":"5f527c0484e1ba4f6553dba1","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db70"},{"id":"5f527c0484e1ba4f6553db9f","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db3e"},{"id":"5f527c0484e1ba4f6553dba0","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db3f"},{"id":"5f527c0484e1ba4f6553db9d","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db3c"},{"id":"5f527c0484e1ba4f6553dba2","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db40"},{"id":"5f527c0484e1ba4f6553dba4","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db42"},{"id":"5f527c0484e1ba4f6553dba3","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db41"},{"id":"5f527c0484e1ba4f6553dba5","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db43"},{"id":"5f527c0484e1ba4f6553dba6","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db44"},{"id":"5f527c0484e1ba4f6553dba8","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db46"},{"id":"5f527c0484e1ba4f6553dba7","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db45"},{"id":"5f527c0484e1ba4f6553dba9","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db47"},{"id":"5f527c0484e1ba4f6553dbaa","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db48"},{"id":"5f527c0484e1ba4f6553dbab","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db49"},{"id":"5f527c0484e1ba4f6553dbae","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db4c"},{"id":"5f527c0484e1ba4f6553dbb0","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db4e"},{"id":"5f527c0484e1ba4f6553dbad","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db4b"},{"id":"5f527c0484e1ba4f6553dbaf","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db4d"},{"id":"5f527c0484e1ba4f6553dbac","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db4a"},{"id":"5f527c0484e1ba4f6553dbb1","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db4f"},{"id":"5f527c0484e1ba4f6553dbb4","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db52"},{"id":"5f527c0484e1ba4f6553dbb2","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db50"},{"id":"5f527c0484e1ba4f6553dbb5","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db53"},{"id":"5f527c0484e1ba4f6553dbb3","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db51"},{"id":"5f527c0484e1ba4f6553dbb6","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db54"},{"id":"5f527c0484e1ba4f6553dbb7","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db55"},{"id":"5f527c0484e1ba4f6553dbb8","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db56"},{"id":"5f527c0484e1ba4f6553dbbc","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db5a"},{"id":"5f527c0484e1ba4f6553dbb9","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db57"},{"id":"5f527c0484e1ba4f6553dbba","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db58"},{"id":"5f527c0484e1ba4f6553dbbb","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db59"},{"id":"5f527c0484e1ba4f6553dbbd","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db5b"},{"id":"5f527c0484e1ba4f6553dbbe","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0384e1ba4f6553db5c"},{"id":"5f527c0484e1ba4f6553dbbf","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db5d"},{"id":"5f527c0484e1ba4f6553dbc0","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db5e"},{"id":"5f527c0484e1ba4f6553dbc1","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db5f"},{"id":"5f527c0484e1ba4f6553dbc3","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db61"},{"id":"5f527c0484e1ba4f6553dbc6","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db64"},{"id":"5f527c0484e1ba4f6553dbc4","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db62"},{"id":"5f527c0484e1ba4f6553dbc5","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db63"},{"id":"5f527c0484e1ba4f6553dbc2","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db60"},{"id":"5f527c0484e1ba4f6553dbca","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db68"},{"id":"5f527c0484e1ba4f6553dbcb","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db69"},{"id":"5f527c0484e1ba4f6553dbc9","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db67"},{"id":"5f527c0484e1ba4f6553dbc7","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db65"},{"id":"5f527c0484e1ba4f6553dbc8","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db66"},{"id":"5f527c0484e1ba4f6553dbcc","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db6a"},{"id":"5f527c0484e1ba4f6553dbcd","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db6b"},{"id":"5f527c0484e1ba4f6553dbce","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db6c"},{"id":"5f527c0484e1ba4f6553dbd1","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db6f"},{"id":"5f527c0484e1ba4f6553dbd0","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db6e"},{"id":"5f527c0484e1ba4f6553dbcf","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db6d"},{"id":"5f527c0484e1ba4f6553dbd5","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db7a"},{"id":"5f527c0484e1ba4f6553dbd3","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db78"},{"id":"5f527c0484e1ba4f6553dbd6","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db7b"},{"id":"5f527c0484e1ba4f6553dbd4","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db79"},{"id":"5f527c0484e1ba4f6553dbd2","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db77"},{"id":"5f527c0484e1ba4f6553dbd7","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db72"},{"id":"5f527c0484e1ba4f6553dbd8","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db73"},{"id":"5f527c0484e1ba4f6553dbda","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db75"},{"id":"5f527c0484e1ba4f6553dbd9","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db74"},{"id":"5f527c0484e1ba4f6553dbdb","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db76"},{"id":"5f527c0484e1ba4f6553dbdc","role_id":"5f527c0384e1ba4f6553db2c","permission_id":"5f527c0484e1ba4f6553db7c"},{"id":"5f527c0484e1ba4f6553dbdd","role_id":"5f527c0384e1ba4f6553db32","permission_id":"5f527c0384e1ba4f6553db34"},{"id":"5f527c0484e1ba4f6553dbde","role_id":"5f527c0384e1ba4f6553db32","permission_id":"5f527c0384e1ba4f6553db35"},{"id":"5f527c0484e1ba4f6553dbdf","role_id":"5f527c0384e1ba4f6553db32","permission_id":"5f527c0384e1ba4f6553db36"},{"id":"5f527c0484e1ba4f6553dbe0","role_id":"5f527c0384e1ba4f6553db32","permission_id":"5f527c0484e1ba4f6553db71"},{"id":"5f527c0484e1ba4f6553dbe1","role_id":"5f527c0384e1ba4f6553db33","permission_id":"5f527c0484e1ba4f6553db70"},{"id":"5f527c0484e1ba4f6553dbe2","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db37"},{"id":"5f527c0484e1ba4f6553dbe3","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db38"},{"id":"5f527c0484e1ba4f6553dbe4","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db39"},{"id":"5f527c0484e1ba4f6553dbe5","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db3a"},{"id":"5f527c0484e1ba4f6553dbe6","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db3b"},{"id":"5f527c0484e1ba4f6553dbe8","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db3d"},{"id":"5f527c0484e1ba4f6553dbeb","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db70"},{"id":"5f527c0484e1ba4f6553dbea","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db3f"},{"id":"5f527c0484e1ba4f6553dbe9","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db3e"},{"id":"5f527c0484e1ba4f6553dbe7","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db3c"},{"id":"5f527c0484e1ba4f6553dbec","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db40"},{"id":"5f527c0484e1ba4f6553dbed","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db41"},{"id":"5f527c0484e1ba4f6553dbee","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db42"},{"id":"5f527c0484e1ba4f6553dbef","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db43"},{"id":"5f527c0484e1ba4f6553dbf0","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db44"},{"id":"5f527c0484e1ba4f6553dbf1","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db45"},{"id":"5f527c0484e1ba4f6553dbf2","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db46"},{"id":"5f527c0484e1ba4f6553dbf3","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db47"},{"id":"5f527c0484e1ba4f6553dbf4","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db48"},{"id":"5f527c0484e1ba4f6553dbf6","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db4a"},{"id":"5f527c0484e1ba4f6553dbf5","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db49"},{"id":"5f527c0484e1ba4f6553dbf7","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db4b"},{"id":"5f527c0484e1ba4f6553dbf8","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db4c"},{"id":"5f527c0484e1ba4f6553dbfa","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db4e"},{"id":"5f527c0484e1ba4f6553dbf9","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db4d"},{"id":"5f527c0484e1ba4f6553dbfb","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db4f"},{"id":"5f527c0484e1ba4f6553dbfc","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db50"},{"id":"5f527c0484e1ba4f6553dbfd","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db51"},{"id":"5f527c0484e1ba4f6553dbff","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db53"},{"id":"5f527c0484e1ba4f6553dbfe","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db52"},{"id":"5f527c0484e1ba4f6553dc00","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db54"},{"id":"5f527c0484e1ba4f6553dc01","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db55"},{"id":"5f527c0484e1ba4f6553dc02","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db56"},{"id":"5f527c0484e1ba4f6553dc04","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db58"},{"id":"5f527c0484e1ba4f6553dc05","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db59"},{"id":"5f527c0484e1ba4f6553dc03","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db57"},{"id":"5f527c0484e1ba4f6553dc06","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db5a"},{"id":"5f527c0484e1ba4f6553dc07","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db5b"},{"id":"5f527c0484e1ba4f6553dc08","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0384e1ba4f6553db5c"},{"id":"5f527c0484e1ba4f6553dc09","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db5d"},{"id":"5f527c0484e1ba4f6553dc0a","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db5e"},{"id":"5f527c0484e1ba4f6553dc0b","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db5f"},{"id":"5f527c0484e1ba4f6553dc0c","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db6a"},{"id":"5f527c0484e1ba4f6553dc0d","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db6b"},{"id":"5f527c0484e1ba4f6553dc10","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db6e"},{"id":"5f527c0484e1ba4f6553dc0f","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db6d"},{"id":"5f527c0484e1ba4f6553dc11","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db6f"},{"id":"5f527c0484e1ba4f6553dc0e","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db6c"},{"id":"5f527c0484e1ba4f6553dc12","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db77"},{"id":"5f527c0484e1ba4f6553dc14","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db79"},{"id":"5f527c0484e1ba4f6553dc15","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db7a"},{"id":"5f527c0484e1ba4f6553dc13","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db78"},{"id":"5f527c0484e1ba4f6553dc16","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db7b"},{"id":"5f527c0484e1ba4f6553dc17","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db72"},{"id":"5f527c0484e1ba4f6553dc18","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db73"},{"id":"5f527c0484e1ba4f6553dc19","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db74"},{"id":"5f527c0484e1ba4f6553dc1a","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db75"},{"id":"5f527c0484e1ba4f6553dc1b","role_id":"5f527c0384e1ba4f6553db31","permission_id":"5f527c0484e1ba4f6553db76"},{"id":"5f527c0484e1ba4f6553dc1c","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db38"},{"id":"5f527c0484e1ba4f6553dc1d","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db39"},{"id":"5f527c0484e1ba4f6553dc1e","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db3a"},{"id":"5f527c0484e1ba4f6553dc1f","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db3b"},{"id":"5f527c0484e1ba4f6553dc22","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db3e"},{"id":"5f527c0484e1ba4f6553dc23","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db3f"},{"id":"5f527c0484e1ba4f6553dc21","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db3d"},{"id":"5f527c0484e1ba4f6553dc24","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0484e1ba4f6553db70"},{"id":"5f527c0484e1ba4f6553dc20","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db3c"},{"id":"5f527c0484e1ba4f6553dc25","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db40"},{"id":"5f527c0484e1ba4f6553dc26","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db41"},{"id":"5f527c0484e1ba4f6553dc27","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db43"},{"id":"5f527c0484e1ba4f6553dc28","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db44"},{"id":"5f527c0484e1ba4f6553dc2c","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db48"},{"id":"5f527c0484e1ba4f6553dc2a","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db46"},{"id":"5f527c0484e1ba4f6553dc2b","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db47"},{"id":"5f527c0484e1ba4f6553dc29","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db45"},{"id":"5f527c0484e1ba4f6553dc2d","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db4f"},{"id":"5f527c0484e1ba4f6553dc2f","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db51"},{"id":"5f527c0484e1ba4f6553dc30","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db52"},{"id":"5f527c0484e1ba4f6553dc31","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db53"},{"id":"5f527c0484e1ba4f6553dc2e","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db50"},{"id":"5f527c0484e1ba4f6553dc32","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db54"},{"id":"5f527c0484e1ba4f6553dc33","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db55"},{"id":"5f527c0484e1ba4f6553dc37","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db59"},{"id":"5f527c0484e1ba4f6553dc35","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db57"},{"id":"5f527c0484e1ba4f6553dc36","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db58"},{"id":"5f527c0484e1ba4f6553dc34","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db56"},{"id":"5f527c0484e1ba4f6553dc38","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db5a"},{"id":"5f527c0484e1ba4f6553dc39","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0384e1ba4f6553db49"},{"id":"5f527c0484e1ba4f6553dc3a","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0484e1ba4f6553db72"},{"id":"5f527c0484e1ba4f6553dc3b","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0484e1ba4f6553db73"},{"id":"5f527c0484e1ba4f6553dc3c","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0484e1ba4f6553db74"},{"id":"5f527c0484e1ba4f6553dc3d","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0484e1ba4f6553db75"},{"id":"5f527c0484e1ba4f6553dc3e","role_id":"5f527c0384e1ba4f6553db2d","permission_id":"5f527c0484e1ba4f6553db76"},{"id":"5f527c0484e1ba4f6553dc3f","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0384e1ba4f6553db3b"},{"id":"5f527c0484e1ba4f6553dc40","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0384e1ba4f6553db3c"},{"id":"5f527c0484e1ba4f6553dc41","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0384e1ba4f6553db3e"},{"id":"5f527c0484e1ba4f6553dc42","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0384e1ba4f6553db40"},{"id":"5f527c0484e1ba4f6553dc43","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0384e1ba4f6553db41"},{"id":"5f527c0484e1ba4f6553dc44","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0384e1ba4f6553db43"},{"id":"5f527c0484e1ba4f6553dc45","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0384e1ba4f6553db44"},{"id":"5f527c0484e1ba4f6553dc46","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0384e1ba4f6553db45"},{"id":"5f527c0484e1ba4f6553dc47","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0384e1ba4f6553db47"},{"id":"5f527c0484e1ba4f6553dc48","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0384e1ba4f6553db4f"},{"id":"5f527c0484e1ba4f6553dc49","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0384e1ba4f6553db50"},{"id":"5f527c0484e1ba4f6553dc4a","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0384e1ba4f6553db55"},{"id":"5f527c0484e1ba4f6553dc4b","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0384e1ba4f6553db49"},{"id":"5f527c0484e1ba4f6553dc4c","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0484e1ba4f6553db72"},{"id":"5f527c0484e1ba4f6553dc4d","role_id":"5f527c0384e1ba4f6553db2e","permission_id":"5f527c0484e1ba4f6553db75"},{"id":"5f527c0484e1ba4f6553dc4e","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0384e1ba4f6553db3b"},{"id":"5f527c0484e1ba4f6553dc4f","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0384e1ba4f6553db3c"},{"id":"5f527c0484e1ba4f6553dc50","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0384e1ba4f6553db3e"},{"id":"5f527c0484e1ba4f6553dc51","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0384e1ba4f6553db40"},{"id":"5f527c0484e1ba4f6553dc52","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0384e1ba4f6553db41"},{"id":"5f527c0484e1ba4f6553dc53","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0384e1ba4f6553db43"},{"id":"5f527c0484e1ba4f6553dc54","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0384e1ba4f6553db44"},{"id":"5f527c0484e1ba4f6553dc55","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0384e1ba4f6553db45"},{"id":"5f527c0484e1ba4f6553dc56","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0384e1ba4f6553db4f"},{"id":"5f527c0484e1ba4f6553dc57","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0384e1ba4f6553db50"},{"id":"5f527c0484e1ba4f6553dc58","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0384e1ba4f6553db55"},{"id":"5f527c0484e1ba4f6553dc59","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0384e1ba4f6553db49"},{"id":"5f527c0484e1ba4f6553dc5a","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0484e1ba4f6553db72"},{"id":"5f527c0484e1ba4f6553dc5b","role_id":"5f527c0384e1ba4f6553db2f","permission_id":"5f527c0484e1ba4f6553db75"}],"permissions_apps":[],"settings":[{"id":"5f527c0584e1ba4f6553dc64","group":"core","key":"db_hash","value":"e520efb9-db54-4e63-9c74-e637fa569e3a","type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc65","group":"core","key":"next_update_check","value":"1599328116","type":"number","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:48:36","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc66","group":"core","key":"notifications","value":"[]","type":"array","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc67","group":"core","key":"session_secret","value":"ff618311c5439378665907efb7dae886c614c4b407f626d6a97921e6e8f06671","type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc68","group":"core","key":"theme_session_secret","value":"0dbbdbe1e7577f40f7a8e2cd538252bd5fc93e3003f46677c0718abfdd758c48","type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc69","group":"core","key":"ghost_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAPsCsI3V8H8mlnHd/US29Nl9w5ErzZWViZ/zACvqkBCCQ5VJMJM/alyW5VN+fRmZ\nvsWphTfHVph2S1wWfeBKpRgsmTUTjneJRDsbKOhb8OX8Pr/lD2YyYb9w+f1fS5aCv+Tjur2e\nuYcJLD/b9Wfxs82a8SpPzphcSGExhijBNPLnAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc6a","group":"core","key":"ghost_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICXgIBAAKBgQD7ArCN1fB/JpZx3f1EtvTZfcORK82VlYmf8wAr6pAQgkOVSTCTP2pcluVT\nfn0Zmb7FqYU3x1aYdktcFn3gSqUYLJk1E453iUQ7GyjoW/Dl/D6/5Q9mMmG/cPn9X0uWgr/k\n47q9nrmHCSw/2/Vn8bPNmvEqT86YXEhhMYYowTTy5wIDAQABAoGBAIx5m9803Mv8B3QgPS5o\niazyMjGbZwmQz52hO/ZX5lom3LW+aR3u/pJkrGDGyDI8lcQ/M9vchjz67v/RYo7Ul4FNLcZx\njusUVjFaLkNwizbm5qH2jposljaSorS1Cck2JVk8fIfVvSVJVD1YGdz4uBN7a2Np5eRfdPqB\n5vWDRIN5AkEA/8MgrkrK8NWcVD0YIhVpNVSSoPC59M0v8P4v0mkVJ1MTNAKtnRNU2VkYQYfF\nH/X/bC6wPJp1JEZ7ZmYaNGhaDQJBAPs+bltGDo5NbRzfq8ysia6USmtrXooeYGhugBxUzHJk\nUKFOpovGC4BMgG700HTcNxflA6q0yASB80IoBmHKB8MCQGUNjc4PkxuQ6tGFyhE9cHI31i9s\nrayBnd2UMQTrrAX7luzH2hzj1HSD9Ud/A23bXm46DDwFsx8AEiWlVdk7GckCQQD2g2AmYEvB\n5khMD05y6UvCJrAKS/Qb0whHDzzWD4H2L2VJkHErLat1PBq8Q6sRLk1EPalmYjDq81JqM/ln\nE6KfAkEAn7n0Xi88PbrGCSJchYeIP80UTasVMqWs3Tvxomkovi2ecEI3nypK/id67i/npzs1\nNTj4nG7/qS72tXB4mX3C5Q==\n-----END RSA PRIVATE KEY-----\n","type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc6b","group":"core","key":"members_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAISCL/J5O5aEqhmdqUkMPHWfX10rtPjpH9E8I93ojo49BIvNKeQkSZXL+to8CqRH\nk4sDfii/QNgdc5KwEPGMDFUiQFJMz+t3XwqzEE7K/K1e+OWi9pjhCIM6i1Lj3KOoMM4Kk8rd\nXeZ8HPVqP6VlNdv53vPCiBzomFh4z9Ik96hxAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc6c","group":"core","key":"members_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICXAIBAAKBgQCEgi/yeTuWhKoZnalJDDx1n19dK7T46R/RPCPd6I6OPQSLzSnkJEmVy/ra\nPAqkR5OLA34ov0DYHXOSsBDxjAxVIkBSTM/rd18KsxBOyvytXvjlovaY4QiDOotS49yjqDDO\nCpPK3V3mfBz1aj+lZTXb+d7zwogc6JhYeM/SJPeocQIDAQABAoGATbOBpi/M72Bv1oGLKRiR\nw29nHl1oy6T4UVtwtMmP/nxtVAVEkVFuDPcF7D7VBqwqKpQrwH0V6ySqBtRTuzCicdNhTe5T\ndTnvhlbair1B/ge0K4WuDkDE4R119hHUuReKqQS4PK8FtgOJWcegEIVBDP5W3xhDn9qgcuXC\nmrwjLTkCQQDp/EejlKmqx/0rqFx4c/vuIZBvJzVHJIMOaMyB5tF9e5014A+VA49+N16Q8qT9\nTCjgtFDcoj2A3yo4VUM2ZqCXAkEAkPnB8m6DZ8HUt6tL8nGz10efo3jsebHk2EXwva7sbK8+\nt60nbgOgl7bX+mVeRTx1wBO5g0bgSGYumMy7F6sYNwJBAMT6wUSQ7pT7b8LFJbOx0LEUSIeK\ntmuB7WLADwp8ynedDShBzFxidvaGnnHgE825w+AHbNuRKb3IIb7y6HZvx1kCQF2U2tzi+W/A\nbs8gg6W8P5p3YVTkecEUU+RHQV8vyu1dMd7eptUdAkzsef3n8TJAFMIn+VgEKEKTswBvJ1Xf\n24ECQFyjR4k/mO76o2Xn6Cga1XpgFLMS5dX8rEL7cb8LqaQLw8ehefHIMYZ55qWVuj5eNGS1\n5ttuXuFfjODOiLX/Qa4=\n-----END RSA PRIVATE KEY-----\n","type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc6d","group":"core","key":"members_email_auth_secret","value":"7af75f224ff93319f9272de1732d31f2e2058477288e5427be57693a688f5dee3b3d2c5651aa1e183c0a7902abca8df4025c111f82f8a77545d8d7b0ab1b6b7f","type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc6e","group":"core","key":"members_stripe_webhook_id","value":null,"type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc6f","group":"core","key":"members_stripe_webhook_secret","value":null,"type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc70","group":"site","key":"title","value":"read and write","type":"string","flags":"PUBLIC","created_at":"2015-04-11 12:34:58","created_by":"1","updated_at":"2017-12-14 14:39:39","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc71","group":"site","key":"description","value":"Economics and Data Science amongst other things","type":"string","flags":"PUBLIC","created_at":"2015-04-11 12:34:58","created_by":"1","updated_at":"2017-12-14 14:39:39","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc72","group":"site","key":"logo","value":null,"type":"string","flags":null,"created_at":"2015-04-11 12:34:58","created_by":"1","updated_at":"2017-12-14 14:39:39","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc73","group":"site","key":"cover_image","value":"/content/images/2017/07/IMG_20170527_172339-PANO-1.jpg","type":"string","flags":null,"created_at":"2015-04-11 12:34:58","created_by":"1","updated_at":"2017-12-14 14:39:39","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc74","group":"site","key":"icon","value":null,"type":"string","flags":null,"created_at":"2019-12-23 21:42:00","created_by":"1","updated_at":"2019-12-23 21:42:00","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc75","group":"site","key":"accent_color","value":"","type":"string","flags":"PUBLIC","created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc76","group":"site","key":"lang","value":"en","type":"string","flags":null,"created_at":"2019-12-23 21:42:00","created_by":"1","updated_at":"2019-12-23 21:42:00","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc77","group":"site","key":"timezone","value":"Etc/UTC","type":"string","flags":null,"created_at":"2019-12-23 21:42:00","created_by":"1","updated_at":"2019-12-23 21:42:00","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc78","group":"site","key":"codeinjection_head","value":"<script>\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n\n  ga('create', 'UA-64426512-1', 'auto');\n  ga('send', 'pageview');\n\n</script>\n\n<!-- Agate theme -->  \n<link rel=\"stylesheet\" href=\"//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/agate.min.css\">  \n<script src=\"//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js\"></script> \n\n<script src=\"//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js\"></script>\n\n<script src=\"//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js\"></script>\n\n<!-- css emojis  --> \n<link href=\"https://afeld.github.io/emoji-css/emoji.css\" rel=\"stylesheet\">","type":"string","flags":null,"created_at":"2015-04-11 12:34:58","created_by":"1","updated_at":"2017-12-14 14:39:39","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc79","group":"site","key":"codeinjection_foot","value":"<script>\n// https://highlightjs.org/\nactivateHighlight = function() {  \n    document.querySelectorAll(\"pre code\").forEach(\n        function(currentValue, currentIndex, listObj) {\n            hljs.highlightBlock(currentValue);\n        }\n    );\n}\n\nif (window.attachEvent) {  \n    window.attachEvent('onload', loadHighlight);\n} else {\n    if (window.onload) {\n        var originalOnload = window.onload;\n        var newOnload = function(evt) {\n            originalOnload(evt);\n            activateHighlight(evt);\n        };\n        window.onload = newOnload;\n    } else {\n        window.onload = activateHighlight;\n    }\n}\n</script>","type":"string","flags":null,"created_at":"2015-04-11 12:34:58","created_by":"1","updated_at":"2017-12-14 14:39:39","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc7a","group":"site","key":"facebook","value":null,"type":"string","flags":null,"created_at":"2019-12-23 21:42:00","created_by":"1","updated_at":"2019-12-23 21:42:00","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc7b","group":"site","key":"twitter","value":null,"type":"string","flags":null,"created_at":"2019-12-23 21:42:00","created_by":"1","updated_at":"2019-12-23 21:42:00","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc7c","group":"site","key":"navigation","value":"[{\"label\":\"Home\",\"url\":\"http://lukesingham.com/\"},{\"label\":\"About\",\"url\":\"http://lukesingham.com/about/\"}]","type":"array","flags":null,"created_at":"2015-04-11 12:34:58","created_by":"1","updated_at":"2017-12-14 14:39:39","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc7d","group":"site","key":"secondary_navigation","value":"[]","type":"array","flags":null,"created_at":"2020-01-02 13:36:03","created_by":"1","updated_at":"2020-01-02 13:36:03","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc7e","group":"site","key":"meta_title","value":null,"type":"string","flags":null,"created_at":"2020-01-02 13:36:03","created_by":"1","updated_at":"2020-01-02 13:36:03","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc7f","group":"site","key":"meta_description","value":null,"type":"string","flags":null,"created_at":"2020-01-02 13:36:03","created_by":"1","updated_at":"2020-01-02 13:36:03","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc80","group":"site","key":"og_image","value":null,"type":"string","flags":null,"created_at":"2020-01-02 13:36:03","created_by":"1","updated_at":"2020-01-02 13:36:03","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc81","group":"site","key":"og_title","value":null,"type":"string","flags":null,"created_at":"2020-01-02 13:36:03","created_by":"1","updated_at":"2020-01-02 13:36:03","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc82","group":"site","key":"og_description","value":null,"type":"string","flags":null,"created_at":"2020-01-02 13:36:03","created_by":"1","updated_at":"2020-01-02 13:36:03","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc83","group":"site","key":"twitter_image","value":null,"type":"string","flags":null,"created_at":"2020-01-02 13:36:03","created_by":"1","updated_at":"2020-01-02 13:36:03","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc84","group":"site","key":"twitter_title","value":null,"type":"string","flags":null,"created_at":"2020-01-02 13:36:03","created_by":"1","updated_at":"2020-01-02 13:36:03","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc85","group":"site","key":"twitter_description","value":null,"type":"string","flags":null,"created_at":"2020-01-02 13:36:03","created_by":"1","updated_at":"2020-01-02 13:36:03","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc86","group":"theme","key":"active_theme","value":"casper","type":"string","flags":"RO","created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc87","group":"private","key":"is_private","value":"false","type":"boolean","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc88","group":"private","key":"password","value":"","type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc89","group":"private","key":"public_hash","value":"d81830dff85ae14925a32515b9e421","type":"string","flags":null,"created_at":"2019-12-23 21:42:00","created_by":"1","updated_at":"2019-12-23 21:42:00","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc8a","group":"members","key":"default_content_visibility","value":"public","type":"string","flags":null,"created_at":"2020-01-02 13:36:03","created_by":"1","updated_at":"2020-01-02 13:36:03","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc8b","group":"members","key":"members_allow_free_signup","value":"true","type":"boolean","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc8c","group":"members","key":"members_from_address","value":"noreply","type":"string","flags":"RO","created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc8d","group":"members","key":"stripe_product_name","value":"Ghost Subscription","type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc8e","group":"members","key":"stripe_secret_key","value":null,"type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc8f","group":"members","key":"stripe_publishable_key","value":null,"type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc90","group":"members","key":"stripe_plans","value":"[{\"name\":\"Monthly\",\"currency\":\"usd\",\"interval\":\"month\",\"amount\":500},{\"name\":\"Yearly\",\"currency\":\"usd\",\"interval\":\"year\",\"amount\":5000}]","type":"array","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc91","group":"members","key":"stripe_connect_publishable_key","value":null,"type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc92","group":"members","key":"stripe_connect_secret_key","value":null,"type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc93","group":"members","key":"stripe_connect_livemode","value":null,"type":"boolean","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc94","group":"members","key":"stripe_connect_display_name","value":null,"type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc95","group":"members","key":"stripe_connect_account_id","value":null,"type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc96","group":"portal","key":"portal_name","value":"true","type":"boolean","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc97","group":"portal","key":"portal_button","value":"true","type":"boolean","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc98","group":"portal","key":"portal_plans","value":"[\"free\", \"monthly\", \"yearly\"]","type":"array","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc99","group":"portal","key":"portal_button_style","value":"icon-and-text","type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc9a","group":"portal","key":"portal_button_icon","value":null,"type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc9b","group":"portal","key":"portal_button_signup_text","value":"Subscribe","type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc9c","group":"email","key":"mailgun_domain","value":null,"type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc9d","group":"email","key":"mailgun_api_key","value":null,"type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc9e","group":"email","key":"mailgun_base_url","value":null,"type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dc9f","group":"amp","key":"amp","value":"true","type":"boolean","flags":null,"created_at":"2019-12-23 21:42:00","created_by":"1","updated_at":"2019-12-23 21:42:00","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dca0","group":"amp","key":"amp_gtag_id","value":null,"type":"string","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dca1","group":"labs","key":"labs","value":"{\"codeInjectionUI\":true,\"publicAPI\":true}","type":"object","flags":null,"created_at":"2015-04-11 12:34:58","created_by":"1","updated_at":"2017-12-14 14:39:39","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dca2","group":"slack","key":"slack","value":"[{\"url\":\"\"}]","type":"array","flags":null,"created_at":"2019-12-23 21:42:00","created_by":"1","updated_at":"2019-12-23 21:42:00","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dca3","group":"unsplash","key":"unsplash","value":"{\"isActive\": true}","type":"object","flags":null,"created_at":"2019-12-23 21:42:00","created_by":"1","updated_at":"2019-12-23 21:42:00","updated_by":"1"},{"id":"5f527c0584e1ba4f6553dca4","group":"views","key":"shared_views","value":"[]","type":"array","flags":null,"created_at":"2020-09-04 17:40:21","created_by":"1","updated_at":"2020-09-04 17:40:21","updated_by":"1"}],"tags":[{"id":"5f527c0384e1ba4f6553db2b","name":"Getting Started","slug":"getting-started","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-09-04 17:40:19","created_by":"1","updated_at":"2020-09-04 17:40:19","updated_by":"1"},{"id":"5f527ea284e1ba4f6553dcb3","name":"machine-learning","slug":"machine-learning","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:39:08","created_by":"1","updated_at":"2017-08-07 04:39:08","updated_by":null},{"id":"5f527ea284e1ba4f6553dcb1","name":"r","slug":"r","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:39:09","created_by":"1","updated_at":"2017-08-07 04:39:09","updated_by":null},{"id":"5f527ea284e1ba4f6553dcb0","name":"r machine-learning","slug":"r-machine-learning","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:38:36","created_by":"1","updated_at":"2017-08-07 04:38:36","updated_by":null},{"id":"5f527ea284e1ba4f6553dcb6","name":"javascript","slug":"javascript","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:42:30","created_by":"1","updated_at":"2017-08-07 04:42:30","updated_by":null},{"id":"5f527ea284e1ba4f6553dcb7","name":"geospatial","slug":"geospatial","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:42:30","created_by":"1","updated_at":"2017-08-07 04:42:30","updated_by":null},{"id":"5f527ea284e1ba4f6553dcb5","name":"text-mining","slug":"text-mining","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:40:41","created_by":"1","updated_at":"2017-08-07 04:40:41","updated_by":null},{"id":"5f527ea284e1ba4f6553dcb9","name":"package-management","slug":"package-management","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:43:32","created_by":"1","updated_at":"2017-08-07 04:43:32","updated_by":null},{"id":"5f527ea284e1ba4f6553dcbc","name":"android","slug":"android","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:45:19","created_by":"1","updated_at":"2017-08-07 04:45:19","updated_by":null},{"id":"5f527ea284e1ba4f6553dcbe","name":"tax","slug":"tax","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 11:57:14","created_by":"1","updated_at":"2017-08-07 11:57:14","updated_by":null},{"id":"5f527ea284e1ba4f6553dcc1","name":"windows; ubuntu","slug":"windows-ubuntu-2","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-09-19 16:40:33","created_by":"1","updated_at":"2017-09-19 16:40:33","updated_by":null},{"id":"5f527ea284e1ba4f6553dcbf","name":"packrat","slug":"packrat","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:43:32","created_by":"1","updated_at":"2017-08-07 04:43:32","updated_by":null},{"id":"5f527ea284e1ba4f6553dcc0","name":"windows ubuntu","slug":"windows-ubuntu","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-09-19 16:37:41","created_by":"1","updated_at":"2017-09-19 16:37:41","updated_by":null},{"id":"5f527ea284e1ba4f6553dcc2","name":"windows","slug":"windows","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-09-19 16:42:42","created_by":"1","updated_at":"2017-09-19 16:42:42","updated_by":null},{"id":"5f527ea284e1ba4f6553dcbb","name":"make-it-fast","slug":"make-it-fast","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:44:49","created_by":"1","updated_at":"2017-08-07 04:44:49","updated_by":null},{"id":"5f527ea284e1ba4f6553dcc7","name":"deployment","slug":"deployment","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-10-11 11:51:31","created_by":"1","updated_at":"2017-10-11 11:51:31","updated_by":null},{"id":"5f527ea284e1ba4f6553dcc8","name":"springboot","slug":"springboot","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-10-11 13:07:02","created_by":"1","updated_at":"2017-10-11 13:07:02","updated_by":null},{"id":"5f527ea284e1ba4f6553dcc9","name":"dash.py","slug":"dash-py","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-11-14 13:46:48","created_by":"1","updated_at":"2017-11-14 13:46:48","updated_by":null},{"id":"5f527ea284e1ba4f6553dcc5","name":"docker","slug":"docker","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-10-11 11:51:31","created_by":"1","updated_at":"2017-10-11 11:51:31","updated_by":null},{"id":"5f527ea284e1ba4f6553dccb","name":"economics","slug":"economics","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:41:27","created_by":"1","updated_at":"2017-08-07 04:41:27","updated_by":null},{"id":"5f527ea284e1ba4f6553dcc3","name":"ubuntu","slug":"ubuntu","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-09-19 16:42:42","created_by":"1","updated_at":"2017-09-19 16:42:42","updated_by":null},{"id":"5f527ea284e1ba4f6553dcbd","name":"shiny","slug":"shiny","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:46:34","created_by":"1","updated_at":"2017-08-07 04:46:34","updated_by":null},{"id":"5f527ea284e1ba4f6553dcc6","name":"nginx","slug":"nginx","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-10-11 11:51:31","created_by":"1","updated_at":"2017-10-11 11:51:31","updated_by":null},{"id":"5f527ea284e1ba4f6553dcb2","name":"python","slug":"python","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:37:58","created_by":"1","updated_at":"2017-08-07 04:37:58","updated_by":null},{"id":"5f527ea284e1ba4f6553dcb4","name":"viz","slug":"viz","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:40:41","created_by":"1","updated_at":"2017-08-07 04:40:41","updated_by":null},{"id":"5f527ea284e1ba4f6553dcd1","name":"Human Rights","slug":"human-rights","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-02-27 22:00:12","created_by":"1","updated_at":"2020-02-27 22:00:12","updated_by":null},{"id":"5f527ea284e1ba4f6553dcd2","name":"Tamil","slug":"tamil","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-02-27 22:00:12","created_by":"1","updated_at":"2020-02-27 22:00:12","updated_by":null},{"id":"5f527ea284e1ba4f6553dccf","name":"Sri Lanka","slug":"sri-lanka","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-02-27 22:00:12","created_by":"1","updated_at":"2020-02-27 22:00:12","updated_by":null},{"id":"5f527ea284e1ba4f6553dcce","name":"CHOGM","slug":"chogm","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-02-27 22:00:12","created_by":"1","updated_at":"2020-02-27 22:00:12","updated_by":null},{"id":"5f527ea284e1ba4f6553dcd5","name":"tech","slug":"tech","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-04-26 22:04:08","created_by":"1","updated_at":"2020-04-26 22:04:08","updated_by":null},{"id":"5f527ea284e1ba4f6553dcd6","name":"network","slug":"network","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-05-17 22:09:24","created_by":"1","updated_at":"2020-05-17 22:09:24","updated_by":null},{"id":"5f527ea284e1ba4f6553dcd7","name":"wifi","slug":"wifi","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-05-17 22:09:24","created_by":"1","updated_at":"2020-05-17 22:09:24","updated_by":null},{"id":"5f527ea284e1ba4f6553dcd3","name":"United Nations","slug":"united-nations","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-02-27 22:00:12","created_by":"1","updated_at":"2020-02-27 22:00:12","updated_by":null},{"id":"5f527ea284e1ba4f6553dcc4","name":"os","slug":"os","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-09-19 16:42:42","created_by":"1","updated_at":"2017-09-19 16:42:42","updated_by":null},{"id":"5f527ea284e1ba4f6553dcca","name":"australia","slug":"australia","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:41:27","created_by":"1","updated_at":"2017-08-07 04:41:27","updated_by":null},{"id":"5f527ea284e1ba4f6553dcd9","name":"foss","slug":"foss","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-05-31 15:05:21","created_by":"1","updated_at":"2020-05-31 15:05:21","updated_by":null},{"id":"5f527ea284e1ba4f6553dcda","name":"self-host","slug":"self-host","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-05-31 15:05:21","created_by":"1","updated_at":"2020-05-31 15:05:21","updated_by":null},{"id":"5f527ea284e1ba4f6553dcd4","name":"r-pi","slug":"r-pi","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-04-26 22:04:08","created_by":"1","updated_at":"2020-04-26 22:04:08","updated_by":null},{"id":"5f527ea284e1ba4f6553dccc","name":"politics","slug":"politics","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 11:57:14","created_by":"1","updated_at":"2017-08-07 11:57:14","updated_by":null},{"id":"5f527ea284e1ba4f6553dcd8","name":"rss","slug":"rss-tag","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-05-31 15:05:21","created_by":"1","updated_at":"2020-05-31 15:05:21","updated_by":null},{"id":"5f527ea284e1ba4f6553dccd","name":"shinyproxy","slug":"shinyproxy","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-11-14 13:46:48","created_by":"1","updated_at":"2017-11-14 13:46:48","updated_by":null},{"id":"5f527ea284e1ba4f6553dcb8","name":"leaflet.js","slug":"leaflet-js","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:42:30","created_by":"1","updated_at":"2017-08-07 04:42:30","updated_by":null},{"id":"5f527ea284e1ba4f6553dcd0","name":"Civil War","slug":"civil-war","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2020-02-27 22:00:12","created_by":"1","updated_at":"2020-02-27 22:00:12","updated_by":null},{"id":"5f527ea284e1ba4f6553dcba","name":"microbenchmark","slug":"microbenchmark","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2017-08-07 04:44:49","created_by":"1","updated_at":"2017-08-07 04:44:49","updated_by":null}],"posts_tags":[{"id":"5f527c0484e1ba4f6553dc5c","post_id":"5f527c0484e1ba4f6553db80","tag_id":"5f527c0384e1ba4f6553db2b","sort_order":0},{"id":"5f527c0484e1ba4f6553dc5d","post_id":"5f527c0484e1ba4f6553db82","tag_id":"5f527c0384e1ba4f6553db2b","sort_order":0},{"id":"5f527c0484e1ba4f6553dc5e","post_id":"5f527c0484e1ba4f6553db84","tag_id":"5f527c0384e1ba4f6553db2b","sort_order":0},{"id":"5f527c0484e1ba4f6553dc5f","post_id":"5f527c0484e1ba4f6553db86","tag_id":"5f527c0384e1ba4f6553db2b","sort_order":0},{"id":"5f527c0484e1ba4f6553dc60","post_id":"5f527c0484e1ba4f6553db88","tag_id":"5f527c0384e1ba4f6553db2b","sort_order":0},{"id":"5f527c0484e1ba4f6553dc61","post_id":"5f527c0484e1ba4f6553db8a","tag_id":"5f527c0384e1ba4f6553db2b","sort_order":0},{"id":"5f527c0484e1ba4f6553dc62","post_id":"5f527c0484e1ba4f6553db8c","tag_id":"5f527c0384e1ba4f6553db2b","sort_order":0},{"id":"5f527ea284e1ba4f6553dd23","post_id":"5f527ea284e1ba4f6553dcdd","tag_id":"5f527ea284e1ba4f6553dcb1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd24","post_id":"5f527ea284e1ba4f6553dcdd","tag_id":"5f527ea284e1ba4f6553dcb4","sort_order":1},{"id":"5f527ea284e1ba4f6553dd25","post_id":"5f527ea284e1ba4f6553dcdd","tag_id":"5f527ea284e1ba4f6553dcb5","sort_order":2},{"id":"5f527ea284e1ba4f6553dd29","post_id":"5f527ea284e1ba4f6553dcdf","tag_id":"5f527ea284e1ba4f6553dccb","sort_order":0},{"id":"5f527ea284e1ba4f6553dd2c","post_id":"5f527ea284e1ba4f6553dcdf","tag_id":"5f527ea284e1ba4f6553dccc","sort_order":3},{"id":"5f527ea284e1ba4f6553dd2b","post_id":"5f527ea284e1ba4f6553dcdf","tag_id":"5f527ea284e1ba4f6553dcbe","sort_order":2},{"id":"5f527ea284e1ba4f6553dd2a","post_id":"5f527ea284e1ba4f6553dcdf","tag_id":"5f527ea284e1ba4f6553dcca","sort_order":1},{"id":"5f527ea284e1ba4f6553dd35","post_id":"5f527ea284e1ba4f6553dce6","tag_id":"5f527ea284e1ba4f6553dcb1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd36","post_id":"5f527ea284e1ba4f6553dce6","tag_id":"5f527ea284e1ba4f6553dcb4","sort_order":1},{"id":"5f527ea284e1ba4f6553dd37","post_id":"5f527ea284e1ba4f6553dce6","tag_id":"5f527ea284e1ba4f6553dcb6","sort_order":2},{"id":"5f527ea284e1ba4f6553dd38","post_id":"5f527ea284e1ba4f6553dce6","tag_id":"5f527ea284e1ba4f6553dcb7","sort_order":3},{"id":"5f527ea284e1ba4f6553dd39","post_id":"5f527ea284e1ba4f6553dce6","tag_id":"5f527ea284e1ba4f6553dcb8","sort_order":4},{"id":"5f527ea284e1ba4f6553dd3c","post_id":"5f527ea284e1ba4f6553dce7","tag_id":"5f527ea284e1ba4f6553dcb1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd3d","post_id":"5f527ea284e1ba4f6553dce7","tag_id":"5f527ea284e1ba4f6553dcbf","sort_order":1},{"id":"5f527ea284e1ba4f6553dd3e","post_id":"5f527ea284e1ba4f6553dce7","tag_id":"5f527ea284e1ba4f6553dcb9","sort_order":2},{"id":"5f527ea284e1ba4f6553dd42","post_id":"5f527ea284e1ba4f6553dce9","tag_id":"5f527ea284e1ba4f6553dcb1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd43","post_id":"5f527ea284e1ba4f6553dce9","tag_id":"5f527ea284e1ba4f6553dcba","sort_order":1},{"id":"5f527ea284e1ba4f6553dd44","post_id":"5f527ea284e1ba4f6553dce9","tag_id":"5f527ea284e1ba4f6553dcbb","sort_order":2},{"id":"5f527ea284e1ba4f6553dd46","post_id":"5f527ea284e1ba4f6553dcea","tag_id":"5f527ea284e1ba4f6553dcb2","sort_order":0},{"id":"5f527ea284e1ba4f6553dd47","post_id":"5f527ea284e1ba4f6553dcea","tag_id":"5f527ea284e1ba4f6553dcb3","sort_order":1},{"id":"5f527ea284e1ba4f6553dd48","post_id":"5f527ea284e1ba4f6553dcea","tag_id":"5f527ea284e1ba4f6553dcb4","sort_order":2},{"id":"5f527ea284e1ba4f6553dd4a","post_id":"5f527ea284e1ba4f6553dceb","tag_id":"5f527ea284e1ba4f6553dcbc","sort_order":0},{"id":"5f527ea284e1ba4f6553dd51","post_id":"5f527ea284e1ba4f6553dcf1","tag_id":"5f527ea284e1ba4f6553dcb2","sort_order":0},{"id":"5f527ea284e1ba4f6553dd52","post_id":"5f527ea284e1ba4f6553dcf1","tag_id":"5f527ea284e1ba4f6553dcb1","sort_order":1},{"id":"5f527ea284e1ba4f6553dd58","post_id":"5f527ea284e1ba4f6553dcf5","tag_id":"5f527ea284e1ba4f6553dcb1","sort_order":0},{"id":"5f527ea284e1ba4f6553dd5a","post_id":"5f527ea284e1ba4f6553dcf5","tag_id":"5f527ea284e1ba4f6553dcbd","sort_order":2},{"id":"5f527ea284e1ba4f6553dd59","post_id":"5f527ea284e1ba4f6553dcf5","tag_id":"5f527ea284e1ba4f6553dcbb","sort_order":1},{"id":"5f527ea284e1ba4f6553dd60","post_id":"5f527ea284e1ba4f6553dcfa","tag_id":"5f527ea284e1ba4f6553dcc2","sort_order":0},{"id":"5f527ea284e1ba4f6553dd61","post_id":"5f527ea284e1ba4f6553dcfa","tag_id":"5f527ea284e1ba4f6553dcc3","sort_order":1},{"id":"5f527ea284e1ba4f6553dd62","post_id":"5f527ea284e1ba4f6553dcfa","tag_id":"5f527ea284e1ba4f6553dcc4","sort_order":2},{"id":"5f527ea284e1ba4f6553dd64","post_id":"5f527ea284e1ba4f6553dcfb","tag_id":"5f527ea284e1ba4f6553dcb2","sort_order":0},{"id":"5f527ea284e1ba4f6553dd67","post_id":"5f527ea284e1ba4f6553dcfb","tag_id":"5f527ea284e1ba4f6553dcc5","sort_order":3},{"id":"5f527ea284e1ba4f6553dd65","post_id":"5f527ea284e1ba4f6553dcfb","tag_id":"5f527ea284e1ba4f6553dcb1","sort_order":1},{"id":"5f527ea284e1ba4f6553dd6a","post_id":"5f527ea284e1ba4f6553dcfb","tag_id":"5f527ea284e1ba4f6553dcc8","sort_order":6},{"id":"5f527ea284e1ba4f6553dd68","post_id":"5f527ea284e1ba4f6553dcfb","tag_id":"5f527ea284e1ba4f6553dcc6","sort_order":4},{"id":"5f527ea284e1ba4f6553dd66","post_id":"5f527ea284e1ba4f6553dcfb","tag_id":"5f527ea284e1ba4f6553dcbd","sort_order":2},{"id":"5f527ea284e1ba4f6553dd69","post_id":"5f527ea284e1ba4f6553dcfb","tag_id":"5f527ea284e1ba4f6553dcc7","sort_order":5},{"id":"5f527ea384e1ba4f6553dd73","post_id":"5f527ea284e1ba4f6553dd02","tag_id":"5f527ea284e1ba4f6553dcb2","sort_order":0},{"id":"5f527ea384e1ba4f6553dd75","post_id":"5f527ea284e1ba4f6553dd02","tag_id":"5f527ea284e1ba4f6553dcc9","sort_order":2},{"id":"5f527ea384e1ba4f6553dd76","post_id":"5f527ea284e1ba4f6553dd02","tag_id":"5f527ea284e1ba4f6553dccd","sort_order":3},{"id":"5f527ea384e1ba4f6553dd74","post_id":"5f527ea284e1ba4f6553dd02","tag_id":"5f527ea284e1ba4f6553dcc5","sort_order":1},{"id":"5f527ea384e1ba4f6553dd7a","post_id":"5f527ea284e1ba4f6553dd04","tag_id":"5f527ea284e1ba4f6553dcb3","sort_order":0},{"id":"5f527ea384e1ba4f6553dd7b","post_id":"5f527ea284e1ba4f6553dd04","tag_id":"5f527ea284e1ba4f6553dcb1","sort_order":1},{"id":"5f527ea384e1ba4f6553dd91","post_id":"5f527ea284e1ba4f6553dd19","tag_id":"5f527ea284e1ba4f6553dcce","sort_order":0},{"id":"5f527ea384e1ba4f6553dd94","post_id":"5f527ea284e1ba4f6553dd19","tag_id":"5f527ea284e1ba4f6553dcd1","sort_order":3},{"id":"5f527ea384e1ba4f6553dd93","post_id":"5f527ea284e1ba4f6553dd19","tag_id":"5f527ea284e1ba4f6553dcd0","sort_order":2},{"id":"5f527ea384e1ba4f6553dd92","post_id":"5f527ea284e1ba4f6553dd19","tag_id":"5f527ea284e1ba4f6553dccf","sort_order":1},{"id":"5f527ea384e1ba4f6553dd95","post_id":"5f527ea284e1ba4f6553dd19","tag_id":"5f527ea284e1ba4f6553dcd2","sort_order":4},{"id":"5f527ea384e1ba4f6553dd96","post_id":"5f527ea284e1ba4f6553dd19","tag_id":"5f527ea284e1ba4f6553dcd3","sort_order":5},{"id":"5f527ea384e1ba4f6553dd9a","post_id":"5f527ea284e1ba4f6553dd1c","tag_id":"5f527ea284e1ba4f6553dcd4","sort_order":0},{"id":"5f527ea384e1ba4f6553dd9b","post_id":"5f527ea284e1ba4f6553dd1c","tag_id":"5f527ea284e1ba4f6553dcd5","sort_order":1},{"id":"5f527ea384e1ba4f6553dda1","post_id":"5f527ea284e1ba4f6553dd1f","tag_id":"5f527ea284e1ba4f6553dcd6","sort_order":0},{"id":"5f527ea384e1ba4f6553dda2","post_id":"5f527ea284e1ba4f6553dd1f","tag_id":"5f527ea284e1ba4f6553dcd7","sort_order":1},{"id":"5f527ea384e1ba4f6553dda4","post_id":"5f527ea284e1ba4f6553dd20","tag_id":"5f527ea284e1ba4f6553dcd8","sort_order":0},{"id":"5f527ea384e1ba4f6553dda5","post_id":"5f527ea284e1ba4f6553dd20","tag_id":"5f527ea284e1ba4f6553dcd9","sort_order":1},{"id":"5f527ea384e1ba4f6553dda6","post_id":"5f527ea284e1ba4f6553dd20","tag_id":"5f527ea284e1ba4f6553dcd5","sort_order":2},{"id":"5f527ea384e1ba4f6553dda7","post_id":"5f527ea284e1ba4f6553dd20","tag_id":"5f527ea284e1ba4f6553dcda","sort_order":3}],"apps":[],"app_settings":[],"app_fields":[],"invites":[],"brute":[{"key":"oHUubZQTM66eOWJCFaoi+8dO/eXPG5zwBOW8P5YAuKM=","firstRequest":1599241776395,"lastRequest":1599241776395,"lifetime":1599245376397,"count":1}],"webhooks":[],"integrations":[{"id":"5f527c0484e1ba4f6553db8e","type":"builtin","name":"Zapier","slug":"zapier","icon_image":null,"description":"Built-in Zapier integration","created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db90","type":"internal","name":"Ghost Backup","slug":"ghost-backup","icon_image":null,"description":"Internal DB Backup integration","created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"},{"id":"5f527c0484e1ba4f6553db92","type":"internal","name":"Ghost Scheduler","slug":"ghost-scheduler","icon_image":null,"description":"Internal Scheduler integration","created_at":"2020-09-04 17:40:20","created_by":"1","updated_at":"2020-09-04 17:40:20","updated_by":"1"}],"api_keys":[{"id":"5f527c0484e1ba4f6553db8f","type":"admin","secret":"28aa0402038758f5bc88a8d3da95f9ef945c0f7e9ff37a4a987a7837c4617d82","role_id":"5f527c0384e1ba4f6553db31","integration_id":"5f527c0484e1ba4f6553db8e","last_seen_at":null,"last_seen_version":null,"created_at":1599241220571,"created_by":"1","updated_at":1599241220571,"updated_by":"1"},{"id":"5f527c0484e1ba4f6553db91","type":"admin","secret":"46a9d768644117262ada903640dea55fc451131068ac37723e64cd758468114a","role_id":"5f527c0384e1ba4f6553db32","integration_id":"5f527c0484e1ba4f6553db90","last_seen_at":null,"last_seen_version":null,"created_at":1599241220577,"created_by":"1","updated_at":1599241220577,"updated_by":"1"},{"id":"5f527c0484e1ba4f6553db93","type":"admin","secret":"42c1cd6fa42e91dbc3df114340f93da2d5aa423df7e06f31d12ef8485733dc85","role_id":"5f527c0384e1ba4f6553db33","integration_id":"5f527c0484e1ba4f6553db92","last_seen_at":null,"last_seen_version":null,"created_at":1599241220583,"created_by":"1","updated_at":1599241220583,"updated_by":"1"}],"members":[],"labels":[],"members_labels":[],"members_stripe_customers":[],"members_stripe_customers_subscriptions":[],"actions":[],"emails":[]}}